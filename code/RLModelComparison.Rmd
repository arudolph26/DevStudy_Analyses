---
title: "RLModelComparison"
author: "Zeynep Enkavi"
date: "August 16, 2016"
output: html_document
---

```{r}
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
```

```{r}
data <- read.csv('/Users/zeynepenkavi/Downloads/ProbLearn406620.csv')

data$play1_pass0 <- with(data, ifelse(Response == 1, 1, 0))
```

```{r}
fit.single.alpha <- function(x){
  
  fn <- function(p){
    
    EV <- c(0,0,0,0)
    condition <- x$Trial_type
    pred_error <- 0
    choice_prob <- rep(0,nrow(x))
    
    #turn sink on
    sink(file='/Users/zeynepenkavi/Downloads/fit_single_alpha.log', append=T)
    
    for(i in 1:nrow(x)){
      if(x$play1_pass0[i]==1){
        choice_prob[i] <- 1/(1+exp(p[1]*EV[condition[i]]))
        #choice_prob[i] <- 1/(1+exp(EV[condition[i]]))
      }
      else if(x$play1_pass0[i]==0){
        choice_prob[i] <- 1 - 1/(1+exp(p[1]*EV[condition[i]]))
        #choice_prob[i] <- 1 - 1/(1+exp(EV[condition[i]]))
      }
      
      if(x$Points_earned[i]!=0){
        pred_error <- p[2]*(x$Points_earned[i]-EV[condition[i]])
        #pred_error <- p[1]*(x$Points_earned[i]-EV[condition[i]])
        EV[condition[i]] <- EV[condition[i]]+pred_error
      }
      cat(c(i,EV,p[1],p[2],"\n"))
    }
    
    #turn sink off
    sink()
    
    #replace 1 and 0 to avoid log(1) and log(0)
    choice_prob <- ifelse(choice_prob == 1, 0.9999, 
                          ifelse(choice_prob == 0, 0.0001, choice_prob))
    
    #get binomial log likelihood
    err <- x$play1_pass0 * log(choice_prob) + (1 - x$play1_pass0)*log(1-choice_prob)
    
    #sum of negative log likelihood (to be minimized)
    sumerr <- -sum(err)
    
    return(sumerr)  
  }
  
  fit <- optim(par = c(1, 0), fn, hessian = TRUE)
  #fit <- optim(par = c(0.5), fn, hessian = TRUE, method="Brent", lower=0, upper=1)
  
  sumerr <- fit$value
  b <- fit$par[1]
  #b <- 1
  alpha <- fit$par[2]
  out <- data.frame(sumerr, b, alpha)
  
  return(out) 
}

fit.single.alpha(data)
```

```{r}
fit.singlealpha.log <- read.table("/Users/zeynepenkavi/Downloads/fit_single_alpha.log", col.names = c("trial", "ev1", "ev2", "ev3", "ev4", "b", "alpha"))

fit.singlealpha.log %>%
  select(-trial) %>%
  gather(parameter, value) %>%
  ggplot(aes(value))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(~parameter, scales='free')

fit.singlealpha.log %>%
  select(starts_with('ev')) %>%
  gather(key,value)%>%
  group_by(key)%>%
  summarise(mean=mean(value))
```

#Remember to check:
#what do you get from the bayesian method?
#what happens if you fix b=1? is alpha much smaller then?

#Quick meeting prep

Examine the output of `select_optimal_parameters(subject)` in `prediction_error_final.py`

Based on `AllLearningParameterswExponentSingleAlpha.csv` the estimates for this subject should be 0.01 for `alpha_neg` and `0.92` for the exponent.

Based on my fit the estimates are closer to 0.015 and 0.887. Close but not perfect.

```{r}
py.output <- read.csv('/Users/zeynepenkavi/Downloads/LearningParameterswExponentFixedAPos406620.csv', col.names = c("alphaneg_draw", "exponent_draw", "alphaneg_opt", "exponent_opt", "pred_error"))
head(py.output)

py.output %>%
  gather(key,value) %>%
  ggplot(aes(value))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(~key, scales='free')
```

#Same python model in r

```{r}
fit.exp.single.alpha <- function(x){
  
  fn <- function(p){
    
    EV <- c(0,0,0,0)
    TrialNum <- x$Trial_type
    Response <- x$Response
    Outcome <- x$Points_earned
    Prediction_Error <- 0
    alphapos <- 0.05
    alphaneg <- p[1]
    beta <- 1
    exponent <- p[2]
    choice_prob <- rep(0,nrow(x))
    
    #turn sink on
    sink(file='/Users/zeynepenkavi/Downloads/fit_exp_single_alpha.log', append=T)
    
    for(i in 1:nrow(x)){
      
      if(Response[i] == 0){
        choice_prob[i] <- 1
      }
      else if(Response[i] == 1){
        choice_prob[i] <- exp(EV[TrialNum[i]]*beta)/exp(EV[TrialNum[i]]*beta+1)
      }
      else if(Response[i] == 2){
        choice_prob[i] <- 1 - exp(EV[TrialNum[i]]*beta)/exp(EV[TrialNum[i]]*beta+1)
      }
      
      if(Outcome[i] != 0){
        if (Outcome[i]>EV[TrialNum[i]]){
          Prediction_Error <- alphapos*(Outcome[i]-EV[TrialNum[i]])^exponent
          EV[TrialNum[i]] <- EV[TrialNum[i]]+Prediction_Error
        }
        else if (Outcome[i]<EV[TrialNum[i]]){
          Prediction_Error = -1*alphaneg*(EV[TrialNum[i]]-Outcome[i])^exponent
          EV[TrialNum[i]] <- EV[TrialNum[i]]+Prediction_Error
        }
        else if (Outcome[i] == EV[TrialNum[i]]){
          Prediction_Error = 0
          EV[TrialNum[i]] <- EV[TrialNum[i]]+Prediction_Error
        }
      }
      
      cat(c(i,EV,p[1],p[2],"\n"))
    }
    
    #turn sink off
    sink()
    
    choicelogprob <- 0
    
    for(i in 1:length(choice_prob)){
      choicelogprob = choicelogprob - log(choice_prob[i])
    }
    
    return(choicelogprob)  
  }
  
  #fit <- optim(par = c(runif(1, 0, 0.4),runif(1, 0, 1)), fn, hessian = TRUE)
  fit <- optim(par = c(0.01,0.92), fn, hessian = TRUE)

  choicelogprob <- fit$value
  alphaneg <- fit$par[1]
  exponent <- fit$par[2]
  out <- data.frame(choicelogprob, alphaneg, exponent)
  
  return(out) 
}

fit.exp.single.alpha(data)
```

```{r}
fit.exp.singlealpha.log <- read.table("/Users/zeynepenkavi/Downloads/fit_exp_single_alpha.log", col.names = c("trial", "ev1", "ev2", "ev3", "ev4", "alphaneg", "exponent"))

fit.exp.singlealpha.log %>% 
  select(-trial) %>%
  gather(key, value) %>%
  ggplot(aes(value))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(~key, scales='free')
```

simulate data and then fit both in py and r


```{r}
values = read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/Developmental study/Task/Dev_Learning_Study/values.csv', col.names = c(1,2,3,4), header=F)

values = values[1:45,]
values = values %>% gather(condition, outcome)
values$condition = as.numeric(as.factor(values$condition))

sim.data <- data.frame(Trialnumber = c(1:180), Trial_type = c(rep(1, 45), rep(2, 45), rep(3, 45), rep(4, 45)), Response=NA, Points_earned=NA, choiceprob=NA)

sim.alpha = 0.05
sim.beta = 0.5

sim.EV = c(0,0,0,0)
sim.TrialNum = sim.data$Trial_type

for(i in 1:nrow(sim.data)){
  sim.data$choiceprob[i] = exp(sim.EV[sim.TrialNum[i]]*sim.beta)/(exp(sim.EV[sim.TrialNum[i]]*sim.beta)+1)
  
  sim.data$Response[i] = rbinom(1,1,sim.data$choiceprob[i])
  #sim.data$Response[i] = ifelse(sim.data$choiceprob[i]>=0.5, 1, 0) #make it a hardmax
  
  sim.data$Points_earned[i] = ifelse(sim.data$Response[i]==1, values$outcome[i], ifelse(sim.data$Response[i]==0, 0, NA))
  
  sim.EV[sim.TrialNum[i]] <- ifelse(sim.data$Response[i]==1,sim.EV[sim.TrialNum[i]]+sim.alpha*(sim.data$Points_earned[i]-sim.EV[sim.TrialNum[i]]),sim.EV[sim.TrialNum[i]])
}

# sim.data %>%
#   ggplot(aes(Trialnumber,Response))+
#   geom_point()+
#   geom_smooth(method='glm', method.args=list(family='binomial'))+
#   theme_bw()+
#   facet_wrap(~Trial_type)
```

WHY DOES IT NOT WORK WELL WHEN ESTIMATING BOTH PARAMETERS?
Does a better job estimating a single parameter better when one of them is fixed?

```{r}
sim.single.alpha <- function(x){
  
  fn <- function(p){
    
    prob <- rep(0, nrow(x))
    err <- rep(0, nrow(x))
    ev <- c(0,0,0,0)
    
    for(i in 1:nrow(x)){
      
      #u.play <- ev[x$Trial_type[i]]
      #u.pass <- 0
      
      #prob[i] <- 1 / (1 + exp(p[1]*(u.pass - u.play)))
      
      prob[i] = exp(ev[x$Trial_type[i]]*p[1])/(exp(ev[x$Trial_type[i]]*p[1])+1) #same thing
      #prob[i] = exp(ev[x$Trial_type[i]]*p[1])/(exp(ev[x$Trial_type[i]]*p[1])+1) 
      
      ev[x$Trial_type[i]] <- ifelse(x$Response[i] == 1, ev[x$Trial_type[i]]+p[2]*(x$Points_earned[i]-ev[x$Trial_type[i]]) ,ev[x$Trial_type[i]])
      #ev[x$Trial_type[i]] <- ifelse(x$Response[i] == 1, ev[x$Trial_type[i]]+0.05*(x$Points_earned[i]-ev[x$Trial_type[i]]) ,ev[x$Trial_type[i]])
  
    }
   
    prob <- ifelse(prob == 1, 0.999999999999, 
                          ifelse(prob == 0, 0.000000000001, prob))
    
    #get log likelihood
    err <- x$Response * log(prob) + (1 - x$Response)*log(1-prob)
    
    #sum of negative log likelihood (to be minimized)
    sumerr <- -sum(err)
    
    return(sumerr)  
  }
  
  fit <- optim(par = c(0.5, 0.5), fn, hessian = TRUE, method = "L-BFGS-B",
               lower = c(0, 0), upper = c(Inf, 1))
  #fit <- optim(par = c(1), fn, hessian = TRUE, method = "L-BFGS-B",
               #lower = c(0), upper = c(Inf))
  
  sumerr <- fit$value
  beta <- fit$par[1]
  alpha <- fit$par[2]
  out <- data.frame(sumerr, beta, alpha)
  #out <- data.frame(sumerr, fit$par[1], 0.05)
  
  return(out) 
}

sim.single.alpha(sim.data)
```

Can it tell that another dataset has different parameters? Maybe.

```{r}
sim2.data <- data.frame(Trialnumber = c(1:180), Trial_type = c(rep(1, 45), rep(2, 45), rep(3, 45), rep(4, 45)), Response=NA, Points_earned=NA, choiceprob=NA)

sim2.alpha = 0.1
sim2.beta = 0.35

sim2.EV = c(0,0,0,0)
sim2.TrialNum = sim2.data$Trial_type

for(i in 1:nrow(sim2.data)){
  sim2.data$choiceprob[i] = exp(sim2.EV[sim2.TrialNum[i]]*sim2.beta)/(exp(sim2.EV[sim2.TrialNum[i]]*sim2.beta)+1)
  
  sim2.data$Response[i] = rbinom(1,1,sim2.data$choiceprob[i])
  
  sim2.data$Points_earned[i] = ifelse(sim2.data$Response[i]==1, values$outcome[i], ifelse(sim2.data$Response[i]==0, 0, NA))
  
  sim2.EV[sim2.TrialNum[i]] <- ifelse(sim2.data$Response[i]==1,sim2.EV[sim2.TrialNum[i]]+sim2.alpha*(sim2.data$Points_earned[i]-sim2.EV[sim2.TrialNum[i]]),sim2.EV[sim2.TrialNum[i]])
}

sim.single.alpha(sim2.data)
```

```{r}

sim3.data <- data.frame(Trialnumber = c(1:180), Trial_type = c(rep(1, 45), rep(2, 45), rep(3, 45), rep(4, 45)), Response=NA, Points_earned=NA, choiceprob=NA)

sim3.alpha = 0.01
sim3.beta = 0.1
sim3.gamma = 0.6

sim3.EV = c(0,0,0,0)
sim3.TrialNum = sim3.data$Trial_type

values$outcome_exp <- ifelse(values$outcome<0, -1*(abs(values$outcome)^sim3.gamma), values$outcome^sim3.gamma)

for(i in 1:nrow(sim3.data)){
  sim3.data$choiceprob[i] = exp(sim3.EV[sim3.TrialNum[i]]*sim3.beta)/(exp(sim3.EV[sim3.TrialNum[i]]*sim3.beta)+1)
  
  sim3.data$Response[i] = rbinom(1,1,sim3.data$choiceprob[i])
  
  sim3.data$Points_earned[i] = ifelse(sim3.data$Response[i]==1, values$outcome[i], ifelse(sim3.data$Response[i]==0, 0, NA))
  
  sim3.data$Points_exp[i] = values$outcome_exp[i]
  
  #sim3.EV[sim3.TrialNum[i]] <- ifelse(sim3.data$Response[i]==1, sim3.EV[sim3.TrialNum[i]]+sim3.alpha*(sim3.data$Points_earned[i]-sim3.EV[sim3.TrialNum[i]]),sim3.EV[sim3.TrialNum[i]])
  
  sim3.EV[sim3.TrialNum[i]] <- ifelse(sim3.data$Response[i]==1 & sim3.data$Points_earned[i] > 0, sim3.EV[sim3.TrialNum[i]]+sim3.alpha*(sim3.data$Points_earned[i]^sim3.gamma-sim3.EV[sim3.TrialNum[i]]), ifelse(sim3.data$Response[i]==1 & sim3.data$Points_earned[i] < 0, sim3.alpha*((-1)*abs(sim3.data$Points_earned[i])^sim3.gamma-sim3.EV[sim3.TrialNum[i]]),  sim3.EV[sim3.TrialNum[i]]))
}

```

```{r}
sim.single.alpha.exponent <- function(x, input_par){
  
  fn <- function(p){
    
    prob <- rep(0, nrow(x))
    err <- rep(0, nrow(x))
    ev <- c(0,0,0,0)
    
    for(i in 1:nrow(x)){
      
      prob[i] = exp(ev[x$Trial_type[i]]*p[1])/(exp(ev[x$Trial_type[i]]*p[1])+1) 
      
      #ev[x$Trial_type[i]] <- ifelse(x$Response[i] == 1, ev[x$Trial_type[i]]+p[2]*(x$Points_earned[i]-ev[x$Trial_type[i]]) ,ev[x$Trial_type[i]])
      
      ev[x$Trial_type[i]] <- ifelse(x$Response[i]==1 & x$Points_earned[i] > 0, ev[x$Trial_type[i]]+p[2]*(x$Points_earned[i]^0.6-ev[x$Trial_type[i]]), ifelse(x$Response[i]==1 & x$Points_earned[i] < 0, p[2]*((-1)*abs(x$Points_earned[i])^0.6-ev[x$Trial_type[i]]),  ev[x$Trial_type[i]]))
  
    }
   
    prob <- ifelse(prob == 1, 0.999999999999, 
                          ifelse(prob == 0, 0.000000000001, prob))
    
    #get log likelihood
    err <- x$Response * log(prob) + (1 - x$Response)*log(1-prob)
    
    #sum of negative log likelihood (to be minimized)
    sumerr <- -sum(err)
    
    return(sumerr)  
  }
  
  fit <- optim(par = input_par, fn, hessian = TRUE, method = "L-BFGS-B",
               lower = c(0, 0), upper = c(Inf, 1))
  
  sumerr <- fit$value
  beta <- fit$par[1]
  alpha <- fit$par[2]
  gamma <- 0.6
  out <- data.frame(sumerr, beta, alpha, gamma)

  return(out) 
}

sim.single.alpha.exponent(sim3.data, c(1,1))

#Run 1000 with different starting values and check distributions of point estimates for both parameters
out <- data.frame(beta = rep(NA, 1000), alpha = rep(NA, 1000))
# for(i in 1:1000){
#   input_par = c(runif(1, 0, 1),runif(1,0, 0.5))
#   tmp <- sim.single.alpha.exponent(sim3.data, input_par)
#   out$starting
#   out$beta[i] <- tmp$beta
#   out$alpha[i] <- tmp$alpha
# }

input_par_df <- data.frame(beta_start = runif(1000, 0, 1), alpha_start = runif(1000,0, 0.5))

for(i in 1:400){
  tmp <- sim.single.alpha.exponent(sim3.data, c(input_par_df$beta_start[i], input_par_df$alpha_start[i]))
  out$beta[i] <- tmp$beta
  out$alpha[i] <- tmp$alpha
}

out %>% gather(key, value) %>%
  ggplot(aes(value))+
  geom_histogram(bins = 100)+
  theme_bw()+
  facet_wrap(~key)+
  xlim(0, 0.25)

```

```{r}
sim4.data <- data.frame(Trialnumber = c(1:180), Trial_type = c(rep(1, 45), rep(2, 45), rep(3, 45), rep(4, 45)), Response=NA, Points_earned=NA, choiceprob=NA)

sim4.alphapos = 0.05
sim4.alphaneg = 0.01
sim4.beta = 1
sim4.gamma = 0.6

sim4.EV = c(0,0,0,0)
sim4.TrialNum = sim4.data$Trial_type

for(i in 1:nrow(sim4.data)){
  sim4.data$choiceprob[i] = exp(sim4.EV[sim4.TrialNum[i]]*sim4.beta)/(exp(sim4.EV[sim4.TrialNum[i]]*sim4.beta)+1)
  
  sim4.data$Response[i] = rbinom(1,1,sim4.data$choiceprob[i])
  
  sim4.data$Points_earned[i] = ifelse(sim4.data$Response[i]==1, values$outcome[i], ifelse(sim4.data$Response[i]==0, 0, NA))
  
  sim4.EV[sim4.TrialNum[i]] <- ifelse(sim4.data$Response[i]==1 & sim4.data$Points_earned[i] > 0, sim4.EV[sim4.TrialNum[i]]+sim4.alphapos*(sim4.data$Points_earned[i]^sim4.gamma-sim4.EV[sim4.TrialNum[i]]), ifelse(sim4.data$Response[i]==1 & sim4.data$Points_earned[i] < 0, sim4.alphaneg*((-1)*abs(sim4.data$Points_earned[i])^sim4.gamma-sim4.EV[sim4.TrialNum[i]]),  sim4.EV[sim4.TrialNum[i]]))
}

#What does Python optimization output look like?
py_optim_df <- read.csv('/Users/zeynepenkavi/Downloads/py_optim_df.csv')
head(py_optim_df)

py_optim_df %>%
  select(alpha_neg_opt, gamma_opt) %>%
  gather(key,value) %>%
  ggplot(aes(value))+
  geom_histogram()+
  theme_bw()+
  facet_wrap(~key)

py_optim_df %>%
  ggplot(aes(alpha_neg_init, alpha_neg_opt))+
  geom_point()+
  theme_bw()
```

What does a person's data look like

```{r}
real.data <- read.csv('/Users/zeynepenkavi/Downloads/ProbLearn406620.csv')

real.data <- real.data[order(real.data$Trial_type),]
```

What does the predicted data using optimized parameters look like?

```{r}
opt.data <- data.frame(Trialnumber = c(1:180), Trial_type = c(rep(1, 45), rep(2, 45), rep(3, 45), rep(4, 45)), Response=NA, Points_earned=NA, choiceprob=NA)

opt.alpha = 0.2446894
opt.beta = 0.02446153

opt.EV = c(0,0,0,0)
opt.TrialNum = opt.data$Trial_type

for(i in 1:nrow(opt.data)){
  opt.data$choiceprob[i] = exp(opt.EV[opt.TrialNum[i]]*opt.beta)/(exp(opt.EV[opt.TrialNum[i]]*opt.beta)+1)
  
  opt.data$Response[i] = rbinom(1,1,opt.data$choiceprob[i])
  
  opt.data$Points_earned[i] = ifelse(opt.data$Response[i]==1, values$outcome[i], ifelse(opt.data$Response[i]==0, 0, NA))
  
  opt.EV[opt.TrialNum[i]] <- ifelse(opt.data$Response[i]==1,opt.EV[opt.TrialNum[i]]+opt.alpha*(opt.data$Points_earned[i]-opt.EV[opt.TrialNum[i]]),opt.EV[opt.TrialNum[i]])
}
```

Even if the point estimates aren't right does it correctly distinguish between two models for a given dataset? Maybe.

```{r}
sim.double.alpha <- function(x){
  
  fn <- function(p){
    
    prob <- rep(0, nrow(x))
    err <- rep(0, nrow(x))
    ev <- c(0,0,0,0)
    
    for(i in 1:nrow(x)){
      
      
      prob[i] = exp(ev[x$Trial_type[i]]*p[1])/(exp(ev[x$Trial_type[i]]*p[1])+1) 
      
      ev[x$Trial_type[i]] <- ifelse(x$Response[i] == 1 & x$Points_earned[i]>ev[x$Trial_type[i]], ev[x$Trial_type[i]]+p[2]*(x$Points_earned[i]-ev[x$Trial_type[i]]) , ifelse(x$Response[i] == 1 & x$Points_earned[i]<ev[x$Trial_type[i]], ev[x$Trial_type[i]]+p[3]*(x$Points_earned[i]-ev[x$Trial_type[i]]), ev[x$Trial_type[i]]))
      
     
    }
   
    prob <- ifelse(prob == 1, 0.999999999999, 
                          ifelse(prob == 0, 0.000000000001, prob))
    
    #get log likelihood
    err <- x$Response * log(prob) + (1 - x$Response)*log(1-prob)
    
    #sum of negative log likelihood (to be minimized)
    sumerr <- -sum(err)
    
    return(sumerr)  
  }
  
  fit <- optim(par = c(0.5,0.5,0.5), fn, hessian = TRUE, method = "L-BFGS-B", lower = c(0, 0, 0), upper = c(Inf, 1, 1))
  
  sumerr <- fit$value
  beta <- fit$par[1]
  alphapos <- fit$par[2]
  alphaneg <- fit$par[3]
  out <- data.frame(sumerr, beta, alphapos, alphaneg)
  
  return(out) 
}

sim.double.alpha(sim.data)
```

Recover parameters the John Myles White way

```{r}
invlogit <- function(z)
{
  return(1 / (1 + exp(-z)))
}

expected.value <- function(EV_vector, condition, response, outcome, alpha){
  
  EV_vector[condition] <- ifelse(response == 1, EV_vector[condition]+alpha*(outcome-EV_vector[condition]), EV_vector[condition])
  
  return(EV_vector)
}

log.likelihood <- function(sim.data, b, alpha)
{
  ll <- 0
  
  #sink(file='/Users/zeynepenkavi/Downloads/log_likelihood.log', append=T)
  
  for (i in 1:nrow(sim.data)){
    
    if(i == 1){
      optim_EV <- c(0,0,0,0)
    }
    else{
      optim_EV <- expected.value(optim_EV, sim.data$Trial_type[i], sim.data$Response[i], sim.data$Points_earned[i], alpha)
    }
    
    u2 <- optim_EV[sim.data$Trial_type[i]] #utility of playing where sim.data$Response == 1 means playing
    u1 <- 0 #passing - the utility of passing should be 0
 
    #p <- invlogit(b * (u2 - u1))
    p <- exp(u2*b)/(exp(u2*b)+1) #same result
 
    if (sim.data$Response[i] == 1)
    {
      ll <- ll + log(p)
    }
    else
    {
      ll <- ll + log(1 - p)
    }
   #cat(c(i,optim_EV,b,alpha,p,ll,"\n")) 
  }
  #sink()
  return(ll)
}

logit.estimator <- function(sim.data)
{ 
  wrapper <- function(x) {-log.likelihood(sim.data, x[1], x[2])}
  optimization.results <- optim(c(0.5, 0.5), wrapper,  method = 'L-BFGS-B', lower = c(0,0), upper = c(Inf, 1))
  return(c(optimization.results$value,optimization.results$par))
}

#Sys.time()
logit.estimator(sim.data)
#Sys.time()
```

Code from http://www.johnmyleswhite.com/notebook/2011/06/18/speeding-up-mle-code-in-r/

```{r}
discounted.value <- function(x, t, delta)
{
  return(x * delta ^ t)
}
invlogit <- function(z)
{
  return(1 / (1 + exp(-z)))
}
n <- 100
 
choices <- data.frame(X1 = rep(1, each = n),
                      T1 = rep(0, each = n),
                      X2 = rep(3, each = n),
                      T2 = rep(1, each = n),
                      C = rep(c(0, 1), by = n / 2))

# log.likelihood <- function(choices, a, delta)
# {
#   ll <- 0
#  
#   for (i in 1:nrow(choices))
#   {
#     u2 <- discounted.value(choices[i, 'X2'], choices[i, 'T2'], delta)
#     u1 <- discounted.value(choices[i, 'X1'], choices[i, 'T1'], delta)
#  
#     p <- invlogit(a * (u2 - u1))
#  
#     if (choices[i, 'C'] == 1)
#     {
#       ll <- ll + log(p)
#     }
#     else
#     {
#       ll <- ll + log(1 - p)
#     }
#   }
#  
#   return(ll)
# }
log.likelihood <- function(choices, a, delta)
{
  u2 <- discounted.value(choices$X2, choices$T2, delta)
  u1 <- discounted.value(choices$X1, choices$T1, delta)
  p <- invlogit(a * (u2 - u1))
  likelihoods <- ifelse(choices$C == 1, p, 1 - p)
  return(sum(log(likelihoods)))
}
logit.estimator <- function(choices)
{ 
  wrapper <- function(x) {-log.likelihood(choices, x[1], x[2])}
  optimization.results <- optim(c(1, 1), wrapper, method = 'L-BFGS-B', lower = c(0, 0), upper = c(Inf, 1))
  return(c(optimization.results$value,optimization.results$par))
}
logit.estimator(choices)
```

Same thing in the way I know how to write it:

```{r}
fit.exp.bounded <- function(x){
  
  #equivalent of the MATLAB log lik fnc
  # input p for parameters where p[1] is b0 and p[2] is k
  fn <- function(p){
    
    u.ss <- x$X1 * p[2] ^ x$T1
    u.ll <- x$X2 * p[2] ^ x$T2
    
    #Calculate choice probs
    #logt: smaller beta (p[1]) larger error
    prob <- 1 / (1 + exp(p[1]*(u.ss - u.ll)))
    
    #get log likelihood
    err <- x$C * log(prob) + (1 - x$C)*log(1-prob)
    
    #sum of negative log likelihood (to be minimized)
    sumerr <- -sum(err)
    
    return(sumerr)  
  }
  
  fit <- optim(par = c(1, 1), fn, hessian = TRUE, method = "L-BFGS-B",
               lower = c(0, 0), upper = c(Inf, 1))
  
  sumerr <- fit$value
  a <- fit$par[1]
  delta <- fit$par[2]
  out <- data.frame(sumerr, a, delta)
  
  return(out) 
}

fit.exp.bounded(choices)
```

Can't get simulation to work. Go back to basics.

```{r}
sim.data <- data.frame(x=c(1,2,3,4), y=c(4,6,10,12))

summary(lm(y~x, data = sim.data))

sim.lm <- function(x){
  
  fn <- function(p){
    
    sumerr = sum(x$y - (p[1]*x$x + p[2]))^2
    
    return(sumerr)  
  }
  
  fit <- optim(par = c(1, 1), fn, hessian = TRUE)
  
  sumerr <- fit$value
  m <- fit$par[1]
  c <- fit$par[2]
  out <- data.frame(sumerr, m, c)
  
  return(out) 
}

sim.lm(sim.data)
#This kind of works but the values esp the intercept depends a lot on the starting value
```