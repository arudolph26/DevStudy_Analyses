---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values=cbbPalette) + scale_color_manual(values=cbbPalette)+theme(legend.position="bottom")

input_dir = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/rl_fits/'

fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}

process_fits = function(data){
  require(tidyverse)
  data = data %>% select(-contains("Unnamed"), -X)
  return(data)
}

rbind.all.columns <- function(x, y) {
  
  if(ncol(x) == 0 | ncol(y) == 0){
    out = plyr::rbind.fill(x, y)
  } else{
    x.diff <- setdiff(colnames(x), colnames(y))
    y.diff <- setdiff(colnames(y), colnames(x))
    x[, c(as.character(y.diff))] <- NA
    y[, c(as.character(x.diff))] <- NA
    out = rbind(x, y)
  }
  return(out)
}

pos_log <- function(column){
  col_min = min(column, na.rm=T)
  a = 1-col_min
  column = column+a
  return(log(column))
}

neg_log <- function(column){
  col_max = max(column, na.rm=T)
  column = col_max+1-column
  return(log(column))
}
```

```{r}
# fits = list.files(path=input_dir, pattern = "All")
fits = c('LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_All.csv',
         'LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_All.csv',
         'LearningParams_Fit_alpha-beta-exp_Fix_All.csv',
         'LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_All.csv',
         
         'LearningParams_Fit_alpha-beta_Fix_exp_All.csv',
         'LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_All.csv')
```

```{r eval=FALSE}
# Save plot of neglogprob distributions for all subject for each model
for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  p = data %>% 
  ggplot(aes(neglogprob))+
  geom_histogram()+
  facet_wrap(~sub_id, scales='free')
  p_name = gsub('.csv','',f)
  p_name = gsub('LearningParams','Neglogs',p_name)
  ggsave(paste0(p_name, '.jpeg'), plot=p, device = 'jpeg', path = paste0(fig_path, 'neglogs'), width = 30, height = 30, units = "in", limitsize = FALSE)
}
```

```{r}
#Look up df for AIC and BIC calculation
num_pars_df = data.frame(model = c('LearningParams_Fit_alpha_Fix_beta-exp_','LearningParams_Fit_alpha_neg_Fix_alpha_pos-beta-exp_','LearningParams_Fit_alpha_neg-alpha_pos_Fix_beta-exp_','LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg_Fix_exp_pos_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_pos_Fix_exp_neg_','LearningParams_Fit_alpha_neg-alpha_pos-exp_Fix_beta_','LearningParams_Fit_alpha_neg-alpha_pos-exp_neg_Fix_beta-exp_pos_','LearningParams_Fit_alpha_neg-alpha_pos-exp_neg-exp_pos_Fix_beta_','LearningParams_Fit_alpha_neg-alpha_pos-exp_pos_Fix_beta-exp_neg_','LearningParams_Fit_alpha_neg-beta_Fix_alpha_pos-exp_','LearningParams_Fit_alpha_neg-beta-exp_neg_Fix_alpha_pos-exp_pos_','LearningParams_Fit_alpha_neg-beta-exp_pos_Fix_alpha_pos-exp_neg_','LearningParams_Fit_alpha_neg-exp_Fix_alpha_pos-beta_','LearningParams_Fit_alpha_neg-exp_neg_Fix_alpha_pos-beta-exp_pos_','LearningParams_Fit_alpha_neg-exp_neg-exp_pos_Fix_alpha_pos-beta_','LearningParams_Fit_alpha_neg-exp_pos_Fix_alpha_pos-beta-exp_neg_','LearningParams_Fit_alpha_pos_Fix_alpha_neg-beta-exp_','LearningParams_Fit_alpha_pos-beta-exp_Fix_alpha_neg_','LearningParams_Fit_alpha_pos-beta-exp_neg_Fix_alpha_neg-exp_pos_','LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_','LearningParams_Fit_alpha_pos-beta-exp_pos_Fix_alpha_neg-exp_neg_','LearningParams_Fit_alpha_pos-exp_Fix_alpha_neg-beta_','LearningParams_Fit_alpha_pos-exp_neg_Fix_alpha_neg-beta-exp_pos_','LearningParams_Fit_alpha_pos-exp_neg-exp_pos_Fix_alpha_neg-beta_','LearningParams_Fit_alpha_pos-exp_pos_Fix_alpha_neg-beta-exp_neg_','LearningParams_Fit_alpha-beta-exp_Fix_','LearningParams_Fit_alpha-beta-exp_neg_Fix_exp_pos_','LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha-beta-exp_pos_Fix_exp_neg_','LearningParams_Fit_alpha-exp_Fix_beta_','LearningParams_Fit_alpha-exp_neg_Fix_beta-exp_pos_','LearningParams_Fit_alpha-exp_neg-exp_pos_Fix_beta_','LearningParams_Fit_alpha-exp_pos_Fix_beta-exp_neg_','LearningParams_Fit_beta-exp_Fix_alpha_','LearningParams_Fit_beta-exp_neg-exp_pos_Fix_alpha_', 'LearningParams_Fit_alpha-beta_Fix_exp_'), pars = c(1,1,2,3,4,4,5,4,3,3,4,3,2,3,3,2,2,3,2,1,3,3,4,3,2,2,3,2,2,3,4,3,2,2,3,2,2,3,2))
num_pars_df = num_pars_df %>%
  mutate(model = as.character(model))
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran 38 models where these parameters were allowed to vary for gains and losses and were either estimated for each subject or fixed for all subjects. 
Of these we are going to focus on 6 where no parameter is fixed to an arbitarary value but either fit for every subject or omitted from the model (only for exponents).

Each model was fit 50 times for each subject minimizing negative log probability. Fit quality is evaluated using BIC for each fit. 

## Model comparison

Which is the best model?

```{r}
all_sub_pars = data.frame()
best_sub_pars = data.frame()

for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  data = data %>% 
    mutate(model = as.character(model)) %>%
    left_join(num_pars_df, by='model') %>%
    mutate(AIC = 2*neglogprob+2*pars,
           BIC = 2*neglogprob+pars*log(180))
  all_sub_pars = rbind.all.columns(all_sub_pars,data.frame(data))
  data = data %>%
    group_by(sub_id) %>%
    slice(which.min(neglogprob))
  best_sub_pars = rbind.all.columns(best_sub_pars,data.frame(data))
}

all_sub_pars = all_sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)

best_sub_pars = best_sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)
```

```{r message = FALSE, warning = FALSE}
best_sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(min_col = factor(ifelse(mean_bic == min(mean_bic, na.rm=T), 1,0)),
         x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model))))))) %>%
  ggplot(aes(factor(x_axis), mean_bic))+
  geom_point(aes(col = min_col))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=min_col))+
  theme(legend.position = "none")+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_color_manual(values = c("black", "red"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))

ggsave("RL_models_comp.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

How to read model names: Parameters that are fitted are listed following the `Fit` keyword in the model name, parameters that are fixed are listed following the `Fix` keyword in the model name.  

For example in `Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_` the parameters `alpha_pos`, `beta`, `exp_neg` and `exp_pos` are fitted to data while `alpha_neg` is fixed.  


```{r}
best_sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  arrange(mean_bic)
```

So far we only looked at average BIC's. To confirm that the overall tendency for smaller BICs using `Fit_alpha-beta-exp_neg-exp_pos_Fix_` is meaningful we compare the fit indices for the best two models for each subject. 

To do this we calculate a Bayes Factors as listed in [Wagenmaakers (2007)](https://link-springer-com.stanford.idm.oclc.org/content/pdf/10.3758%2FBF03194105.pdf)

$BF_{01} = exp(\frac{BIC_1 - BIC_0}{2})$

the smaller this is the more evidence for H1. Here we take the simpler model to be H1 and the more complicated one as H0. The percentage of participants with weak evidence for H0 is:

```{r}
tmp = best_sub_pars %>%
  filter(model %in% c('Fit_alpha-beta-exp_neg-exp_pos_Fix_', 'Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_')) %>%
  select(sub_id, BIC, model) %>%
  group_by(sub_id) %>%
  mutate(model = gsub("-", "_", model)) %>%
  spread(model, BIC) %>%
  mutate(BF01 = exp((Fit_alpha_beta_exp_neg_exp_pos_Fix_ - Fit_alpha_neg_alpha_pos_beta_exp_neg_exp_pos_Fix_)/2),
         simpler_model_better = ifelse(BF01<3,1,0)) %>%
  drop_na(simpler_model_better)

sum(tmp$simpler_model_better)/nrow(tmp)*100
```

Therefore I choose the 4 parameter model with $\alpha$ $\beta$ and two $\gamma$ for positive and negative prediction errors for the remaining analyses and to search for age differences.

### Age differences in fit

Do models differ in fit by age group?   

Only for the model with two learning rates the fits are more variable for kids than for other groups.

```{r}
best_sub_pars %>%
  group_by(age_group, model) %>%
  summarise(mean_bic = mean(BIC),
            sem_bic = mean(BIC)) %>%
  mutate(x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model))))))) %>%
  ggplot(aes(factor(x_axis),mean_bic, color=age_group))+
  geom_point()+
  geom_errorbar(aes(ymin = mean_bic-sem_bic, ymax = mean_bic+sem_bic))+
  xlab("")+
  ylab("Mean BIC across subjects")+
  theme(axis.text.x = element_blank(),
        legend.title = element_blank())

ggsave("RL_models_comp_age.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

## Age differences in parameters

### Preferred model

```{r warning=FALSE, message=FALSE}
best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_")%>% 
  filter(xopt_exp_pos > (-3))%>%
  select_if(~sum(!is.na(.)) > 0) %>%
  select(-model, -pars, -neglogprob, -sub_id, -AIC, -BIC, -contains("x0")) %>%
  mutate(xopt_alpha_log = pos_log(xopt_alpha),
         xopt_beta_log = pos_log(xopt_beta),
         xopt_exp_neg_log = neg_log(xopt_exp_neg),
         xopt_exp_pos_log = pos_log(xopt_exp_pos)) %>%
  select(-xopt_alpha, -xopt_beta, -xopt_exp_neg, -xopt_exp_pos) %>%
  gather(par, value, -age_group) %>%
  mutate(par = ifelse(par == "xopt_alpha_log", "log(\u03b1)", ifelse(par == "xopt_beta_log", "log(\u03b2)", ifelse(par == "xopt_exp_neg_log", "log(\u03b3_loss)", ifelse(par == "xopt_exp_pos_log", "log(\u03b3_gain)",par))))) %>%
  ggplot(aes(value))+
  geom_density(aes(fill=age_group), color=NA, alpha=0.5)+
  facet_wrap(~par, scales='free')+
  xlab("")+
  theme(legend.position = "none")+
  ylab("")

ggsave("RL_models_par_dist.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

```{r warning=FALSE, message=FALSE}
best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_")%>% 
  select_if(~sum(!is.na(.)) > 0) %>%
  filter(xopt_exp_pos > (-3))%>%
  select(-model, -pars, -neglogprob, -sub_id, -AIC, -BIC, -contains("x0")) %>%
  mutate(xopt_alpha_log = pos_log(xopt_alpha),
         xopt_beta_log = pos_log(xopt_beta),
         xopt_exp_neg_log = neg_log(xopt_exp_neg),
         xopt_exp_pos_log = pos_log(xopt_exp_pos)) %>%
  select(-xopt_alpha, -xopt_beta, -xopt_exp_neg, -xopt_exp_pos) %>%
  gather(par, value, -age_group) %>%
  ggplot(aes(age_group, value))+
  geom_boxplot(aes(fill=age_group))+
  facet_wrap(~par, scales="free")+
  xlab("")+
  theme(legend.position = "none")
```

No difference in alpha

```{r}
summary(lm(pos_log(xopt_alpha) ~ age_group, best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

**Adults make decisions based more on EV compared to kids**

```{r}
summary(lm(pos_log(xopt_beta) ~ age_group, best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

**Adults distort negative outcomes less. A negative outcome doesn't feel as bad as it is for kids** 

```{r}
summary(lm(neg_log(xopt_exp_neg) ~ age_group, best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

No difference in distortion of positive prediction errors.

```{r}
summary(lm(pos_log(xopt_exp_pos) ~ age_group, best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

## BART correlation

Do these parameters correlate with "risk taking" as measured by BART?

```{r}
workspace_scripts = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/'

source(paste0(workspace_scripts, 'bart_data.R'))
```

```{r}
adjusted.pumps <- function(subject_data){
  subject_data_adjusted = subject_data[subject_data$exploded == 0,]
  subject_pumps <- subject_data_adjusted %>% 
    group_by(trial.num) %>%
    summarise(total_pumps = sum(finished))
  out <- data.frame(mean_adjusted_pumps = mean(subject_pumps$total_pumps))
  return(out)
}
```

Correlation between adjusted pumps and parameters:

```{r warning=FALSE, message=FALSE}
best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_") %>%
  select(sub_id, xopt_alpha, xopt_beta, xopt_exp_pos, xopt_exp_neg) %>%
  left_join(bart_data %>%
  group_by(Sub_id) %>%
  do(adjusted.pumps(.)) %>%
    rename(sub_id=Sub_id), by="sub_id") %>%
  gather(key, value, -sub_id, -mean_adjusted_pumps) %>%
  ggplot(aes(mean_adjusted_pumps, pos_log(value)))+
  geom_point()+
  facet_wrap(~key, scales='free')
```

```{r}
best_sub_pars %>%
  filter(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_") %>%
  select(sub_id, xopt_alpha, xopt_beta, xopt_exp_pos, xopt_exp_neg) %>%
  left_join(bart_data %>%
  group_by(Sub_id) %>%
  do(adjusted.pumps(.)) %>%
    rename(sub_id=Sub_id), by="sub_id") %>%
  gather(key, value, -sub_id, -mean_adjusted_pumps) %>%
  group_by(key) %>%
  drop_na() %>%
  summarise(bart_cor = cor(mean_adjusted_pumps, pos_log(value)))
```
