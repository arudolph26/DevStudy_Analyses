---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/helper_functions/ggplot_colors.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_preds_data.R')
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran models where these parameters were allowed to vary for gains and losses and were either estimated for each subject. 

Each model was fit 50 times for each subject minimizing negative log probability. Fit quality is evaluated using BIC for each fit. 

## Model comparison

Which is the best model?

```{r warning=FALSE, message=FALSE}
mean_pred_df = ave_sub_preds %>%
  group_by(model) %>%
  summarise(ave_pred_prop = mean(cor_pred_prop),
            sem_pred_prop = sem(cor_pred_prop)) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(-pars), 
            by="model") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(ave_pred_prop, x_axis)]), ordered=TRUE),
         pred_order = as.numeric(x_axis))

mean_bic_df = best_sub_pars %>%
  select(x_axis, BIC) %>%
  group_by(x_axis) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(-mean_bic, x_axis)]), ordered=TRUE), 
         bic_order = as.numeric(x_axis)) 

rank_diff_models = mean_pred_df %>%
  left_join(mean_bic_df %>% select(x_axis, bic_order), by="x_axis") %>%
  mutate(rank_diff = abs(pred_order - bic_order)) %>%
  filter(rank_diff>4)
rank_diff_models = rank_diff_models$x_axis
```

Plotting each model's prediction accuracy when predicting the quarter of left out data for each subject after fitting the model on the remaining three quarters of data.  

Model are listed from worst to best.

```{r warning=FALSE, message=FALSE}
mean_pred_df %>%
  mutate(rank_diff = factor(ifelse(x_axis %in% rank_diff_models, 1, 0))) %>%
  ggplot(aes(x_axis, ave_pred_prop))+
  geom_point(aes(col=rank_diff))+
  geom_errorbar(aes(ymin = ave_pred_prop-sem_pred_prop, ymax = ave_pred_prop+sem_pred_prop, col=rank_diff))+
  xlab("")+
  ylab("Mean prediction accuracy")+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  scale_color_manual(values=c("black", "red"))+
  theme(legend.position = "none")

ggsave("RL_models_comp_pred.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

```{r warning=FALSE, message=FALSE}
mean_bic_df %>%
  mutate(rank_diff = factor(ifelse(x_axis %in% rank_diff_models, 1, 0))) %>%
  ggplot(aes(factor(x_axis), mean_bic))+
  geom_point(aes(col=rank_diff))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=rank_diff))+
  theme(legend.position = "none",
        panel.grid=element_blank())+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  scale_color_manual(values=c("black", "red"))+
  theme(legend.position = "none")

ggsave("RL_models_comp_BIC.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

Do models differ from each other based on prediction accuracy? The worst two are significantly worse than the others. The other models do not differ from each in average prediction accuracy.

```{r}
tmp = with(ave_sub_preds, pairwise.t.test(cor_pred_prop, model), p.adj="fdr")
tmp = data.frame(tmp$p.value)
tmp = tmp %>%
  mutate(model1 = row.names(tmp)) %>%
  gather(model2, p_value, -model1) %>%
  mutate(model2 = gsub('\\.', '-', model2)) %>%
  filter(p_value <0.05) %>%
  arrange(model1)

tmp
```

Do average BIC's for each model differ from each other? No.

```{r}
tmp = with(best_sub_pars, pairwise.t.test(BIC, model))
tmp = data.frame(tmp$p.value)
tmp = tmp %>%
  mutate(model1 = row.names(tmp)) %>%
  gather(model2, p_value, -model1) %>%
  mutate(model2 = gsub('\\.', '-', model2)) %>%
  filter(p_value <0.05) %>%
  arrange(model1)

tmp
```

### Learner group difference in fit

```{r}
models = unique(ave_sub_preds$model)

out_df = data.frame(model=NA, learner_p = NA)
for(m in models){
  reg_out = summary(lm(cor_pred_prop ~ learner, ave_sub_preds %>% filter(model == m)))
  learner_p = coef(reg_out)["learner","Pr(>|t|)"]
  cur_row = data.frame(model = m, learner_p = learner_p)
  out_df = rbind(out_df, cur_row)
}
out_df = out_df[-1,]
out_df = out_df %>%
  mutate(learner_p = p.adjust(learner_p, method="fdr"))%>%
  filter(learner_p<0.05)
out_df
```

```{r}
ave_sub_preds %>%
  filter(!is.na(learner)) %>%
  group_by(learner, model) %>%
  summarise(ave_pred_prop = mean(cor_pred_prop),
            sem_pred_prop = sem(cor_pred_prop)) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(-pars), 
            by="model") %>%
  left_join(mean_pred_df %>%
              select(model, pred_order), by="model") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(pred_order, x_axis)]), ordered=TRUE)) %>%
  ggplot(aes(factor(x_axis),ave_pred_prop, color=factor(learner)))+
  geom_point()+
  geom_errorbar(aes(ymin = ave_pred_prop-sem_pred_prop, ymax = ave_pred_prop+sem_pred_prop))+
  xlab("")+
  ylab("Prediction accuracy across subjects")+
  theme(panel.grid = element_blank())+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  labs(color="Learner")+
  geom_hline(yintercept = 0.5, linetype="dashed")

ggsave("RL_models_comp_learner_pred.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

### Differing conclusions

Do the models tell the same story when checking for age differences across parameter?

Or a similar story that doesn't make sense with the behavior?

Which parameters show learner group differences in which models?

```{r}
models = unique(best_sub_pars$model)[unique(best_sub_pars$model) %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta-exp-lossave_Fix_")==FALSE]

learner_diff_df = data.frame(model = NA, par = NA, learner=NA)

for(i in 1:length(models)){
  cur_data = best_sub_pars %>% filter(model == models[i])
  cur_data = cur_data %>% select(learner, contains("xopt"))
  cur_data = cur_data[,colSums(is.na(cur_data))<nrow(cur_data)]
  cur_data = transform_remove_skew(cur_data, verbose=FALSE)
  names(cur_data)[1] = "learner"
  pars = names(cur_data)[!names(cur_data) %in% c("learner")]
  
    for(j in 1:length(pars)){
      p_vals = summary(lm(cur_data[,pars[j]] ~ cur_data[,"learner"]))$coefficients['cur_data[, "learner"]','Pr(>|t|)']
      learner_diff_df[nrow(learner_diff_df) + 1,] = list(models[i],pars[j], p_vals[1])
    }
}
learner_diff_df = learner_diff_df[-1,]
# learner_diff_df %>%
#   arrange(adult, teen)
learner_diff_df %>%
  mutate(learner_adjust = p.adjust(learner), method="fdr")%>%
  filter(learner<0.05)
```

```{r}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

fct_brdr_clrs = learner_diff_df %>%
  #excluding models with lambda
  #filter(grepl("lossave_Fix", model)==FALSE) %>%
  mutate(learner_adjust = p.adjust(learner), method="fdr")%>%
  select(model, par, learner, learner_adjust) %>%
  mutate(fct_brdr = ifelse(learner<0.5, "purple", NA),
         fct_brdr = ifelse(learner_adjust<0.5, "red", fct_brdr)) %>%
  left_join(tmp, by="model") %>%
  select(par, x_axis, fct_brdr) %>%
  mutate(par = gsub(".logTr", "", par))
```

```{r message = FALSE, warning=FALSE}
best_sub_pars %>%
  #Excluding models that do not separate between gain and loss learning
  filter(model %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta-exp-lossave_Fix_") == FALSE) %>%
  select(contains("xopt"), "sub_id", "x_axis", "learner") %>%
  gather(key, value, -learner, -sub_id, -x_axis) %>%
  mutate(par=key,
         key = ifelse(key == "xopt_alpha", "\u03b1", ifelse(key == "xopt_beta", "\u03b2", ifelse(key == "xopt_exp", "\u03b3", ifelse(key == "xopt_alpha_pos", "\u03b1_gain", ifelse(key == "xopt_alpha_neg", "\u03b1_loss",ifelse(key == "xopt_exp_pos", "\u03b3_gain", ifelse(key == "xopt_exp_neg", "\u03b3_loss", ifelse(key == "xopt_lossave", "\u03bb", NA)))))))))%>%
  group_by(learner, x_axis, key, par) %>%
  summarise(mean_val = mean(value,na.rm=T),
            sem_val = sem(value)) %>%
  left_join(fct_brdr_clrs, by=c("par", "x_axis")) %>%
  ungroup()%>%
  mutate(learner = factor(ifelse(learner == 1, "Learner", "Non-learner"), levels = c("Learner", "Non-learner"))) %>%
  # filter(grepl("\u03bb",x_axis) == FALSE) %>%
  # filter(grepl("\u03bb",key) == FALSE) %>%
    ggplot(aes(learner, mean_val))+
    geom_bar(aes(fill=learner, color=fct_brdr), stat="identity",size=1.25)+
    geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(key~x_axis, scales='free', labeller = label_wrap_gen(10))+
  ylab("")+
  xlab("")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.title=element_blank())+
  scale_color_manual(values=c("purple", "red", NA))+
  guides(color=FALSE)

ggsave("RL_par_diffs_all.jpeg", device = "jpeg", path = fig_path, width = 10, height = 8, units = "in", dpi = 450)
```

## Cor btw PEs

```{r}
input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/'

preds = list.files(path=paste0(input_path, "rl_preds/"), pattern = "All")

all_mods_preds = data.frame()
all_mods_pes = data.frame()

for(f in preds){
  data = read.csv(paste0(input_path, "rl_preds/", f))
  all_mods_preds = rbind.all.columns(all_mods_preds,data.frame(data))
  data = data %>% select(PE, sub_id, X, model)
  names(data)[which(names(data)=="PE")] = paste0("PE_",unique(data$model))
  data = data %>% select(-model)
  if(nrow(all_mods_pes)==0){
    all_mods_pes = data
  }
  else{
    all_mods_pes = all_mods_pes %>%
      left_join(data, by=c("sub_id", "X"))
  }
}

all_mods_pes = all_mods_pes %>%
  select(X, sub_id, everything())
```

```{r}
mods = grep("PE", names(all_mods_pes), value=T)
cor_df = data.frame(sub_id = NA, var1 = NA, var2=NA, cor_val=NA)
for(i in 1:(length(mods) - 1) ){
  rem_mods = mods[-c(1:i)]
  
  for(j in 1:length(rem_mods)){
    cur_data = all_mods_pes %>% select(sub_id, mods[i], rem_mods[j])
    
    cur_cor_df = cur_data %>% 
      group_by(sub_id) %>% 
      do(data.frame(Cor=t(cor(.[,2], .[,3], use="pairwise")))) %>% 
      mutate(var1 = mods[i],
             var2 = rem_mods[j]) %>%
      select(sub_id, var1, var2, everything())
    
    names(cur_cor_df) = c("sub_id", "var1", "var2", "cor_val")
    
    cor_df = rbind(cor_df, data.frame(cur_cor_df))
    
  }
} 

cor_df = cor_df[-1,]
```

Distributuons of correlations between PE's generated by different models for each subject (sorted by the median correlation for model pair)

```{r warning=FALSE, message=FALSE}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

cor_df %>%
  mutate(var1 = gsub("PE_Preds_", "", var1),
         var2 = gsub("PE_Preds_", "", var2)) %>%
  #Exclude models that don't distinguish gain and loss learning at all
  filter(var1 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  filter(var2 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  #Exclude models with lambda
  filter(grepl("lossave_Fix", var1) == FALSE) %>%
  filter(grepl("lossave_Fix", var2) == FALSE) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_type = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_type) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T)) %>%
  ggplot(aes(reorder(cor_type,med_cor_val), cor_val ))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle=90),
        legend.position = "none")+
  ylab("Correlation between PEs")+
  xlab("")+
  coord_flip()

ggsave("PE_corrs.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

Create average PE's for all subjects using the three models that have highly correlated PEs. These three models are also the best fitting models. The PEs created in this way will be used for imaging analyses.

```{r}
get_ave_pe = function(sub_data){
  sub_data = sub_data %>%
  select(X, sub_id, "PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")
  
  drop_cols = c()
  
  for(col in c("PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")){
    if(sum(is.na(sub_data[,col]))==180){
      drop_cols = c(drop_cols, col)
    }
  }
  
  rem_cols = setdiff(c("PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_"), drop_cols)
  
  sub_data = sub_data %>%
    mutate(ave_PE = rowSums(select(., rem_cols))/length(rem_cols)) %>%
    select(X, sub_id, ave_PE) %>%
  filter(is.na(ave_PE) == FALSE)
  
  return(sub_data)
}

ave_pes = all_mods_pes %>%
  group_by(sub_id) %>%
  do(get_ave_pe(.))

# write.csv(ave_pes, '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_1/ave_pes.csv', row.names=FALSE)
```

## Cor btw EVs

```{r}
all_mods_evs = data.frame()

for(f in preds){
  data = read.csv(paste0(input_path, "rl_preds/", f))
  data = data %>% select(EV, sub_id, X, model)
  names(data)[which(names(data)=="EV")] = paste0("EV_",unique(data$model))
  data = data %>% select(-model)
  if(nrow(all_mods_evs)==0){
    all_mods_evs = data
  }
  else{
    all_mods_evs = all_mods_evs %>%
      left_join(data, by=c("sub_id", "X"))
  }
}

all_mods_evs = all_mods_evs %>%
  select(X, sub_id, everything())
```

```{r}
mods = grep("EV", names(all_mods_evs), value=T)
cor_df = data.frame(sub_id = NA, var1 = NA, var2=NA, cor_val=NA)
for(i in 1:(length(mods) - 1) ){
  rem_mods = mods[-c(1:i)]
  
  for(j in 1:length(rem_mods)){
    cur_data = all_mods_evs %>% select(sub_id, mods[i], rem_mods[j])
    
    cur_cor_df = cur_data %>% 
      group_by(sub_id) %>% 
      do(data.frame(Cor=t(cor(.[,2], .[,3], use="pairwise")))) %>% 
      mutate(var1 = mods[i],
             var2 = rem_mods[j]) %>%
      select(sub_id, var1, var2, everything())
    
    names(cur_cor_df) = c("sub_id", "var1", "var2", "cor_val")
    
    cor_df = rbind(cor_df, data.frame(cur_cor_df))
    
  }
} 

cor_df = cor_df[-1,]
```

```{r}
cor_df %>%
  mutate(var1 = gsub("EV_Preds_", "", var1),
         var2 = gsub("EV_Preds_", "", var2)) %>%
  #Exclude models that don't distinguish gain and loss learning at all
  filter(var1 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  filter(var2 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  #Exclude models with lambda
  filter(grepl("lossave_Fix", var1) == FALSE) %>%
  filter(grepl("lossave_Fix", var2) == FALSE) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_type = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_type) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T)) %>%
  ggplot(aes(reorder(cor_type,med_cor_val), cor_val ))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle=90),
        legend.position = "none")+
  ylab("Correlation between EVs")+
  xlab("")+
  coord_flip()

```

```{r}
get_ave_ev = function(sub_data){
  sub_data = sub_data %>%
  select(X, sub_id, "EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","EV_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")
  
  drop_cols = c()
  
  for(col in c("EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","EV_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")){
    if(sum(is.na(sub_data[,col]))==180){
      drop_cols = c(drop_cols, col)
    }
  }
  
  rem_cols = setdiff(c("EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","EV_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_"), drop_cols)
  
  sub_data = sub_data %>%
    mutate(ave_EV = rowSums(select(., rem_cols))/length(rem_cols)) %>%
    select(X, sub_id, ave_EV) %>%
  filter(is.na(ave_EV) == FALSE)
  
  return(sub_data)
}

ave_evs = all_mods_evs %>%
  group_by(sub_id) %>%
  do(get_ave_ev(.))

# write.csv(ave_evs, '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_1/ave_evs.csv', row.names=FALSE)
```
