---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values=cbbPalette) + scale_color_manual(values=cbbPalette)+theme(legend.position="bottom")

input_dir = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/rl_fits/'

fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}

process_fits = function(data){
  require(tidyverse)
  data = data %>% select(-contains("Unnamed"), -X)
  return(data)
}

rbind.all.columns <- function(x, y) {
  
  if(ncol(x) == 0 | ncol(y) == 0){
    out = plyr::rbind.fill(x, y)
  } else{
    x.diff <- setdiff(colnames(x), colnames(y))
    y.diff <- setdiff(colnames(y), colnames(x))
    x[, c(as.character(y.diff))] <- NA
    y[, c(as.character(x.diff))] <- NA
    out = rbind(x, y)
  }
  return(out)
}

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')
```

```{r}
fits = list.files(path=input_dir, pattern = "All")
old_fits = c('LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_All.csv',
         'LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_All.csv',
         'LearningParams_Fit_alpha-beta-exp_Fix_All.csv',
         'LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_All.csv',
         'LearningParams_Fit_alpha-beta_Fix_exp_All.csv',
         'LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_All.csv')
fits=fits[fits %in% old_fits == FALSE]
```

```{r eval=FALSE}
# Save plot of neglogprob distributions for all subject for each model
for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  p = data %>% 
  ggplot(aes(neglogprob))+
  geom_histogram()+
  facet_wrap(~sub_id, scales='free')
  p_name = gsub('.csv','',f)
  p_name = gsub('LearningParams','Neglogs',p_name)
  ggsave(paste0(p_name, '.jpeg'), plot=p, device = 'jpeg', path = paste0(fig_path, 'neglogs'), width = 30, height = 30, units = "in", limitsize = FALSE)
}
```

```{r}
#Look up df for AIC and BIC calculation
# num_pars_df = data.frame(model = c('LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha-beta-exp_Fix_','LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha-beta_Fix_exp_','LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_'), pars = c(3,4,5,2,4,2))

num_pars_df = data.frame(model = c('LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp-lossave_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos-lossave_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp-lossave_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-lossave_Fix_exp_','LearningParams_Fit_alpha-beta_Fix_exp-lossave_','LearningParams_Fit_alpha-beta-exp_Fix_lossave_','LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_','LearningParams_Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_','LearningParams_Fit_alpha-beta-exp-lossave_Fix_','LearningParams_Fit_alpha-beta-lossave_Fix_exp_'), pars = c(3,4,5,6,5,4,2,3,4,5,4,3),x_axis = c("\u03b1_gain, \u03b1_loss, \u03b2", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss, \u03bb", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3, \u03bb", "\u03b1_gain, \u03b1_loss, \u03b2, \u03bb", "\u03b1, \u03b2", "\u03b1, \u03b2, \u03b3", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss, \u03bb", "\u03b1, \u03b2, \u03b3, \u03bb", "\u03b1, \u03b2, \u03bb"))

num_pars_df = num_pars_df %>%
  mutate(model = as.character(model),
         x_axis = as.character(x_axis))
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran models where these parameters were allowed to vary for gains and losses and were either estimated for each subject. 

Each model was fit 50 times for each subject minimizing negative log probability. Fit quality is evaluated using BIC for each fit. 

## Model comparison

Which is the best model?

Read in bounded parameter estimates.

```{r}
all_sub_pars = data.frame()
best_sub_pars = data.frame()

for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  data = data %>% 
    mutate(model = as.character(model)) %>%
    left_join(num_pars_df, by='model') %>%
    mutate(AIC = 2*neglogprob+2*pars,
           BIC = 2*neglogprob+pars*log(180))
  all_sub_pars = rbind.all.columns(all_sub_pars,data.frame(data))
  data = data %>%
    group_by(sub_id) %>%
    slice(which.min(neglogprob))
  best_sub_pars = rbind.all.columns(best_sub_pars,data.frame(data))
}

all_sub_pars = all_sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)

best_sub_pars = best_sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)
```

Plotting the best model/model with lowest average BIC for the bounded estimations.

```{r message = FALSE, warning = FALSE}
best_sub_pars %>%
  select(x_axis, BIC) %>%
  group_by(x_axis) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(min_col = factor(ifelse(mean_bic == min(mean_bic, na.rm=T), 1,0))) %>%
  ggplot(aes(factor(x_axis), mean_bic))+
  geom_point(aes(col = min_col))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=min_col))+
  theme(legend.position = "none",
        panel.grid=element_blank())+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_color_manual(values = c("black", "red"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))

ggsave("RL_models_comp.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

Qualitatively three types of models based on average fit:

Worst: Not distinguishing between gains/losses in any way   
- \alpha, \beta 
- \alpha, \beta, \gamma

Medium: Account for difference between gain/loss in one parameter
- \alpha_gain, \alpha_loss, \beta   
- \alpha_gain, \alpha_loss, \beta, \lambda  
- \alpha, \beta, \gamma, \lambda   
- \alpha, \beta, \lambda

Best:
- \alpha_gain, \alpha_loss, \beta, \gamma
- \alpha_gain, \alpha_loss, \beta, \gamma_gain, \gamma_loss
- \alpha_gain, \alpha_loss, \beta, \gamma_gain, \gamma_loss, \lambda
- \alpha_gain, \alpha_loss, \beta, \gamma, \lambda
- \alpha, \beta, \gamma_gain, \gamma_loss
- \alpha, \beta, \gamma_gain, \gamma_loss, \lambda

```{r}
best_sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  arrange(mean_bic)
```

So far we only looked at average BIC's. To confirm that the overall tendency for smaller BICs in the best model is meaningful we compare the fit indices for the best two models for each subject.   

To do this we calculate a Bayes Factors as listed in [Wagenmaakers (2007)](https://link-springer-com.stanford.idm.oclc.org/content/pdf/10.3758%2FBF03194105.pdf)

$BF_{01} = exp(\frac{BIC_1 - BIC_0}{2})$

the smaller this is the more evidence for H1 (the 4 parameter model with a single learning rate and two exponents). Here we take the simpler model to be H1 and the more complicated one as H0 (the 5 parameter model with two learning rates and two exponents). $BF_{01}$ < 3 suggests weak evidence for H0 and therefore a preference for H1.  

The percentage of participants with weak evidence for H0 and for whom therefore the simpler model H1 would be preferred is:

```{r}
tmp = best_sub_pars %>%
  filter(model %in% c('Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_', 'Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_')) %>%
  # filter(model %in% c('Fit_alpha-beta-exp_neg-exp_pos_Fix_', 'Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_')) %>%
  select(sub_id, BIC, model) %>%
  group_by(sub_id) %>%
  mutate(model = gsub("-", "_", model)) %>%
  spread(model, BIC) %>%
  mutate(BF01 = exp((Fit_alpha_beta_exp_neg_exp_pos_Fix_lossave_ - Fit_alpha_neg_alpha_pos_beta_exp_Fix_lossave_)/2),
         simpler_model_better = ifelse(BF01<3,1,0)) %>%
  drop_na(simpler_model_better)

sum(tmp$simpler_model_better)/nrow(tmp)*100
```

The 'best' model using bounded estimation is the best for 76.6% of the subjects.

### Age differences in fit

Do models differ in fit by age group?   

Only for the model with two learning rates the fits are more variable for kids than for other groups.

```{r}
best_sub_pars %>%
  group_by(age_group, x_axis) %>%
  summarise(mean_bic = mean(BIC),
            sem_bic = mean(BIC)) %>%
  ggplot(aes(factor(x_axis),mean_bic, color=age_group))+
  geom_point()+
  geom_errorbar(aes(ymin = mean_bic-sem_bic, ymax = mean_bic+sem_bic))+
  xlab("")+
  ylab("Mean BIC across subjects")+
  theme(axis.text.x = element_blank(),
        legend.title = element_blank(),
        panel.grid = element_blank())

ggsave("RL_models_comp_age.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

### Comparison of BIC distributions

Bayes Factor check above suggested that for a majority of the subjects the 4 parameter model with the lowest BIC is supported more than the next best model. But are do these BIC distributions even have different means? The simplest model has a higher mean than the rest but the others do not differ from each other.

```{r}
summary(lm(BIC ~ model, best_sub_pars))
```

### Differing conclusions

Do the models tell the same story when checking for age differences across parameter?

Or a similar story that doesn't make sense with the behavior?

Which parameters show age differences in which models?

```{r}
models = unique(best_sub_pars$model)[unique(best_sub_pars$model) %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_")==FALSE]

age_diff_df = data.frame(model = NA, par = NA, teen=NA, adult=NA)

for(i in 1:length(models)){
  cur_data = best_sub_pars %>% filter(model == models[i])
  cur_data = cur_data %>% select(age_group, contains("xopt"))
  cur_data = cur_data[,colSums(is.na(cur_data))<nrow(cur_data)]
  cur_data = transform_remove_skew(cur_data)
  names(cur_data)[1] = "age_group"
  pars = names(cur_data)[!names(cur_data) %in% c("age_group")]
  
    for(j in 1:length(pars)){
      p_vals = summary(lm(cur_data[,pars[j]] ~ cur_data[,"age_group"]))$coefficients[c(2,3),4]
      age_diff_df[nrow(age_diff_df) + 1,] = list(models[i],pars[j], p_vals[1], p_vals[2])
    }
}
age_diff_df = age_diff_df[-1,]
# age_diff_df %>%
#   arrange(adult, teen)
age_diff_df %>%
  mutate(adult_adjust = p.adjust(adult), method="fdr")%>%
  filter(adult<0.05)
```

```{r}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

fct_brdr_clrs = age_diff_df %>%
  #excluding models with lambda
  filter(grepl("lossave_Fix", model)==FALSE) %>%
  mutate(adult_adjust = p.adjust(adult), method="fdr")%>%
  select(model, par, adult, adult_adjust) %>%
  mutate(fct_brdr = ifelse(adult<0.5, "purple", NA),
         fct_brdr = ifelse(adult_adjust<0.5, "red", fct_brdr)) %>%
  left_join(tmp, by="model") %>%
  select(par, x_axis, fct_brdr)

fct_brdr_clrs %>%
  filter(is.na(fct_brdr)==FALSE)
```

```{r message = FALSE, warning=FALSE}
best_sub_pars %>%
  #Excluding models that do not separate between gain and loss learning
  filter(model %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  select(contains("xopt"), "sub_id", "x_axis", "age_group") %>%
  gather(key, value, -age_group, -sub_id, -x_axis) %>%
  mutate(par=key,
         key = ifelse(key == "xopt_alpha", "\u03b1", ifelse(key == "xopt_beta", "\u03b2", ifelse(key == "xopt_exp", "\u03b3", ifelse(key == "xopt_alpha_pos", "\u03b1_gain", ifelse(key == "xopt_alpha_neg", "\u03b1_loss",ifelse(key == "xopt_exp_pos", "\u03b3_gain", ifelse(key == "xopt_exp_neg", "\u03b3_loss", ifelse(key == "xopt_lossave", "\u03bb", NA)))))))))%>%
  group_by(age_group, x_axis, key, par) %>%
  summarise(mean_val = mean(value,na.rm=T),
            sem_val = sem(value)) %>%
  left_join(fct_brdr_clrs, by=c("par", "x_axis")) %>%
  filter(grepl("\u03bb",x_axis) == FALSE) %>%
  filter(grepl("\u03bb",key) == FALSE) %>%
    ggplot(aes(age_group, mean_val))+
    geom_bar(aes(fill=age_group, color=fct_brdr), stat="identity",size=1.25)+
    geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(key~x_axis, scales='free')+
  ylab("")+
  xlab("")+
  theme(panel.grid = element_blank(),
        legend.position = "none")+
  scale_color_manual(values=c("purple", "red", NA))

ggsave("RL_par_diffs_nolamda.jpeg", device = "jpeg", path = fig_path, width = 10, height = 5.6, units = "in", dpi = 450)
```

**So using RL models to describe behavior in this task**

**- We have models that don't capture the behavioral group difference we are interested in (i.e. the difference between kids and adults in learning from losses)**

## Cor btw PEs

```{r}
input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/'

preds = list.files(path=paste0(input_path, "rl_preds/"), pattern = "All")

all_mods_preds = data.frame()
all_mods_pes = data.frame()

for(f in preds){
  data = read.csv(paste0(input_path, "rl_preds/", f))
  all_mods_preds = rbind.all.columns(all_mods_preds,data.frame(data))
  data = data %>% select(PE, sub_id, X, model)
  names(data)[which(names(data)=="PE")] = paste0("PE_",unique(data$model))
  data = data %>% select(-model)
  if(nrow(all_mods_pes)==0){
    all_mods_pes = data
  }
  else{
    all_mods_pes = all_mods_pes %>%
      left_join(data, by=c("sub_id", "X"))
  }
}

all_mods_pes = all_mods_pes %>%
  select(X, sub_id, everything())
```

```{r}
mods = grep("PE", names(all_mods_pes), value=T)
cor_df = data.frame(sub_id = NA, var1 = NA, var2=NA, cor_val=NA)
for(i in 1:(length(mods) - 1) ){
  rem_mods = mods[-c(1:i)]
  
  for(j in 1:length(rem_mods)){
    cur_data = all_mods_pes %>% select(sub_id, mods[i], rem_mods[j])
    
    cur_cor_df = cur_data %>% 
      group_by(sub_id) %>% 
      do(data.frame(Cor=t(cor(.[,2], .[,3], use="pairwise")))) %>% 
      mutate(var1 = mods[i],
             var2 = rem_mods[j]) %>%
      select(sub_id, var1, var2, everything())
    
    names(cur_cor_df) = c("sub_id", "var1", "var2", "cor_val")
    
    cor_df = rbind(cor_df, data.frame(cur_cor_df))
    
  }
} 

cor_df = cor_df[-1,]
```

Distributuons of correlations between PE's generated by different models for each subject (sorted by the median correlation for model pair)

```{r warning=FALSE, message=FALSE}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

cor_df %>%
  mutate(var1 = gsub("PE_Preds_", "", var1),
         var2 = gsub("PE_Preds_", "", var2)) %>%
  #Exclude models that don't distinguish gain and loss learning at all
  filter(var1 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  filter(var2 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  #Exclude models with lambda
  filter(grepl("lossave_Fix", var1) == FALSE) %>%
  filter(grepl("lossave_Fix", var2) == FALSE) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_type = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_type) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T)) %>%
  ggplot(aes(reorder(cor_type,med_cor_val), cor_val ))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle=90),
        legend.position = "none")+
  ylab("Correlation between PEs")+
  xlab("")+
  coord_flip()

ggsave("PE_corrs.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

Create average PE's for all subjects using the three models that have highly correlated PEs. These three models are also the best fitting models. The PEs created in this way will be used for imaging analyses.

```{r}
get_ave_pe = function(sub_data){
  sub_data = sub_data %>%
  select(X, sub_id, "PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")
  
  drop_cols = c()
  
  for(col in c("PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")){
    if(sum(is.na(sub_data[,col]))==180){
      drop_cols = c(drop_cols, col)
    }
  }
  
  rem_cols = setdiff(c("PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_"), drop_cols)
  
  sub_data = sub_data %>%
    mutate(ave_PE = rowSums(select(., rem_cols))/length(rem_cols)) %>%
    select(X, sub_id, ave_PE) %>%
  filter(is.na(ave_PE) == FALSE)
  
  return(sub_data)
}

ave_pes = all_mods_pes %>%
  group_by(sub_id) %>%
  do(get_ave_pe(.))

# write.csv(ave_pes, '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_1/ave_pes.csv', row.names=FALSE)
```

## Cor btw EVs

```{r}
all_mods_evs = data.frame()

for(f in preds){
  data = read.csv(paste0(input_path, "rl_preds/", f))
  data = data %>% select(EV, sub_id, X, model)
  names(data)[which(names(data)=="EV")] = paste0("EV_",unique(data$model))
  data = data %>% select(-model)
  if(nrow(all_mods_evs)==0){
    all_mods_evs = data
  }
  else{
    all_mods_evs = all_mods_evs %>%
      left_join(data, by=c("sub_id", "X"))
  }
}

all_mods_evs = all_mods_evs %>%
  select(X, sub_id, everything())
```

```{r}
mods = grep("EV", names(all_mods_evs), value=T)
cor_df = data.frame(sub_id = NA, var1 = NA, var2=NA, cor_val=NA)
for(i in 1:(length(mods) - 1) ){
  rem_mods = mods[-c(1:i)]
  
  for(j in 1:length(rem_mods)){
    cur_data = all_mods_evs %>% select(sub_id, mods[i], rem_mods[j])
    
    cur_cor_df = cur_data %>% 
      group_by(sub_id) %>% 
      do(data.frame(Cor=t(cor(.[,2], .[,3], use="pairwise")))) %>% 
      mutate(var1 = mods[i],
             var2 = rem_mods[j]) %>%
      select(sub_id, var1, var2, everything())
    
    names(cur_cor_df) = c("sub_id", "var1", "var2", "cor_val")
    
    cor_df = rbind(cor_df, data.frame(cur_cor_df))
    
  }
} 

cor_df = cor_df[-1,]
```

```{r}
cor_df %>%
  mutate(var1 = gsub("EV_Preds_", "", var1),
         var2 = gsub("EV_Preds_", "", var2)) %>%
  #Exclude models that don't distinguish gain and loss learning at all
  filter(var1 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  filter(var2 %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta_Fix_exp-lossave_") == FALSE) %>%
  #Exclude models with lambda
  filter(grepl("lossave_Fix", var1) == FALSE) %>%
  filter(grepl("lossave_Fix", var2) == FALSE) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_type = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_type) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T)) %>%
  ggplot(aes(reorder(cor_type,med_cor_val), cor_val ))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle=90),
        legend.position = "none")+
  ylab("Correlation between EVs")+
  xlab("")+
  coord_flip()

```

```{r}
get_ave_ev = function(sub_data){
  sub_data = sub_data %>%
  select(X, sub_id, "EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","EV_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")
  
  drop_cols = c()
  
  for(col in c("EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","EV_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_")){
    if(sum(is.na(sub_data[,col]))==180){
      drop_cols = c(drop_cols, col)
    }
  }
  
  rem_cols = setdiff(c("EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_","EV_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_","EV_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_"), drop_cols)
  
  sub_data = sub_data %>%
    mutate(ave_EV = rowSums(select(., rem_cols))/length(rem_cols)) %>%
    select(X, sub_id, ave_EV) %>%
  filter(is.na(ave_EV) == FALSE)
  
  return(sub_data)
}

ave_evs = all_mods_evs %>%
  group_by(sub_id) %>%
  do(get_ave_ev(.))

# write.csv(ave_evs, '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_1/ave_evs.csv', row.names=FALSE)
```

## Imaging 

```{r message = FALSE, warning=FALSE, eval=FALSE}
m1_roi_mzvals = read.csv(paste0(input_dir, 'm1_roi_mzvals.csv'))

m1_roi_mzvals %>%
  select(-X) %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult"))) %>%
  left_join(ez_fits_correctinc %>%
              filter(machine == "+5/-495" & par == "drift") %>%
              select(sub_id, par, machine, value), by = "sub_id") %>%
  group_by(roi_name, age_group) %>%
  summarise(cor_mzval_drift = cor(m_zvals, value, use="complete.obs")) %>%
  # filter(age_group != "teen") %>%
  ggplot(aes(roi_name, abs(cor_mzval_drift), fill=age_group))+
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  xlab("")+
  # scale_fill_manual(values=c("#E69F00","#009E73"))+
  theme(legend.title=element_blank(),
        panel.grid = element_blank()) +
  ylab("Correlation between drift rate and activity in ROI")
```

```{r warning=FALSE, message=FALSE, eval=FALSE}
m1_roi_mzvals %>%
  select(-X) %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult"))) %>%
  left_join(ez_fits_correctinc %>%
              filter(machine == "+5/-495" & par == "drift") %>%
              select(sub_id, par, machine, value), by = "sub_id") %>%
  group_by(roi_name, age_group) %>%
  filter(age_group != "teen") %>%
  ggplot(aes(value, m_zvals))+
  # geom_point()+
  geom_smooth(method="lm", se=FALSE, aes(color=age_group))+
  geom_smooth(method="lm", se=FALSE, color="black", linetype="dashed")+
  facet_wrap(~roi_name, nrow=2)+
  theme(legend.title = element_blank(),
        panel.grid = element_blank())+
  xlab("drift rate")+
  ylab("Mean z-value in ROI")+
  xlim(-1.5, 1.5)+
  scale_color_manual(values=c("#E69F00","#009E73"))
```


```{r eval=FALSE}
tmp = m1_roi_mzvals %>%
  select(-X) %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult"))) %>%
  left_join(ez_fits_correctinc %>%
              filter(machine == "+5/-495" & par == "drift") %>%
              select(sub_id, par, machine, value), by = "sub_id") %>%
  filter(age_group != "teen") %>%
  mutate(age_group = factor(age_group, levels = c("kid","adult")))

summary(lm(m_zvals ~ value*age_group, tmp %>% filter(roi_name=="ACC")))

summary(lm(m_zvals ~ value*age_group, tmp %>% filter(roi_name=="RaIns")))

summary(lm(m_zvals ~ value*age_group, tmp %>% filter(roi_name=="PCC")))

contrasts(tmp$age_group)
```