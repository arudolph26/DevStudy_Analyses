---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/helper_functions/ggplot_colors.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_preds_data.R')

library(lme4)
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran models where these parameters were allowed to vary for gains and losses and were either estimated for each subject. 

Each model was fit 50 times for each subject minimizing negative log probability. 

## Model comparison

Fit quality can be assesed in several ways. We chose to do this by fitting the models on %75 of each subject' data and then predicting the left out 25% with the best fitting parameters for each model. The predicted choice accuracy of this left out data was out metric to determine the best model.  

```{r warning=FALSE, message=FALSE}
mean_pred_df = ave_sub_preds %>%
  group_by(model) %>%
  summarise(ave_pred_prop = mean(cor_pred_prop),
            sem_pred_prop = sem(cor_pred_prop)) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(-pars), 
            by="model") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(ave_pred_prop, x_axis)]), ordered=TRUE),
         pred_order = as.numeric(x_axis))
```

Plotting each model's prediction accuracy when predicting the quarter of left out data for each subject after fitting the model on the remaining three quarters of data.  

Model are listed from worst to best.

```{r warning=FALSE, message=FALSE}
mean_pred_df %>%
  ggplot(aes(x_axis, ave_pred_prop))+
  geom_point()+
  geom_errorbar(aes(ymin = ave_pred_prop-sem_pred_prop, ymax = ave_pred_prop+sem_pred_prop))+
  xlab("")+
  ylab("Mean prediction accuracy")+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  theme(legend.position = "none",
        panel.grid = element_blank())

ggsave("RL_models_comp_pred.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

Do models differ from each other based on prediction accuracy? The worst two are significantly worse than the others. The other models do not differ from each in average prediction accuracy.

```{r}
tmp = with(ave_sub_preds, pairwise.t.test(cor_pred_prop, model), p.adj="fdr")
tmp = data.frame(tmp$p.value)
tmp = tmp %>%
  mutate(model1 = row.names(tmp)) %>%
  gather(model2, p_value, -model1) %>%
  mutate(model2 = gsub('\\.', '-', model2)) %>%
  filter(p_value <0.05) %>%
  arrange(model1)

tmp
```

### Learner group difference in fit

Are there any models that show a difference between the learner groups that survives multiple comparison? Yes.

```{r}
models = unique(ave_sub_preds$model)

out_df = data.frame(model=NA, learner_p = NA)
for(m in models){
  reg_out = summary(lm(cor_pred_prop ~ learner, ave_sub_preds %>% filter(model == m)))
  learner_p = coef(reg_out)["learner","Pr(>|t|)"]
  cur_row = data.frame(model = m, learner_p = learner_p)
  out_df = rbind(out_df, cur_row)
}
out_df = out_df[-1,]
out_df = out_df %>%
  mutate(learner_p = p.adjust(learner_p, method="fdr"))%>%
  filter(learner_p<0.05)
out_df
```

What direction are the learner group differences in? Prediction accuracies are consistently higher for learners compared to non-learners.

```{r}
ave_sub_preds %>%
  filter(!is.na(learner)) %>%
  group_by(learner, model) %>%
  summarise(ave_pred_prop = mean(cor_pred_prop),
            sem_pred_prop = sem(cor_pred_prop)) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(-pars), 
            by="model") %>%
  left_join(mean_pred_df %>%
              select(model, pred_order), by="model") %>%
  ungroup()%>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(pred_order, x_axis)]), ordered=TRUE),
         learner = ifelse(learner == 1, "learner", "non-learner")) %>%
  ggplot(aes(factor(x_axis),ave_pred_prop, color=factor(learner)))+
  geom_point()+
  geom_errorbar(aes(ymin = ave_pred_prop-sem_pred_prop, ymax = ave_pred_prop+sem_pred_prop))+
  xlab("")+
  ylab("Prediction accuracy across subjects")+
  theme(panel.grid = element_blank(),
        legend.title = element_blank())+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  geom_hline(yintercept = 0.5, linetype="dashed")

ggsave("RL_models_comp_learner_pred.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

The model type by learner type interaction is not significant but the main effects are. Models that do not distinguish between gains and losses in either the learning rates or the the value exponents have lower prediction accuracies. Non-learners also have lower predictiona accuracies compared to learners.

```{r}
tmp = ave_sub_preds %>%
  mutate(no_gain_loss = ifelse(model %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta-exp-lossave_Fix_", "Fit_alpha-beta_Fix_exp-lossave_"), 1, 0)) 

m = lmer(cor_pred_prop ~ no_gain_loss*factor(learner) + (1|sub_id), tmp)
summary(m)
```

```{r}
confint.merMod(m)
```

Comparing the prediction accuracies of the models that do distinguish between gains and losses in either the learning rates or the exponents we do not find any differences among them. The learner group main effect shows up in this model as well.

```{r}
summary(lm(cor_pred_prop ~ model+learner, tmp %>% filter(no_gain_loss == 0)))
```

## Differing conclusions

Comparing prediction accuracy of left out data eliminates only 3 of 12 models but does not allow us to distinguish between the remaining ones. Since the literature does not point to one model over the others strongly either we compare the parameter estimates from each of the remaining 9 models between learners and non-learners to see whether they tell the same story on the behavioral difference.

Below is a list of the parameters that show significant differences that survive multiple comparisons between the learner groups. 

```{r}
models = unique(best_sub_pars$model)[unique(best_sub_pars$model) %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta-exp-lossave_Fix_", "Fit_alpha-beta_Fix_exp-lossave_")==FALSE]

learner_diff_df = data.frame(model = NA, par = NA, learner=NA)

for(i in 1:length(models)){
  cur_data = best_sub_pars %>% filter(model == models[i])
  cur_data = cur_data %>% select(learner, contains("xopt"))
  cur_data = cur_data[,colSums(is.na(cur_data))<nrow(cur_data)]
  cur_data = transform_remove_skew(cur_data, verbose=FALSE)
  names(cur_data)[1] = "learner"
  pars = names(cur_data)[!names(cur_data) %in% c("learner")]
  
    for(j in 1:length(pars)){
      p_vals = summary(lm(cur_data[,pars[j]] ~ cur_data[,"learner"]))$coefficients['cur_data[, "learner"]','Pr(>|t|)']
      learner_diff_df[nrow(learner_diff_df) + 1,] = list(models[i],pars[j], p_vals[1])
    }
}
learner_diff_df = learner_diff_df[-1,]

learner_diff_df %>%
  mutate(learner_adjust = p.adjust(learner), method="fdr")%>%
  filter(learner<0.05)
```

```{r}
#Helper functions for plot coloring
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

fct_brdr_clrs = learner_diff_df %>%
  mutate(learner_adjust = p.adjust(learner), method="fdr")%>%
  select(model, par, learner, learner_adjust) %>%
  mutate(fct_brdr = ifelse(learner<0.5, "purple", NA),
         fct_brdr = ifelse(learner_adjust<0.5, "red", fct_brdr)) %>%
  left_join(tmp, by="model") %>%
  select(par, x_axis, fct_brdr) %>%
  mutate(par = gsub(".logTr", "", par))
```

```{r}
#Create df with transformed parameter estimates for plotting since the transformed distributions were used in checking for group differences.

log_lookup = best_sub_pars %>% 
  select(sub_id, model, contains("xopt"), x_axis, learner) %>%
  gather(key, value, -sub_id, -model, -x_axis, -learner) %>%
  group_by(model, key) %>%
  summarise(skw = skew(value)) %>%
  filter(abs(skw)>1) %>%
  arrange(model)

best_sub_pars_trans = data.frame()

for(i in 1:length(models)){
  cur_model = models[i]
  cur_model_data = best_sub_pars %>% filter(model == cur_model)
  cur_skw_vars = log_lookup %>% filter(model == cur_model)
  
  for(j in 1:nrow(cur_skw_vars)){
    cur_model_data[,cur_skw_vars$key[j]] = pos_log(cur_model_data[,cur_skw_vars$key[j]])
  }
  
  best_sub_pars_trans = rbind.all.columns(best_sub_pars_trans, cur_model_data)
}
```

The differences listed above are visualized below. Red outlines survive multiple comparisons. Purple outlines are significant in isolation but don't survive multiple comparisons.

```{r message = FALSE, warning=FALSE}
best_sub_pars_trans %>%
  select(contains("xopt"), "sub_id", "x_axis", "learner") %>%
  gather(key, value, -learner, -sub_id, -x_axis) %>%
  mutate(par=key,
         key = ifelse(key == "xopt_alpha", "\u03b1", ifelse(key == "xopt_beta", "\u03b2", ifelse(key == "xopt_exp", "\u03b3", ifelse(key == "xopt_alpha_pos", "\u03b1_gain", ifelse(key == "xopt_alpha_neg", "\u03b1_loss",ifelse(key == "xopt_exp_pos", "\u03b3_gain", ifelse(key == "xopt_exp_neg", "\u03b3_loss", ifelse(key == "xopt_lossave", "\u03bb", NA)))))))))%>%
  group_by(learner, x_axis, key, par) %>%
  summarise(mean_val = mean(value,na.rm=T),
            sem_val = sem(value)) %>%
  left_join(fct_brdr_clrs, by=c("par", "x_axis")) %>%
  ungroup()%>%
  mutate(learner = factor(ifelse(learner == 1, "Learner", "Non-learner"), levels = c("Learner", "Non-learner"))) %>%
  left_join(mean_pred_df %>%
              select(x_axis, pred_order), by="x_axis") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(pred_order, x_axis)]), ordered=TRUE)) %>%
    ggplot(aes(learner, mean_val))+
    geom_bar(aes(fill=learner, color=fct_brdr), stat="identity",size=1.25)+
    geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(key~x_axis, scales='free', labeller = label_wrap_gen(10))+
  ylab("")+
  xlab("")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.title=element_blank())+
  scale_color_manual(values=c("purple", "red", NA))+
  guides(color=FALSE)

ggsave("RL_par_diffs_all.jpeg", device = "jpeg", path = fig_path, width = 10, height = 8, units = "in", dpi = 450)
```

## Model similarity

The remaining 9 models are indistinguishable from each by predictive accuracy. They also offer different narratives with respect to the behavioral difference observed between the two learner groups.  

The two narratives group the remaining 6 models in two categories: those that explain the difference with learning rates (alpha_alpha) models and those that explain it with exponents (exp_exp) models.  

We can quantify how similar the models are to each other at a finer scale (instead of the aggregate scale of average choice prediction) by comparing the prediction errors generated by the models to each other.  

The motivation for this comparison came from the difficulty in choosing the RPE regressor for the imaging analyses. Since the models were not different from each in other metrics we wondered whether we could average RPEs from them without loosing information and possibly getting a more informative regressor. 

```{r}
input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/'

preds = list.files(path=paste0(input_path, "rl_preds_0_25/"), pattern = "All")

all_mods_preds = data.frame()
all_mods_pes = data.frame()

for(f in preds){
  data = read.csv(paste0(input_path, "rl_preds_0_25/", f))
  all_mods_preds = rbind.all.columns(all_mods_preds,data.frame(data))
  data = data %>% select(PE, sub_id, X, model)
  names(data)[which(names(data)=="PE")] = paste0("PE_",unique(data$model))
  data = data %>% select(-model)
  if(nrow(all_mods_pes)==0){
    all_mods_pes = data
  }
  else{
    all_mods_pes = all_mods_pes %>%
      left_join(data, by=c("sub_id", "X"))
  }
}

all_mods_pes = all_mods_pes %>%
  select(X, sub_id, everything())
```

```{r}
mods = grep("PE", names(all_mods_pes), value=T)
cor_df = data.frame(sub_id = NA, var1 = NA, var2=NA, cor_val=NA)
for(i in 1:(length(mods) - 1) ){
  rem_mods = mods[-c(1:i)]
  
  for(j in 1:length(rem_mods)){
    cur_data = all_mods_pes %>% select(sub_id, mods[i], rem_mods[j])
    
    cur_cor_df = cur_data %>% 
      group_by(sub_id) %>% 
      do(data.frame(Cor=t(cor(.[,2], .[,3], use="pairwise")))) %>% 
      mutate(var1 = mods[i],
             var2 = rem_mods[j]) %>%
      select(sub_id, var1, var2, everything())
    
    names(cur_cor_df) = c("sub_id", "var1", "var2", "cor_val")
    
    cor_df = rbind(cor_df, data.frame(cur_cor_df))
    
  }
} 

cor_df = cor_df[-1,]
```

Distributuons of correlations between PE's generated by different models for each subject (sorted by the median correlation for model pair)

```{r warning=FALSE, message=FALSE}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

exp_models = c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_",
               "Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_",
               "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_", 
               "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos-lossave_Fix_",
               "Fit_alpha_neg-alpha_pos-beta-exp-lossave_Fix_", 
               "Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")

cor_df %>%
  mutate(var1 = gsub("PE_Preds_", "", var1),
         var2 = gsub("PE_Preds_", "", var2)) %>%
  filter(var1 %in% models) %>%
  filter(var2 %in% models) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_btw = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_btw) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T),
         cor_type = ifelse((var1 %in% exp_models)&(var2 %in% exp_models), "exp_exp", ifelse(!(var1 %in% exp_models)&!(var2 %in% exp_models), "alpha_alpha", "exp_alpha"))) %>%
  #exp_exp, alpha-alpha, "exp_alpha"
  ggplot(aes(reorder(cor_btw,med_cor_val), cor_val, color=factor(cor_type)))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle=90),
        legend.title = element_blank(),
        panel.grid = element_blank())+
  ylab("Correlation between RPEs")+
  xlab("")+
  coord_flip()

ggsave("PE_corrs.jpeg", device = "jpeg", path = fig_path, width = 7, height = 6, units = "in", dpi = 450)
```

Predicted RPEs from models with exponents are more similar to each other compared to RPEs predicted by models without exponents. 

```{r}
cor_df %>%
  mutate(var1 = gsub("PE_Preds_", "", var1),
         var2 = gsub("PE_Preds_", "", var2)) %>%
  filter(var1 %in% models) %>%
  filter(var2 %in% models) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_btw = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_btw) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T),
         cor_type = ifelse((var1 %in% exp_models)&(var2 %in% exp_models), "exp_exp", ifelse(!(var1 %in% exp_models)&!(var2 %in% exp_models), "alpha_alpha", "exp_alpha"))) %>%
  ungroup() %>%
  group_by(cor_type) %>%
  summarise(mean_cor = mean(cor_val, na.rm=T),
            sem_cor = sem(cor_val)) %>%
  ggplot(aes(cor_type, mean_cor, col=cor_type))+
  geom_point()+
  geom_errorbar(aes(ymin = mean_cor-sem_cor, ymax = mean_cor+sem_cor))+
  xlab("")+
  ylab("Average correlation between\npredicted RPEs")+
  theme(panel.grid = element_blank(), 
        legend.position="none")
```

Since we have multiple models that generate RPEs that we cannot distinguish from each other behaviorally we can try to **distinguish them using the brain data** by looking at whether the RPEs generated by any of them correlate more highly with the brain data (in the apriori region vStr).

## Brain correlations

```{r}
all_vstr_pe_betas = read.csv(paste0(input_path, 'rpe_cors/all_vstr_pe_betas.csv'))
```

Predicted RPEs for which model or average of three model types correlate highest with brain activity (i.e. have the highest betas)?  
Note = when comparing the models looking at absolute betas to get a sense of the 'strength' of the relationship.

```{r warning=FALSE, message = FALSE}
tmp_levels = c("α + α", "γ + α" ,"γ + γ" ,"α_gain, α_loss, β" , "α, β, λ","α_gain, α_loss, β, λ","α_gain, α_loss, β, γ" ,"α_gain, α_loss, β, γ_gain, γ_loss", "α_gain, α_loss, β, γ_gain, γ_loss, λ", "α_gain, α_loss, β, γ, λ" , "α, β, γ_gain, γ_loss", "α, β, γ_gain, γ_loss, λ"  )

all_vstr_pe_betas %>%
  filter(pe_type != 'pe') %>%
  filter(beta> (-1) & beta<(1))%>%
  group_by(roi, run_num, model, pe_type) %>%
  summarise(mean_beta = mean(beta),
            sem_beta = sem(beta)) %>%
  left_join(num_pars_df %>% 
              select(model, x_axis) %>%
              mutate(model = gsub("LearningParams_", "", model)),
            by = "model") %>%
  mutate(x_axis = ifelse(model == "alpha_alpha", "\u03b1 + \u03b1", ifelse(model == "exp_exp", "\u03b3 + \u03b3", ifelse(model == "exp_alpha", "\u03b3 + \u03b1", x_axis))),
         x_axis = factor(x_axis, levels=tmp_levels)) %>%
  ggplot(aes(x_axis, abs(mean_beta), color=pe_type))+
  geom_point(position=position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = abs(mean_beta) - sem_beta, ymax = abs(mean_beta) + sem_beta), position=position_dodge(width = 0.9), width=0.1)+
  facet_grid(run_num~roi)+
  theme(axis.text.x = element_text(angle = 90),
        panel.grid = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  scale_x_discrete(labels = function(x) str_wrap(x, width=15))

ggsave("PE_betas.jpeg", device = "jpeg", path = fig_path, width = 12, height = 6, units = "in", dpi = 450)
```

Looking only at the averages of three model types since there isn't a clear winner from the individiaul model correlations between predicted RPEs and brain activity and they were indistinguishable behaviorally as well.

```{r warning=FALSE, message=FALSE}
all_vstr_pe_betas %>%
  filter(pe_type != 'pe') %>%
  filter(beta> (-1) & beta<(1))%>%
  group_by(roi, run_num, model, pe_type) %>%
  summarise(mean_beta = mean(beta),
            sem_beta = sem(beta)) %>%
  left_join(num_pars_df %>% 
              select(model, x_axis) %>%
              mutate(model = gsub("LearningParams_", "", model)),
            by = "model") %>%
  mutate(x_axis = ifelse(model == "alpha_alpha", "\u03b1 + \u03b1", ifelse(model == "exp_exp", "\u03b3 + \u03b3", ifelse(model == "exp_alpha", "\u03b3 + \u03b1", x_axis))),
         x_axis = factor(x_axis, levels=tmp_levels)) %>%
  filter(model %in% c("exp_exp", "alpha_alpha", "exp_alpha")) %>%
  ggplot(aes(factor(run_num), abs(mean_beta), color=pe_type))+
  geom_point(position=position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = abs(mean_beta) - sem_beta, ymax = abs(mean_beta) + sem_beta), position=position_dodge(width = 0.9), width=0.25)+
  facet_grid(roi~x_axis)+
  theme(panel.grid = element_blank(), 
        legend.title = element_blank())+
  xlab("Run")

ggsave("PE_betas_3_types.jpeg", device = "jpeg", path = fig_path, width = 8, height = 6, units = "in", dpi = 450)
```

RPEs from the averaging of exp_exp models have both correlations with brain activity. Particularly for lpe's.

```{r warning=FALSE, message=FALSE}
tmp=all_vstr_pe_betas %>%
  filter(pe_type != 'pe') %>%
  filter(beta> (-1) & beta<(1))%>%
  left_join(num_pars_df %>% 
              select(model, x_axis) %>%
              mutate(model = gsub("LearningParams_", "", model)),
            by = "model") %>%
  mutate(x_axis = ifelse(model == "alpha_alpha", "\u03b1 + \u03b1", ifelse(model == "exp_exp", "\u03b3 + \u03b3", ifelse(model == "exp_alpha", "\u03b3 + \u03b1", x_axis))),
         x_axis = factor(x_axis, levels=tmp_levels)) %>%
  filter(model %in% c("exp_exp", "alpha_alpha", "exp_alpha"))
```

```{r}
summary(lmer(beta ~ pe_type*model+(1|sub_num), tmp))
```

### Learner differences in brain correlations

RPEs from the averaging of exp_exp models are also more variablein these correlations. Is this because these correlations can discriminate between learner/not-learner groups?  

Two points from the graph below:
- The learner vs non-learner difference seems to lie in the hpe response between the two groups.  
- If one wanted to make a model selection point from this too would a positive response for both hpe/lpe as seen in exp_exp models be more convincing than the 0 response in the alpha_alpha models?  

```{r message = FALSE, warning=FALSE}
all_vstr_pe_betas %>%
  filter(pe_type != 'pe') %>%
  filter(beta> (-1) & beta<(1))%>%
  left_join(num_pars_df %>% 
              select(model, x_axis) %>%
              mutate(model = gsub("LearningParams_", "", model)),
            by = "model") %>%
  mutate(x_axis = ifelse(model == "alpha_alpha", "\u03b1 + \u03b1", ifelse(model == "exp_exp", "\u03b3 + \u03b3", ifelse(model == "exp_alpha", "\u03b3 + \u03b1", x_axis))),
         x_axis = factor(x_axis, levels=tmp_levels)) %>%
  filter(model %in% c("exp_exp", "exp_alpha", "alpha_alpha")) %>%
  left_join(learner_info %>% rename(sub_num = sub_id), by = "sub_num") %>%
  group_by(roi, pe_type, learner, x_axis) %>%
   summarise(mean_beta = mean(beta),
            sem_beta = sem(beta)) %>%
  ungroup() %>%
  mutate(learner = ifelse(learner == 1, "learner", "non-learner")) %>%
  ggplot(aes(learner, mean_beta, color=pe_type))+
  geom_point(position=position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_beta - sem_beta, ymax = mean_beta + sem_beta), position=position_dodge(width = 0.9), width=0.25)+
   facet_grid(roi~x_axis)+
  theme(panel.grid = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  geom_hline(aes(yintercept=0), linetype="dashed")
```

```{r warning=FALSE, message=FALSE}
tmp = all_vstr_pe_betas %>%
  filter(pe_type != 'pe') %>%
  filter(beta> (-1) & beta<(1))%>%
  left_join(num_pars_df %>% 
              select(model, x_axis) %>%
              mutate(model = gsub("LearningParams_", "", model)),
            by = "model") %>%
  mutate(x_axis = ifelse(model == "alpha_alpha", "\u03b1 + \u03b1", ifelse(model == "exp_exp", "\u03b3 + \u03b3", ifelse(model == "exp_alpha", "\u03b3 + \u03b1", x_axis))),
         x_axis = factor(x_axis, levels=tmp_levels)) %>%
  filter(model %in% c("exp_exp", "exp_alpha", "alpha_alpha")) %>%
  left_join(learner_info %>% rename(sub_num = sub_id), by = "sub_num") %>%
  mutate(learner = ifelse(learner == 1, "learner", "non-learner"))
```

With exp_exp models the interaction between learner and pe_type is significant in the l_vstr

```{r}
summary(lmer(beta ~ learner*pe_type + (1|sub_num), tmp %>% filter(roi == "l_vstr" & model == "exp_exp")))
```

And r_str.

```{r}
summary(lmer(beta ~ learner*pe_type + (1|sub_num), tmp %>% filter(roi == "r_vstr" & model == "exp_exp")))
```

With alpha_alpha models the interaction between learner and pe_type is significant in the l_vstr

```{r}
summary(lmer(beta ~ learner*pe_type + (1|sub_num), tmp %>% filter(roi == "l_vstr" & model == "alpha_alpha")))
```

but not in the r_vstr.

```{r}
summary(lmer(beta ~ learner*pe_type + (1|sub_num), tmp %>% filter(roi == "r_vstr" & model == "alpha_alpha")))
```

What about change across time for learners and non-learners?  
The change of relationship between RPEs predicted by exp_exp models and brain activity have different profiles for learners and non-learners. For non-learners there is no linear/monotonous pattern. For learners the relationship between brain activity and predicted LPEs show a clear linear decrease while the relationship between BOLD and HPE is 0 throughout.


```{r warning=FALSE, message=FALSE}
all_vstr_pe_betas %>%
  filter(pe_type != 'pe') %>%
  filter(beta> (-1) & beta<(1))%>%
  left_join(num_pars_df %>% 
              select(model, x_axis) %>%
              mutate(model = gsub("LearningParams_", "", model)),
            by = "model") %>%
  mutate(x_axis = ifelse(model == "alpha_alpha", "\u03b1 + \u03b1", ifelse(model == "exp_exp", "\u03b3 + \u03b3", ifelse(model == "exp_alpha", "\u03b3 + \u03b1", x_axis))),
         x_axis = factor(x_axis, levels=tmp_levels)) %>%
  filter(model %in% c("exp_exp")) %>%
  left_join(learner_info %>% rename(sub_num = sub_id), by = "sub_num") %>%
  group_by(run_num, roi, pe_type, learner, x_axis) %>%
   summarise(mean_beta = mean(beta),
            sem_beta = sem(beta)) %>%
  ungroup() %>%
  mutate(learner = ifelse(learner == 1, "learner", "non-learner")) %>%
  filter(roi == "l_vstr") %>%
  ggplot(aes(factor(run_num), mean_beta, color=pe_type))+
  geom_point(position=position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_beta - sem_beta, ymax = mean_beta + sem_beta), position=position_dodge(width = 0.9), width=0.25)+
  geom_line(stat='smooth', method='lm', aes(group=pe_type), size=2, span=0.5)+
  facet_wrap(~learner, scales='free')+
  theme(panel.grid = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  geom_hline(aes(yintercept=0), linetype="dashed")
```

**Overall the case for exp_exp seems models is stronger:**  
- The RPEs from these models correlate more strongly with brain activity  
- This relationship shows more fluctuation across the task (though these fluctuations are difficult to describe and interpret)  
- They discriminate between learners and non-learners better.  

In terms of clarifying the confusing picture in the literature using exp models has pros and cons. Since they haven't been used in this line of reseach before the models are not directly comparable. But they take into account a wealth of other literature on value processing and with a single parameter extend RL models to account for known differences in value distortion particularly with respect to the domain (gain vs loss). It also provides an answer for the behavioral difference between the learner groups in this new parameter which could provide a partial reason for why the previous results were not compatible with each other. 