---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values=cbbPalette) + scale_color_manual(values=cbbPalette)+theme(legend.position="bottom")

input_dir = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/rl_fits/'

fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}

process_fits = function(data){
  require(tidyverse)
  data = data %>% select(-contains("Unnamed"), -X)
  return(data)
}

rbind.all.columns <- function(x, y) {
  
  if(ncol(x) == 0 | ncol(y) == 0){
    out = plyr::rbind.fill(x, y)
  } else{
    x.diff <- setdiff(colnames(x), colnames(y))
    y.diff <- setdiff(colnames(y), colnames(x))
    x[, c(as.character(y.diff))] <- NA
    y[, c(as.character(x.diff))] <- NA
    out = rbind(x, y)
  }
  return(out)
}

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')
```

```{r}
# fits = list.files(path=input_dir, pattern = "All")
fits = c('LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_All.csv',
         'LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_All.csv',
         'LearningParams_Fit_alpha-beta-exp_Fix_All.csv',
         'LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_All.csv',
         'LearningParams_Fit_alpha-beta_Fix_exp_All.csv',
         'LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_All.csv')
```

```{r eval=FALSE}
# Save plot of neglogprob distributions for all subject for each model
for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  p = data %>% 
  ggplot(aes(neglogprob))+
  geom_histogram()+
  facet_wrap(~sub_id, scales='free')
  p_name = gsub('.csv','',f)
  p_name = gsub('LearningParams','Neglogs',p_name)
  ggsave(paste0(p_name, '.jpeg'), plot=p, device = 'jpeg', path = paste0(fig_path, 'neglogs'), width = 30, height = 30, units = "in", limitsize = FALSE)
}
```

```{r}
#Look up df for AIC and BIC calculation
num_pars_df = data.frame(model = c('LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha-beta-exp_Fix_','LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha-beta_Fix_exp_','LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_'), pars = c(3,4,5,2,4,2))
num_pars_df = num_pars_df %>%
  mutate(model = as.character(model))
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran 38 models where these parameters were allowed to vary for gains and losses and were either estimated for each subject or fixed for all subjects. 
Of these we are going to focus on 6 where no parameter is fixed to an arbitarary value but either fit for every subject or omitted from the model (only for exponents).

Each model was fit 50 times for each subject minimizing negative log probability. Fit quality is evaluated using BIC for each fit. 

## Model comparison

Which is the best model?

Read in bounded parameter estimates.

```{r}
all_sub_pars = data.frame()
best_sub_pars = data.frame()

for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  data = data %>% 
    mutate(model = as.character(model)) %>%
    left_join(num_pars_df, by='model') %>%
    mutate(AIC = 2*neglogprob+2*pars,
           BIC = 2*neglogprob+pars*log(180))
  all_sub_pars = rbind.all.columns(all_sub_pars,data.frame(data))
  data = data %>%
    group_by(sub_id) %>%
    slice(which.min(neglogprob))
  best_sub_pars = rbind.all.columns(best_sub_pars,data.frame(data))
}

all_sub_pars = all_sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)

best_sub_pars = best_sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)
```

Read in Nelder-Mead estimates.

```{r}
all_sub_pars_nm = data.frame()
best_sub_pars_nm = data.frame()

for(f in fits){
  data = read.csv(paste0(input_dir,'Nelder_Mead/', f))
  data = process_fits(data)
  data = data %>% 
    mutate(model = as.character(model)) %>%
    left_join(num_pars_df, by='model') %>%
    mutate(AIC = 2*neglogprob+2*pars,
           BIC = 2*neglogprob+pars*log(180))
  all_sub_pars_nm = rbind.all.columns(all_sub_pars_nm,data.frame(data))
  data = data %>%
    group_by(sub_id) %>%
    slice(which.min(neglogprob))
  best_sub_pars_nm = rbind.all.columns(best_sub_pars_nm,data.frame(data))
}

all_sub_pars_nm = all_sub_pars_nm %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)

best_sub_pars_nm = best_sub_pars_nm %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")),
         model = gsub("LearningParams_", "", model)) %>%
  drop_na(age_group)
```

Plotting the best model/model with lowest average BIC for the bounded estimations.

```{r message = FALSE, warning = FALSE}
best_sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(min_col = factor(ifelse(mean_bic == min(mean_bic, na.rm=T), 1,0)),
         x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model))))))) %>%
  ggplot(aes(factor(x_axis), mean_bic))+
  geom_point(aes(col = min_col))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=min_col))+
  theme(legend.position = "none",
        panel.grid=element_blank())+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_color_manual(values = c("black", "red"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))

ggsave("RL_models_comp.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

This is different than what the Nelder-Mead estimates would have suggest (which are overall worse/have higher BICs).

```{r warning=FALSE, message=FALSE}
best_sub_pars_nm %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(min_col = factor(ifelse(mean_bic == min(mean_bic, na.rm=T), 1,0)),
         x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model))))))) %>%
  ggplot(aes(factor(x_axis), mean_bic))+
  geom_point(aes(col = min_col))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=min_col))+
  theme(legend.position = "none",
        panel.grid=element_blank())+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_color_manual(values = c("black", "red"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))
```

How to read model names: Parameters that are fitted are listed following the `Fit` keyword in the model name, parameters that are fixed are listed following the `Fix` keyword in the model name.  

For example in `Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_` the parameters `alpha_pos`, `beta`, `exp_neg` and `exp_pos` are fitted to data while `alpha_neg` is fixed.  

```{r}
best_sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  arrange(mean_bic)
```

So far we only looked at average BIC's. To confirm that the overall tendency for smaller BICs in the best model is meaningful we compare the fit indices for the best two models for each subject.   

To do this we calculate a Bayes Factors as listed in [Wagenmaakers (2007)](https://link-springer-com.stanford.idm.oclc.org/content/pdf/10.3758%2FBF03194105.pdf)

$BF_{01} = exp(\frac{BIC_1 - BIC_0}{2})$

the smaller this is the more evidence for H1 (the 4 parameter model with a single learning rate and two exponents). Here we take the simpler model to be H1 and the more complicated one as H0 (the 5 parameter model with two learning rates and two exponents). $BF_{01}$ < 3 suggests weak evidence for H0 and therefore a preference for H1.  

The percentage of participants with weak evidence for H0 and for whom therefore the simpler model H1 would be preferred is:

```{r}
tmp = best_sub_pars %>%
  filter(model %in% c('Fit_alpha-beta-exp_neg-exp_pos_Fix_', 'Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_')) %>%
  select(sub_id, BIC, model) %>%
  group_by(sub_id) %>%
  mutate(model = gsub("-", "_", model)) %>%
  spread(model, BIC) %>%
  mutate(BF01 = exp((Fit_alpha_beta_exp_neg_exp_pos_Fix_ - Fit_alpha_neg_alpha_pos_beta_exp_neg_exp_pos_Fix_)/2),
         simpler_model_better = ifelse(BF01<3,1,0)) %>%
  drop_na(simpler_model_better)

sum(tmp$simpler_model_better)/nrow(tmp)*100
```

The 'best' model using bounded estimation is only the best for 2/3's of the subjects so it's not very convincing.

### Age differences in fit

Do models differ in fit by age group?   

Only for the model with two learning rates the fits are more variable for kids than for other groups.

```{r}
best_sub_pars %>%
  group_by(age_group, model) %>%
  summarise(mean_bic = mean(BIC),
            sem_bic = mean(BIC)) %>%
  mutate(x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model))))))) %>%
  ggplot(aes(factor(x_axis),mean_bic, color=age_group))+
  geom_point()+
  geom_errorbar(aes(ymin = mean_bic-sem_bic, ymax = mean_bic+sem_bic))+
  xlab("")+
  ylab("Mean BIC across subjects")+
  theme(axis.text.x = element_blank(),
        legend.title = element_blank(),
        panel.grid = element_blank())

ggsave("RL_models_comp_age.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

### Comparison of BIC distributions

Bayes Factor check above suggested that for a majority of the subjects the 4 parameter model with the lowest BIC is supported more than the next best model. But are do these BIC distributions even have different means? The simplest model has a higher mean than the rest but the others do not differ from each other.

```{r}
summary(lm(BIC ~ model, best_sub_pars))
```

### Predicting choice

**Focusing only on the four models with the gain loss distinction**

Since there isn't a major difference in fits then how well do they actually account for the behavior?  

To get a sense of this I plugged in the best fitting parameters for each model and predicted choice behavior.  

Note: I'm not even doing proper prediction here using left out data so these results would, if anything, be optimistic.

```{r}
input_dir = "/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/rl_preds/"

models = c('Fit_alpha_neg-alpha_pos-beta-exp_Fix',"Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix","Fit_alpha-beta-exp_neg-exp_pos_Fix","Fit_alpha_neg-alpha_pos-beta_Fix_exp" )

all_preds = data.frame()

for(i in 1:length(models)){
  cur_preds = read.csv(paste0(input_dir,list.files(input_dir, models[i])))
  cur_preds = cur_preds %>%
    select(-X, -Unnamed..0 )
  all_preds = rbind(all_preds, cur_preds)
}

all_preds = all_preds %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult")))%>%
  filter(Response != 0) %>%
  mutate(play1_pass0 = ifelse(Response == 1, 1, 0),
         pred_play1_pass0 = ifelse(choiceprob>0.5, 1, ifelse(choiceprob<0.5, 0, ifelse(choiceprob == 0.5, rbinom(1,1,.5), NA))),
         pred_correct = ifelse(play1_pass0 == pred_play1_pass0, 1,0)) %>%
   mutate(x_axis = ifelse(model == "Preds_Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Preds_Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Preds_Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model)))))))
```

The models that do distinguish between learning from gains and losses account very well for some subjects' data and really badly for others so there is a qualitative difference.   

*But* whether this is in the distortion of the PE or how quickly the PE is incorporated into the EV cannot be disentangled through model comparison.    

```{r}
all_preds %>%
  group_by(x_axis, sub_id, age_group) %>%
  summarise(correct_pct=mean(pred_correct)) %>%
  ggplot(aes(x_axis, correct_pct)) +
  geom_boxplot(aes(fill=age_group), alpha =0.5)+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  xlab("")+
  ylab("Proportion of correct \n choice predictions")+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  ylim(0,1)

ggsave("RL_preds_comp_age.jpeg", device = "jpeg", path = fig_path, width = 9, height = 5, units = "in", dpi = 450)
```

Does this relate to performance? Do people whose choices can be predictied better/worse by the model perform the task better or worse? No.

```{r}
all_preds %>%
  group_by(age_group, sub_id, x_axis) %>%
  summarise(total_points = sum(Points_earned),
            mean_correct = mean(pred_correct)) %>%
  ggplot(aes(total_points, mean_correct))+
  geom_point()+
  geom_smooth(method="lm")+
  facet_grid(age_group~x_axis)
```

### Confusion matrices

To get a better sense of model failures I looked at the confusion matrices between actual and predicted choice. These suggest:  

The models succeed and fail similarly for all age groups  

```{r warning=FALSE, message=FALSE}
all_preds %>%
  filter(!x_axis %in% c("\u03b1 , \u03b2", "\u03b1, \u03b2, \u03b3")) %>%
  mutate(correct_positive = ifelse(play1_pass0 == 1 & pred_play1_pass0 == 1, 1, 0),
         correct_negative = ifelse(play1_pass0 == 0 & pred_play1_pass0 == 0, 1, 0),
         false_positive = ifelse(play1_pass0 == 0 & pred_play1_pass0 == 1, 1, 0),
         false_negative = ifelse(play1_pass0 == 1 & pred_play1_pass0 == 0, 1, 0)) %>%
  group_by(age_group, x_axis) %>%
  summarise(sum_correct_positive = sum(correct_positive),
            sum_correct_negative = sum(correct_negative),
            sum_false_positive = sum(false_positive),
            sum_false_negative = sum(false_negative)) %>%
  gather(key, value, -age_group, -x_axis) %>%
  mutate(TChoice = ifelse(key %in% c("sum_correct_positive", "sum_false_negative"),1,0),
         PChoice = ifelse(key %in% c("sum_correct_positive", "sum_false_positive"), 1, 0)) %>%
  ggplot(mapping = aes(x = factor(TChoice), y = factor(PChoice))) +
  geom_tile(aes(fill = value), colour = "white") +
  scale_fill_gradient(low = "blue", high = "red")+
  geom_text(aes(label = sprintf("%1.0f", value)), vjust = 1) +
  xlab("True Choice")+
  ylab("Predicted Choice")+
  facet_grid(age_group~x_axis)+
  theme(legend.position = "none")

ggsave("RL_confusion_matrices.jpeg", device = "jpeg", path = fig_path, width = 9, height = 5, units = "in", dpi = 450)

#sum_correct_positive: TChoice = 1, PChoice = 1
#sum_correct_negative: TChoice = 0, PChoice = 0,
#sum_false_positive: TChoice = 0, PChoice = 1,
#sum_false_negative: TChoice = 1, PChoice = 0

```

### Differing conclusions

Do the models tell the same story when checking for age differences across parameter?

Or a similar story that doesn't make sense with the behavior?

```{r message = FALSE, warning=FALSE}
best_sub_pars %>%
  filter(model %in% c("Fit_alpha-beta-exp_Fix_", "Fit_alpha-beta_Fix_exp_") == FALSE) %>%
  select(contains("xopt"), "sub_id", "model", "age_group") %>%
  gather(key, value, -age_group, -sub_id, -model) %>%
  mutate(x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model)))))),
         key = ifelse(key == "xopt_alpha", "\u03b1", ifelse(key == "xopt_beta", "\u03b2", ifelse(key == "xopt_exp", "\u03b3", ifelse(key == "xopt_alpha_pos", "\u03b1_gain", ifelse(key == "xopt_alpha_neg", "\u03b1_loss",ifelse(key == "xopt_exp_pos", "\u03b3_gain", ifelse(key == "xopt_exp_neg", "\u03b3_loss", NA))))))))%>%
  group_by(age_group, x_axis, key) %>%
  summarise(mean_val = mean(value,na.rm=T),
            sem_val = sem(value)) %>%
  ggplot(aes(age_group, mean_val))+
    geom_bar(aes(fill=age_group), stat="identity")+
    geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(key~x_axis, scales='free')+
  ylab("")+
  xlab("")+
  theme(panel.grid = element_blank(),
        legend.position = "none")

ggsave("RL_par_diffs.jpeg", device = "jpeg", path = fig_path, width = 9, height = 5, units = "in", dpi = 450)
```


```{r message=FALSE, warning=FALSE}
best_sub_pars_nm %>%
  filter(model %in% c("Fit_alpha-beta-exp_Fix_", "Fit_alpha-beta_Fix_exp_") == FALSE) %>%
  select(contains("xopt"), "sub_id", "model", "age_group") %>%
  gather(key, value, -age_group, -sub_id, -model) %>%
  mutate(x_axis = ifelse(model == "Fit_alpha-beta_Fix_exp_", "\u03b1 , \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta_Fix_exp_", "\u03b1_gain, \u03b1_loss, \u03b2", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3", ifelse(model == "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "\u03b1_gain, \u03b1_loss, \u03b2, \u03b3_gain, \u03b3_loss", ifelse(model == "Fit_alpha-beta-exp_Fix_", "\u03b1, \u03b2, \u03b3", ifelse(model == "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "\u03b1, \u03b2, \u03b3_gain, \u03b3_loss", model)))))),
         key = ifelse(key == "xopt_alpha", "\u03b1", ifelse(key == "xopt_beta", "\u03b2", ifelse(key == "xopt_exp", "\u03b3", ifelse(key == "xopt_alpha_pos", "\u03b1_gain", ifelse(key == "xopt_alpha_neg", "\u03b1_loss",ifelse(key == "xopt_exp_pos", "\u03b3_gain", ifelse(key == "xopt_exp_neg", "\u03b3_loss", NA))))))))%>%
  group_by(age_group, x_axis, key) %>%
  summarise(mean_val = mean(value,na.rm=T),
            sem_val = sem(value)) %>%
  ggplot(aes(age_group, mean_val))+
    geom_bar(aes(fill=age_group), stat="identity")+
    geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(key~x_axis, scales='free')+
  ylab("")+
  xlab("")+
  theme(panel.grid = element_blank(),
        legend.position = "none")

ggsave("RL_par_diffs_NM.jpeg", device = "jpeg", path = fig_path, width = 9, height = 5, units = "in", dpi = 450)
```

Which parameters show age differences in which models?

**Using Nelder-Mead I found different stories:**

**- According to the model with single learning rate and two exponents the difference lies in the distortion of negative prediction errors and betas**
**- According to the model with two learning rates and no exponent (i.e. 1) the difference lies in the learning rate from negative outcomes and betas**

```{r}
models = c("Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "Fit_alpha_neg-alpha_pos-beta_Fix_exp_")

age_diff_df = data.frame(model = NA, par = NA, teen=NA, adult=NA)

for(i in 1:length(models)){
  cur_data = best_sub_pars_nm %>% filter(model == models[i])
  cur_data = cur_data %>% select(age_group, contains("xopt"))
  cur_data = cur_data[,colSums(is.na(cur_data))<nrow(cur_data)]
  cur_data = transform_remove_skew(cur_data)
  names(cur_data)[1] = "age_group"
  pars = names(cur_data)[!names(cur_data) %in% c("age_group")]
  
    for(j in 1:length(pars)){
      p_vals = summary(lm(cur_data[,pars[j]] ~ cur_data[,"age_group"]))$coefficients[c(2,3),4]
      age_diff_df[nrow(age_diff_df) + 1,] = list(models[i],pars[j], p_vals[1], p_vals[2])
    }
}
age_diff_df = age_diff_df[-1,]
age_diff_df %>%
   filter(adult<0.05)
```

**However these were driven primarily by outliers/parameter values that are difficult to make sense of.** 

So I changed the estimation method to 'L-BFGS-B' and priors to what is used in Christakis (2013). This way I could place bounds for the parameters and did not end up with negative learning rates or exponents or huge inverse temperatures.

This lead to 2 changes:

- More convergence failiures. In fact the model with the lowest BIC overall is also the one that fails the most. I'm not sure how to account for this in comparing the models. Exclude subjects who have more than X % of convergence failures and drop model that have more than Y % of subjects removed?

- No age difference for any parameters of any group

```{r}
models = c("Fit_alpha_neg-alpha_pos-beta-exp_Fix_", "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_", "Fit_alpha-beta-exp_neg-exp_pos_Fix_", "Fit_alpha_neg-alpha_pos-beta_Fix_exp_")

age_diff_df = data.frame(model = NA, par = NA, teen=NA, adult=NA)

for(i in 1:length(models)){
  cur_data = best_sub_pars %>% filter(model == models[i])
  cur_data = cur_data %>% select(age_group, contains("xopt"))
  cur_data = cur_data[,colSums(is.na(cur_data))<nrow(cur_data)]
  cur_data = transform_remove_skew(cur_data)
  names(cur_data)[1] = "age_group"
  pars = names(cur_data)[!names(cur_data) %in% c("age_group")]
  
    for(j in 1:length(pars)){
      p_vals = summary(lm(cur_data[,pars[j]] ~ cur_data[,"age_group"]))$coefficients[c(2,3),4]
      age_diff_df[nrow(age_diff_df) + 1,] = list(models[i],pars[j], p_vals[1], p_vals[2])
    }
}
age_diff_df = age_diff_df[-1,]
age_diff_df %>%
  arrange(adult, teen)
# age_diff_df %>%
#   filter(adult<0.05)
```

**So**

**- We have models that don't capture the behavior we are interested in (i.e. the difference between kids and adults in learning from losses)**
**- We have models that are on average at change in predicting behavior.** 

The next step to build a story using RL models could be to take a closer look at subsets the models do a good job for vs others. We can check whether the age group difference exist for one or the other group of subjects. 

1. Is the behavioral effect there for one or the other group?

How do you decide someone is predictable? Do they need to succeed in all models?

2. Is the age group difference between parameters there for the predictable group?

I haven't gone down this rabbit hole because: 
1. I don't want to split the small samples per group even more  
2. I'm not sure what the best way to account for model convergence failures would be  
3. Even if we found age differences in parameter estimates for the 'predictable' subjects we don't have a way to describe the behavior of the others

## Alternative models

Is there another story be told using a different class of models. E.g. drift diffusion?

```{r}
input_dir = "/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/ez_fits/"
```

```{r}
ez_fits_correctinc = read.csv(paste0(input_dir,"EZ_ddm_fits_correct1_incorrect0_all.csv" ))

ez_fits_correctinc = ez_fits_correctinc %>% 
  rename(sub_id = X) %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult"))) %>%
  gather(key, value, -sub_id, -age_group) %>%
  separate(key, into=c("model", "par", "machine"), sep='_') %>%
  mutate(machine= ifelse(machine=="1.0","+5/-495", ifelse(machine=="2.0", "-5/+495", ifelse(machine == "3.0", "-10/+100", ifelse(machine == "4.0", "+10/-100",NA))))) %>%
  filter(value != -Inf)

```

```{r warning=FALSE, message=FALSE}
ez_fits_correctinc %>%
  group_by(age_group, machine, par) %>%
  summarise(mean_val = mean(value, na.rm=T),
            sem_val = sem(value)) %>%
  ggplot(aes(age_group, mean_val))+
  geom_bar(aes(fill=age_group), stat="identity")+
  geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(par~machine, scales="free")+
  theme(legend.position = "none",
        panel.grid = element_blank())+
  ylab("")+
  xlab("")
```

Age differences:

- Adults always have higher drift rates
- Drift rates decrease with EV and variance

```{r}
summary(lm(value ~ age_group*machine, ez_fits_correctinc %>% filter(par=="drift")))
```

- Teens have shorter non-decision times compared to kids and adults

```{r}
summary(lm(value ~ age_group*machine, ez_fits_correctinc %>% filter(par=="non.decision")))
```

- There are no differences in thresholds neither across age groups nor across machines.

```{r}
summary(lm(value ~ age_group*machine, ez_fits_correctinc %>% filter(par=="thresh")))
```

### Predicting choice with alternative model

Much better compared to any of the RL models

```{r}
choice_data = all_preds %>%
  filter(model == "Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_") %>%
  select(Trial_type, Response, sub_id, age_group) %>%
  mutate(correct1_incorrect0 = ifelse(Trial_type==1&Response==2,1,ifelse(Trial_type==2&Response==1,1, ifelse(Trial_type==3&Response==1,1,ifelse(Trial_type==4&Response==2,1,0))))) %>%
  mutate(machine= ifelse(Trial_type==1,"+5/-495", ifelse(Trial_type==2, "-5/+495", ifelse(Trial_type == 3, "-10/+100", ifelse(Trial_type == 4, "+10/-100",NA)))))

choice_data %>%
  left_join(ez_fits_correctinc %>%
              spread(par, value) %>%
              select(-model), by=c("sub_id", "age_group", "machine")) %>%
  mutate(pred_pc = 1/(1+exp(-thresh*drift/0.1^2)),
         pred_correct1_incorrect0 = ifelse(pred_pc>0.5,1,0),
         pred_correct = ifelse(pred_correct1_incorrect0 == correct1_incorrect0,1,0)) %>%
  group_by(sub_id, machine) %>%
  summarise(pred_pct = mean(pred_correct,na.rm=T)) %>%
    mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult"))) %>%
  ggplot(aes(machine, pred_pct, fill=age_group))+
  geom_boxplot(alpha=0.5)+
  theme(legend.title = element_blank(),
        panel.grid = element_blank())+
  ylab("Proportion of correct \n choice predictions")+
  xlab("")+
  ylim(0,1)
```

### Confusion matrices for alternative model

```{r}

```