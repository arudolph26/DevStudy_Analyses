---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/helper_functions/ggplot_colors.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_preds_data.R')

library(lme4)
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran models where these parameters were allowed to vary for gains and losses and were either estimated for each subject. 

Each model was fit 50 times for each subject minimizing negative log probability. Fit quality is evaluated using BIC for each fit. 

## Model comparison

Which is the best model?

```{r warning=FALSE, message=FALSE}
mean_pred_df = ave_sub_preds %>%
  group_by(model) %>%
  summarise(ave_pred_prop = mean(cor_pred_prop),
            sem_pred_prop = sem(cor_pred_prop)) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(-pars), 
            by="model") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(ave_pred_prop, x_axis)]), ordered=TRUE),
         pred_order = as.numeric(x_axis))

mean_bic_df = best_sub_pars %>%
  select(x_axis, BIC) %>%
  group_by(x_axis) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(-mean_bic, x_axis)]), ordered=TRUE), 
         bic_order = as.numeric(x_axis)) 

rank_diff_models = mean_pred_df %>%
  left_join(mean_bic_df %>% select(x_axis, bic_order), by="x_axis") %>%
  mutate(rank_diff = abs(pred_order - bic_order)) %>%
  filter(rank_diff>4)
rank_diff_models = rank_diff_models$x_axis
```

Plotting each model's prediction accuracy when predicting the quarter of left out data for each subject after fitting the model on the remaining three quarters of data.  

Model are listed from worst to best.

```{r warning=FALSE, message=FALSE}
mean_pred_df %>%
  mutate(rank_diff = factor(ifelse(x_axis %in% rank_diff_models, 1, 0))) %>%
  ggplot(aes(x_axis, ave_pred_prop))+
  geom_point(aes(col=rank_diff))+
  geom_errorbar(aes(ymin = ave_pred_prop-sem_pred_prop, ymax = ave_pred_prop+sem_pred_prop, col=rank_diff))+
  xlab("")+
  ylab("Mean prediction accuracy")+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  scale_color_manual(values=c("black", "red"))+
  theme(legend.position = "none")

ggsave("RL_models_comp_pred.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

```{r warning=FALSE, message=FALSE}
mean_bic_df %>%
  mutate(rank_diff = factor(ifelse(x_axis %in% rank_diff_models, 1, 0))) %>%
  ggplot(aes(factor(x_axis), mean_bic))+
  geom_point(aes(col=rank_diff))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=rank_diff))+
  theme(legend.position = "none",
        panel.grid=element_blank())+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  scale_color_manual(values=c("black", "red"))+
  theme(legend.position = "none")

ggsave("RL_models_comp_BIC.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

Do models differ from each other based on prediction accuracy? The worst two are significantly worse than the others. The other models do not differ from each in average prediction accuracy.

```{r}
tmp = with(ave_sub_preds, pairwise.t.test(cor_pred_prop, model), p.adj="fdr")
tmp = data.frame(tmp$p.value)
tmp = tmp %>%
  mutate(model1 = row.names(tmp)) %>%
  gather(model2, p_value, -model1) %>%
  mutate(model2 = gsub('\\.', '-', model2)) %>%
  filter(p_value <0.05) %>%
  arrange(model1)

tmp
```

Do average BIC's for each model differ from each other? No.

```{r}
tmp = with(best_sub_pars, pairwise.t.test(BIC, model))
tmp = data.frame(tmp$p.value)
tmp = tmp %>%
  mutate(model1 = row.names(tmp)) %>%
  gather(model2, p_value, -model1) %>%
  mutate(model2 = gsub('\\.', '-', model2)) %>%
  filter(p_value <0.05) %>%
  arrange(model1)

tmp
```

### Learner group difference in fit

```{r}
models = unique(ave_sub_preds$model)

out_df = data.frame(model=NA, learner_p = NA)
for(m in models){
  reg_out = summary(lm(cor_pred_prop ~ learner, ave_sub_preds %>% filter(model == m)))
  learner_p = coef(reg_out)["learner","Pr(>|t|)"]
  cur_row = data.frame(model = m, learner_p = learner_p)
  out_df = rbind(out_df, cur_row)
}
out_df = out_df[-1,]
out_df = out_df %>%
  mutate(learner_p = p.adjust(learner_p, method="fdr"))%>%
  filter(learner_p<0.05)
out_df
```

```{r}
ave_sub_preds %>%
  filter(!is.na(learner)) %>%
  group_by(learner, model) %>%
  summarise(ave_pred_prop = mean(cor_pred_prop),
            sem_pred_prop = sem(cor_pred_prop)) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(-pars), 
            by="model") %>%
  left_join(mean_pred_df %>%
              select(model, pred_order), by="model") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(pred_order, x_axis)]), ordered=TRUE)) %>%
  ggplot(aes(factor(x_axis),ave_pred_prop, color=factor(learner)))+
  geom_point()+
  geom_errorbar(aes(ymin = ave_pred_prop-sem_pred_prop, ymax = ave_pred_prop+sem_pred_prop))+
  xlab("")+
  ylab("Prediction accuracy across subjects")+
  theme(panel.grid = element_blank())+
  scale_x_discrete(labels = function(x) str_wrap(x, width=5))+
  labs(color="Learner")+
  geom_hline(yintercept = 0.5, linetype="dashed")

ggsave("RL_models_comp_learner_pred.jpeg", device = "jpeg", path = fig_path, width = 7, height = 5, units = "in", dpi = 450)
```

```{r}
tmp = ave_sub_preds %>%
  mutate(no_gain_loss = ifelse(model %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta-exp-lossave_Fix_", "Fit_alpha-beta_Fix_exp-lossave_"), 1, 0)) 

m = lmer(cor_pred_prop ~ no_gain_loss*factor(learner) + (1|sub_id), tmp)
summary(m)
```

```{r}
confint.merMod(m)
```

```{r}
summary(lm(cor_pred_prop ~ model+learner, tmp %>% filter(no_gain_loss == 0)))
```

### Differing conclusions

Do the models tell the same story when checking for age differences across parameter?

Or a similar story that doesn't make sense with the behavior?

Which parameters show learner group differences in which models?

```{r}
models = unique(best_sub_pars$model)[unique(best_sub_pars$model) %in% c("Fit_alpha-beta-exp_Fix_lossave_", "Fit_alpha-beta-exp-lossave_Fix_", "Fit_alpha-beta_Fix_exp-lossave_")==FALSE]

learner_diff_df = data.frame(model = NA, par = NA, learner=NA)

for(i in 1:length(models)){
  cur_data = best_sub_pars %>% filter(model == models[i])
  cur_data = cur_data %>% select(learner, contains("xopt"))
  cur_data = cur_data[,colSums(is.na(cur_data))<nrow(cur_data)]
  cur_data = transform_remove_skew(cur_data, verbose=FALSE)
  names(cur_data)[1] = "learner"
  pars = names(cur_data)[!names(cur_data) %in% c("learner")]
  
    for(j in 1:length(pars)){
      p_vals = summary(lm(cur_data[,pars[j]] ~ cur_data[,"learner"]))$coefficients['cur_data[, "learner"]','Pr(>|t|)']
      learner_diff_df[nrow(learner_diff_df) + 1,] = list(models[i],pars[j], p_vals[1])
    }
}
learner_diff_df = learner_diff_df[-1,]

learner_diff_df %>%
  mutate(learner_adjust = p.adjust(learner), method="fdr")%>%
  filter(learner<0.05)
```

```{r}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

fct_brdr_clrs = learner_diff_df %>%
  mutate(learner_adjust = p.adjust(learner), method="fdr")%>%
  select(model, par, learner, learner_adjust) %>%
  mutate(fct_brdr = ifelse(learner<0.5, "purple", NA),
         fct_brdr = ifelse(learner_adjust<0.5, "red", fct_brdr)) %>%
  left_join(tmp, by="model") %>%
  select(par, x_axis, fct_brdr) %>%
  mutate(par = gsub(".logTr", "", par))
```

Create df with transformed parameter estimates for plotting since the transformed distributions were used in checking for group differences.

```{r}
log_lookup = best_sub_pars %>% 
  select(sub_id, model, contains("xopt"), x_axis, learner) %>%
  gather(key, value, -sub_id, -model, -x_axis, -learner) %>%
  group_by(model, key) %>%
  summarise(skw = skew(value)) %>%
  filter(abs(skw)>1) %>%
  arrange(model)

best_sub_pars_trans = data.frame()

for(i in 1:length(models)){
  cur_model = models[i]
  cur_model_data = best_sub_pars %>% filter(model == cur_model)
  cur_skw_vars = log_lookup %>% filter(model == cur_model)
  
  for(j in 1:nrow(cur_skw_vars)){
    cur_model_data[,cur_skw_vars$key[j]] = pos_log(cur_model_data[,cur_skw_vars$key[j]])
  }
  
  best_sub_pars_trans = rbind.all.columns(best_sub_pars_trans, cur_model_data)
}
```

```{r message = FALSE, warning=FALSE}
best_sub_pars_trans %>%
  select(contains("xopt"), "sub_id", "x_axis", "learner") %>%
  gather(key, value, -learner, -sub_id, -x_axis) %>%
  mutate(par=key,
         key = ifelse(key == "xopt_alpha", "\u03b1", ifelse(key == "xopt_beta", "\u03b2", ifelse(key == "xopt_exp", "\u03b3", ifelse(key == "xopt_alpha_pos", "\u03b1_gain", ifelse(key == "xopt_alpha_neg", "\u03b1_loss",ifelse(key == "xopt_exp_pos", "\u03b3_gain", ifelse(key == "xopt_exp_neg", "\u03b3_loss", ifelse(key == "xopt_lossave", "\u03bb", NA)))))))))%>%
  group_by(learner, x_axis, key, par) %>%
  summarise(mean_val = mean(value,na.rm=T),
            sem_val = sem(value)) %>%
  left_join(fct_brdr_clrs, by=c("par", "x_axis")) %>%
  ungroup()%>%
  mutate(learner = factor(ifelse(learner == 1, "Learner", "Non-learner"), levels = c("Learner", "Non-learner"))) %>%
  left_join(mean_pred_df %>%
              select(x_axis, pred_order), by="x_axis") %>%
  mutate(x_axis = factor(x_axis, levels=unique(x_axis[order(pred_order, x_axis)]), ordered=TRUE)) %>%
    ggplot(aes(learner, mean_val))+
    geom_bar(aes(fill=learner, color=fct_brdr), stat="identity",size=1.25)+
    geom_errorbar(aes(ymin=mean_val-sem_val, ymax=mean_val+sem_val), width=0)+
  facet_grid(key~x_axis, scales='free', labeller = label_wrap_gen(10))+
  ylab("")+
  xlab("")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.title=element_blank())+
  scale_color_manual(values=c("purple", "red", NA))+
  guides(color=FALSE)

ggsave("RL_par_diffs_all.jpeg", device = "jpeg", path = fig_path, width = 10, height = 8, units = "in", dpi = 450)
```

## Cor btw PEs

```{r}
input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/'

preds = list.files(path=paste0(input_path, "rl_preds/"), pattern = "All")

all_mods_preds = data.frame()
all_mods_pes = data.frame()

for(f in preds){
  data = read.csv(paste0(input_path, "rl_preds/", f))
  all_mods_preds = rbind.all.columns(all_mods_preds,data.frame(data))
  data = data %>% select(PE, sub_id, X, model)
  names(data)[which(names(data)=="PE")] = paste0("PE_",unique(data$model))
  data = data %>% select(-model)
  if(nrow(all_mods_pes)==0){
    all_mods_pes = data
  }
  else{
    all_mods_pes = all_mods_pes %>%
      left_join(data, by=c("sub_id", "X"))
  }
}

all_mods_pes = all_mods_pes %>%
  select(X, sub_id, everything())
```

```{r}
mods = grep("PE", names(all_mods_pes), value=T)
cor_df = data.frame(sub_id = NA, var1 = NA, var2=NA, cor_val=NA)
for(i in 1:(length(mods) - 1) ){
  rem_mods = mods[-c(1:i)]
  
  for(j in 1:length(rem_mods)){
    cur_data = all_mods_pes %>% select(sub_id, mods[i], rem_mods[j])
    
    cur_cor_df = cur_data %>% 
      group_by(sub_id) %>% 
      do(data.frame(Cor=t(cor(.[,2], .[,3], use="pairwise")))) %>% 
      mutate(var1 = mods[i],
             var2 = rem_mods[j]) %>%
      select(sub_id, var1, var2, everything())
    
    names(cur_cor_df) = c("sub_id", "var1", "var2", "cor_val")
    
    cor_df = rbind(cor_df, data.frame(cur_cor_df))
    
  }
} 

cor_df = cor_df[-1,]
```

Distributuons of correlations between PE's generated by different models for each subject (sorted by the median correlation for model pair)

```{r warning=FALSE, message=FALSE}
tmp = num_pars_df %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  select(model, x_axis)

exp_models = c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_",
               "Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_",
               "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_", 
               "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos-lossave_Fix_",
               "Fit_alpha_neg-alpha_pos-beta-exp-lossave_Fix_", 
               "Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")

cor_df %>%
  mutate(var1 = gsub("PE_Preds_", "", var1),
         var2 = gsub("PE_Preds_", "", var2)) %>%
  filter(var1 %in% models) %>%
  filter(var2 %in% models) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_btw = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_btw) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T),
         cor_type = ifelse((var1 %in% exp_models)&(var2 %in% exp_models), "exp_exp", ifelse(!(var1 %in% exp_models)&!(var2 %in% exp_models), "alpha_alpha", "exp_alpha"))) %>%
  #exp_exp, alpha-alpha, "exp_alpha"
  ggplot(aes(reorder(cor_btw,med_cor_val), cor_val, color=factor(cor_type)))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle=90),
        legend.title = element_blank(),
        panel.grid = element_blank())+
  ylab("Correlation between PEs")+
  xlab("")+
  coord_flip()

ggsave("PE_corrs.jpeg", device = "jpeg", path = fig_path, width = 7, height = 6, units = "in", dpi = 450)
```

Predict RPEs from models with exponents are more similar to each other compared to RPEs predicted by models without exponents. 

```{r}
cor_df %>%
  mutate(var1 = gsub("PE_Preds_", "", var1),
         var2 = gsub("PE_Preds_", "", var2)) %>%
  filter(var1 %in% models) %>%
  filter(var2 %in% models) %>%
  left_join(tmp, by=c("var1"="model")) %>%
  left_join(tmp, by=c("var2"="model")) %>%
  mutate(cor_btw = paste0(x_axis.x, " VS. ", x_axis.y)) %>%
  group_by(cor_btw) %>%
  mutate(med_cor_val = median(cor_val, na.rm=T),
         cor_type = ifelse((var1 %in% exp_models)&(var2 %in% exp_models), "exp_exp", ifelse(!(var1 %in% exp_models)&!(var2 %in% exp_models), "alpha_alpha", "exp_alpha"))) %>%
  ungroup() %>%
  group_by(cor_type) %>%
  summarise(mean_cor = mean(cor_val, na.rm=T),
            sem_cor = sem(cor_val)) %>%
  ggplot(aes(cor_type, mean_cor, col=cor_type))+
  geom_point()+
  geom_errorbar(aes(ymin = mean_cor-sem_cor, ymax = mean_cor+sem_cor))+
  xlab("")+
  ylab("Average correlation between\npredicted RPEs")+
  theme(panel.grid = element_blank(), 
        legend.position="none")
```

Previously was using these three models to create an average PE for the imaging analyses:
- PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_
- PE_Preds_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_
- PE_Preds_Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_

In light of this new observation distinguishing 'types' of correlations/models I notice that these are all exponent models and very similar to each other.  

In the imaging results we haven't really found anything significant/expected from the literature. This might be because the regressor is bad. Since we have multiple models that generate RPEs and that we cannot distinguish from each other behaviorally we can try to distinguish them using the brain data by looking at whether the RPEs generated by any of them correlate more highly with the brain data (in expected regions).

