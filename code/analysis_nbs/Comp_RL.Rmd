---
title: "RL models comparison"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values=cbbPalette) + scale_color_manual(values=cbbPalette)+theme(legend.position="bottom")

input_dir = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/input/rl_fits/'

fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}

process_fits = function(data){
  require(tidyverse)
  data = data %>% select(-contains("Unnamed"), -X)
  return(data)
}

rbind.all.columns <- function(x, y) {
  
  if(ncol(x) == 0 | ncol(y) == 0){
    out = plyr::rbind.fill(x, y)
  } else{
    x.diff <- setdiff(colnames(x), colnames(y))
    y.diff <- setdiff(colnames(y), colnames(x))
    x[, c(as.character(y.diff))] <- NA
    y[, c(as.character(x.diff))] <- NA
    out = rbind(x, y)
  }
  return(out)
}

pos_log <- function(column){
  col_min = min(column, na.rm=T)
  a = 1-col_min
  column = column+a
  return(log(column))
}

neg_log <- function(column){
  col_max = max(column, na.rm=T)
  column = col_max+1-column
  return(log(column))
}
```

```{r}
fits = list.files(path=input_dir, pattern = "All")
```

```{r eval=FALSE}
# Save plot of neglogprob distributions for all subject for each model
for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  p = data %>% 
  ggplot(aes(neglogprob))+
  geom_histogram()+
  facet_wrap(~sub_id, scales='free')
  p_name = gsub('.csv','',f)
  p_name = gsub('LearningParams','Neglogs',p_name)
  ggsave(paste0(p_name, '.jpeg'), plot=p, device = 'jpeg', path = fig_path, width = 30, height = 30, units = "in", limitsize = FALSE)
}
```

```{r}
#Look up df for AIC and BIC calculation
num_pars_df = data.frame(model = c('LearningParams_Fit_alpha_Fix_beta-exp_','LearningParams_Fit_alpha_neg_Fix_alpha_pos-beta-exp_','LearningParams_Fit_alpha_neg-alpha_pos_Fix_beta-exp_','LearningParams_Fit_alpha_neg-alpha_pos-beta_Fix_exp_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg_Fix_exp_pos_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha_neg-alpha_pos-beta-exp_pos_Fix_exp_neg_','LearningParams_Fit_alpha_neg-alpha_pos-exp_Fix_beta_','LearningParams_Fit_alpha_neg-alpha_pos-exp_neg_Fix_beta-exp_pos_','LearningParams_Fit_alpha_neg-alpha_pos-exp_neg-exp_pos_Fix_beta_','LearningParams_Fit_alpha_neg-alpha_pos-exp_pos_Fix_beta-exp_neg_','LearningParams_Fit_alpha_neg-beta_Fix_alpha_pos-exp_','LearningParams_Fit_alpha_neg-beta-exp_neg_Fix_alpha_pos-exp_pos_','LearningParams_Fit_alpha_neg-beta-exp_pos_Fix_alpha_pos-exp_neg_','LearningParams_Fit_alpha_neg-exp_Fix_alpha_pos-beta_','LearningParams_Fit_alpha_neg-exp_neg_Fix_alpha_pos-beta-exp_pos_','LearningParams_Fit_alpha_neg-exp_neg-exp_pos_Fix_alpha_pos-beta_','LearningParams_Fit_alpha_neg-exp_pos_Fix_alpha_pos-beta-exp_neg_','LearningParams_Fit_alpha_pos_Fix_alpha_neg-beta-exp_','LearningParams_Fit_alpha_pos-beta-exp_Fix_alpha_neg_','LearningParams_Fit_alpha_pos-beta-exp_neg_Fix_alpha_neg-exp_pos_','LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_','LearningParams_Fit_alpha_pos-beta-exp_pos_Fix_alpha_neg-exp_neg_','LearningParams_Fit_alpha_pos-exp_Fix_alpha_neg-beta_','LearningParams_Fit_alpha_pos-exp_neg_Fix_alpha_neg-beta-exp_pos_','LearningParams_Fit_alpha_pos-exp_neg-exp_pos_Fix_alpha_neg-beta_','LearningParams_Fit_alpha_pos-exp_pos_Fix_alpha_neg-beta-exp_neg_','LearningParams_Fit_alpha-beta-exp_Fix_','LearningParams_Fit_alpha-beta-exp_neg_Fix_exp_pos_','LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_','LearningParams_Fit_alpha-beta-exp_pos_Fix_exp_neg_','LearningParams_Fit_alpha-exp_Fix_beta_','LearningParams_Fit_alpha-exp_neg_Fix_beta-exp_pos_','LearningParams_Fit_alpha-exp_neg-exp_pos_Fix_beta_','LearningParams_Fit_alpha-exp_pos_Fix_beta-exp_neg_','LearningParams_Fit_beta-exp_Fix_alpha_','LearningParams_Fit_beta-exp_neg-exp_pos_Fix_alpha_'), pars = c(1,1,2,3,4,4,5,4,3,3,4,3,2,3,3,2,2,3,2,1,3,3,4,3,2,2,3,2,2,3,4,3,2,2,3,2,2,3))
num_pars_df = num_pars_df %>%
  mutate(model = as.character(model))
```

The behavior in the machine game task lends itself to prediction error modeling as frequently done in the literature. 

In this approach the probability of playing a machine is modeled as: 

$p(k_{t} = 1) = \frac{e^{\beta*(EV_t)}}{1+e^{\beta*(EV_t)}}$  

where the $EV_t$ is updated after observing the reward ($r$) in each trial at a learning rate ($\alpha$) by a prediction error that can be distorted non-linearly by an exponent ($\gamma$)  

${EV_{t+1}} = {EV_t} + \alpha * (r - {EV_t})^\gamma$  

The parameters of the model are:  

- $\alpha$ - learning rate. Higher values mean faster learning. Can be allowed to vary for gains and losses as $\alpha_{pos}$ and $\alpha_{neg}$   
- $\gamma$ - value concavity exponent. Higher values mean less distortion of prediction error. Can be omitted (i.e. fixed to 1) and allowed to vary for gains and losses as $exp_{pos}$ and $exp_{neg}$  
- $\beta$ - inverse temperature. Higher values mean subjects are choosing based on expected value, lower means the choice is driven less by EV and more by random guessing (for $\beta = 0$ all choices are equally likely).  

To determine the best model we ran 38 models where these parameters were allowed to vary for gains and losses and were either estimated for each subject or fixed for all subjects.  

Each model was fit 50 times for each subject minimizing negative log probability. Fit quality is evaluated using BIC for each fit. 

## Model comparison

Which is the best model?

```{r}
sub_pars = data.frame()

for(f in fits){
  data = read.csv(paste0(input_dir, f))
  data = process_fits(data)
  data = data %>%
    group_by(sub_id) %>%
    slice(which.min(neglogprob)) %>%
    mutate(model = as.character(model)) %>%
    left_join(num_pars_df, by='model') %>%
    mutate(AIC = 2*neglogprob+2*pars,
           BIC = 2*neglogprob+pars*log(180))
  sub_pars = rbind.all.columns(sub_pars,data.frame(data))
}

sub_pars = sub_pars %>%
  mutate(age_group = ifelse(sub_id<200000, "kid", ifelse(sub_id>200000 & sub_id<300000, "teen", "adult")),
         age_group = factor(age_group, levels = c("kid","teen","adult"))) %>%
  drop_na(age_group)
```

```{r message = FALSE, warning = FALSE}
sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(model = gsub("LearningParams_", "", model),
         min_col = factor(ifelse(mean_bic == min(mean_bic, na.rm=T), 1,0))) %>%
  ggplot(aes(factor(model), mean_bic))+
  geom_point(aes(col = min_col))+
  geom_errorbar(aes(ymin=mean_bic-sem_bic, ymax=mean_bic+sem_bic, col=min_col))+
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "none")+
  xlab("")+
  ylab("Mean BIC across subjects")+
  scale_color_manual(values = c("black", "red"))
```

How to read model names: Parameters that are fitted are listed following the `Fit` keyword in the model name, parameters that are fixed are listed following the `Fix` keyword in the model name.  

For example in `Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_` the parameters `alpha_pos`, `beta`, `exp_neg` and `exp_pos` are fitted to data while `alpha_neg` is fixed.  


```{r}
sub_pars %>%
  select(model, BIC) %>%
  group_by(model) %>%
  summarise(mean_bic = mean(BIC), 
            sem_bic = sem(BIC)) %>%
  mutate(model = gsub("LearningParams_", "", model)) %>%
  arrange(mean_bic)
```

### Age differences in fit

Do models differ in fit by age group? No. Models are consistently best fit to adults but the difference is very small.

```{r}
sub_pars %>%
  group_by(age_group, model) %>%
  summarise(mean_bic = mean(BIC),
            sem_bic = mean(BIC)) %>%
  ggplot(aes(model,mean_bic, color=age_group))+
  geom_point()+
  geom_errorbar(aes(ymin = mean_bic-sem_bic, ymax = mean_bic+sem_bic))+
  xlab("")+
  ylab("Mean BIC across subjects")+
  theme(axis.text.x = element_blank(),
        legend.title = element_blank())
```

## Age differences in parameters

### Best fitting model

The model with the lowest BIC is `Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_`

**Log transformed** parameter distributions

```{r warning=FALSE, message=FALSE}
sub_pars %>%
  filter(model == "LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_")%>% 
  select_if(~sum(!is.na(.)) > 0) %>%
  select(-model, -pars, -neglogprob, -sub_id, -AIC, -BIC, -contains("x0")) %>%
  mutate(xopt_alpha_pos_log = pos_log(xopt_alpha_pos),
         xopt_beta_log = pos_log(xopt_beta),
         xopt_exp_neg_log = neg_log(xopt_exp_neg),
         xopt_exp_pos_log = pos_log(xopt_exp_pos)) %>%
  select(-xopt_alpha_pos, -xopt_beta, -xopt_exp_neg, -xopt_exp_pos) %>%
  gather(par, value, -age_group) %>%
  ggplot(aes(value))+
  geom_density(aes(fill=age_group), color=NA, alpha=0.5)+
  facet_wrap(~par, scales='free')+
  xlab("")+
  theme(legend.position = "none")
```

```{r warning=FALSE, message=FALSE}
sub_pars %>%
  filter(model == "LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_")%>% 
  select_if(~sum(!is.na(.)) > 0) %>%
  select(-model, -pars, -neglogprob, -sub_id, -AIC, -BIC, -contains("x0")) %>%
  mutate(xopt_alpha_pos_log = pos_log(xopt_alpha_pos),
         xopt_beta_log = pos_log(xopt_beta),
         xopt_exp_neg_log = neg_log(xopt_exp_neg),
         xopt_exp_pos_log = pos_log(xopt_exp_pos)) %>%
  select(-xopt_alpha_pos, -xopt_beta, -xopt_exp_neg, -xopt_exp_pos) %>%
  gather(par, value, -age_group) %>%
  ggplot(aes(age_group, log(value)))+
  geom_boxplot(aes(fill=age_group))+
  facet_wrap(~par, scales="free")+
  xlab("")+
  theme(legend.position = "none")
```

No age difference in learning rates for gains.

```{r}
summary(lm(pos_log(xopt_alpha_pos) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_")))
```

No age difference in how much the decision relies on EV.

```{r}
summary(lm(pos_log(xopt_beta) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_")))
```

No age difference in value distortion of losses.

```{r}
summary(lm(neg_log(xopt_exp_neg) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_")))
```

**Adults distort positive prediction errors less than kids.**

```{r}
summary(lm(pos_log(xopt_exp_pos) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha_pos-beta-exp_neg-exp_pos_Fix_alpha_neg_")))
```

The BIC of the best model is not much lower than several other models. One of these models that is comparable (**is there a way to test this?**) in fit is `Fit_alpha-beta-exp_neg-exp_pos_Fix_` where none of the parameters is fixied. Since I don't really have a theoretical reason for fixing any parameter or fixing it any of the parameters at values that I have I prefer this model that is free of this restrictions. 

### Preferred model

```{r warning=FALSE, message=FALSE}
sub_pars %>%
  filter(model == "LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_")%>% 
  filter(xopt_exp_pos > (-3))%>%
  select_if(~sum(!is.na(.)) > 0) %>%
  select(-model, -pars, -neglogprob, -sub_id, -AIC, -BIC, -contains("x0")) %>%
  mutate(xopt_alpha_log = pos_log(xopt_alpha),
         xopt_beta_log = pos_log(xopt_beta),
         xopt_exp_neg_log = neg_log(xopt_exp_neg),
         xopt_exp_pos_log = pos_log(xopt_exp_pos)) %>%
  select(-xopt_alpha, -xopt_beta, -xopt_exp_neg, -xopt_exp_pos) %>%
  gather(par, value, -age_group) %>%
  ggplot(aes(value))+
  geom_density(aes(fill=age_group), color=NA, alpha=0.5)+
  facet_wrap(~par, scales='free')+
  xlab("")+
  theme(legend.position = "none")
```

```{r warning=FALSE, message=FALSE}
sub_pars %>%
  filter(model == "LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_")%>% 
  select_if(~sum(!is.na(.)) > 0) %>%
  filter(xopt_exp_pos > (-3))%>%
  select(-model, -pars, -neglogprob, -sub_id, -AIC, -BIC, -contains("x0")) %>%
  mutate(xopt_alpha_log = pos_log(xopt_alpha),
         xopt_beta_log = pos_log(xopt_beta),
         xopt_exp_neg_log = neg_log(xopt_exp_neg),
         xopt_exp_pos_log = pos_log(xopt_exp_pos)) %>%
  select(-xopt_alpha, -xopt_beta, -xopt_exp_neg, -xopt_exp_pos) %>%
  gather(par, value, -age_group) %>%
  ggplot(aes(age_group, value))+
  geom_boxplot(aes(fill=age_group))+
  facet_wrap(~par, scales="free")+
  xlab("")+
  theme(legend.position = "none")
```

No difference in alpha

```{r}
summary(lm(pos_log(xopt_alpha) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

**Adults make decisions based more on EV compared to kids**

```{r}
summary(lm(pos_log(xopt_beta) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

**Adults distort negative outcomes less. A negative outcome doesn't feel as bad as it is for kids** 

```{r}
summary(lm(neg_log(xopt_exp_neg) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

No difference in distortion of positive prediction errors.

```{r}
summary(lm(pos_log(xopt_exp_pos) ~ age_group, sub_pars %>%
  filter(model == "LearningParams_Fit_alpha-beta-exp_neg-exp_pos_Fix_")))
```

**The best fitting model and the preferred model suggest that age differences lie in different parameters. How should this be resolved?**