---
title: "ROI analyses"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/helper_functions/ggplot_colors.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')

source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
exp_exp_models = c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_",
                   "Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_",
                   "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_", 
                   "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos-lossave_Fix_",
                   "Fit_alpha_neg-alpha_pos-beta-exp-lossave_Fix_", 
                   "Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")

library(lme4)
library(broom)
library(psych)
learner_info = read.csv("~/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_3/learner_info.csv")
learner_info = learner_info %>%
  select(-non_learner) %>%
  rename(sub_id = Sub_id) %>%
  mutate(sub_id = as.numeric(as.character(gsub("sub-", "", sub_id))))

```

# ROI data

## Group differences in PE-related activity

- The only difference we found between the two learner groups in response to task-related regressors was for value sensitivity. 

- Schoenberg et al. (2007) groups their participants similarly based on task performance. They find an uncorrected significant difference between the two learner groups in the caudate/ dorsal striatum which survives small volume correction in an anatomically defined ROI. They interpret this as higher PE related activity for learners compared to non-learners ('engagement of RL signals'). Further, they regress parameter estimates from the peak voxel onto a behavioral measure (*individual difference analysis*).    

- We did not find any difference that survived whole-brain correction between learners and non-learners in response to RPE related activity. Relaxing the threshold to examine RL-related activity more closely for the two learner groups we find:  
  - For LPE we find higher activity for learners in the ACC/dMPFC.    
  - For HPE we find higher activity for non-learners in bilateral caudate and PCC.  

- These are not easily compatible with Schoenberg's findings (in fact might even suggest the opposite since we find larger PE related striatal activity for non-learners compared to learners) but warrant a closer look at these ROIs. We compared PE related activity in 8 apriori ROIs from an earlier meta-analysis. We have two PE related regressors for high and low variance stimuli.  

These are the parameter estimates of PE regressors extracted from level 1's (i.e. each run of each subject) from voxels in 8 apriori ROIs defined in Bartra et al.

```{r}
all_roi_pe_betas = read.csv('~/Dropbox/PoldrackLab/DevStudy_Analyses/input/rois/all_roi_pe_betas.csv')
all_roi_pe_betas = all_roi_pe_betas %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner")) %>%
  filter(abs(value)<10)
all_roi_pe_betas
```

How should this data be analyzed to answer "Does activitiy in ROI X differ between learners and non-learners"?  

Thinking through a single ROI: In the l-vstr ROI there are 81 voxels. One can look at:   
1. parameter estimate with highest absolute value --> might inflate an outlier
2. mean of parameter estimates from all voxels in ROI --> might wash out effect
**3. parameter estimate in each voxel separately and then checking whether any effect is consistent across an ROI. (i.e. compare the distribution of 36 values to 38 values 81 times)**

### HPE

Is there a difference between the distributions of HPE parameter estimates of learner for any of the ROIs? Not when looking at adjusted p's. As expected from the GLMs (model 3). So relaxing it to look at non-adjusted p's

```{r}
rois = unique(all_roi_pe_betas$roi)
out = data.frame()
for(i in 1:length(rois)){
  
  cur_roi = rois[i]
  
  cur_out = all_roi_pe_betas %>% 
    filter(regressor == "hpe" & roi == cur_roi) %>%
    select(-regressor, -roi, -value_type, -X) %>%
    group_by(sub_num, run_num) %>%
    mutate(vox_num = 1:n()) %>%
    group_by(vox_num) %>%
    do(tidy(lm(value ~ run_num*learner,.))) %>%
    filter(term=="learnerNon-learner") %>%
    select(-estimate,-std.error)%>%
    arrange(p.value) %>%
    ungroup()%>%
    mutate(adj_p_value = p.adjust(p.value),
           roi = cur_roi) %>%
    # filter(adj_p_value<0.05)
    filter(p.value<0.05)
  
  if(nrow(cur_out)>0){
    print(paste0("Detected voxels in ",as.character(cur_roi), " that have different means between the learner groups"))
    out = rbind(out, cur_out)
  }
}
```

```{r}
out = out %>%
  mutate(ext = paste(roi, vox_num, sep = "_"))
table(out$roi)
out
```

Looking at means of parameter estimates per voxel for ease of vieweing (otherwise they are more confusing than helpful due to the range of values; plus this is what the regressions are comparing)

Visually comparing the two groups it looks like the larger range of the parameters estimates for non-learners is more noticable than the mean difference between the groups. 

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  mutate(ext = paste(roi, vox_num, sep = "_")) %>%
  filter(ext %in% out$ext) %>%
  ungroup()%>%
  group_by(ext, learner) %>%
  summarise(mean_val = mean(value),
            sem_val = sem(value),
            roi= unique(roi)) %>%
  ggplot(aes(ext, mean_val, color=learner))+
  geom_point(position = position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val),position = position_dodge(width = 0.9), width=0.2)+
  # ggplot(aes(ext, value, color=learner))+
  # geom_boxplot()+
  facet_wrap(~roi, scales="free")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Mean parameter estimate for voxel")
```

### LPE

```{r}
rois = unique(all_roi_pe_betas$roi)
out = data.frame()
for(i in 1:length(rois)){
  
  cur_roi = rois[i]
  
  cur_out = all_roi_pe_betas %>% 
    filter(regressor == "lpe" & roi == cur_roi) %>%
    select(-regressor, -roi, -value_type, -X) %>%
    group_by(sub_num, run_num) %>%
    mutate(vox_num = 1:n()) %>%
    group_by(vox_num) %>%
    do(tidy(lm(value ~ run_num*learner,.))) %>%
    filter(term=="learnerNon-learner") %>%
    select(-estimate,-std.error)%>%
    arrange(p.value) %>%
    ungroup()%>%
    mutate(adj_p_value = p.adjust(p.value),
           roi = cur_roi) %>%
    # filter(adj_p_value<0.05)
    filter(p.value<0.05)
  
  if(nrow(cur_out)>0){
    print(paste0("Detected voxels in ",as.character(cur_roi), " that have different means between the learner groups"))
    out = rbind(out, cur_out)
  }
}
```

```{r}
out = out %>%
  mutate(ext = paste(roi, vox_num, sep = "_"))
table(out$roi)
out
```

Again comparing the two groups larger range of the parameters estimates for non-learners is more noticable than the mean difference between the groups. 

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  mutate(ext = paste(roi, vox_num, sep = "_")) %>%
  filter(ext %in% out$ext) %>%
  ungroup()%>%
  group_by(ext, learner) %>%
  summarise(mean_val = mean(value),
            sem_val = sem(value),
            roi= unique(roi)) %>%
  ggplot(aes(ext, mean_val, color=learner))+
  geom_point(position = position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val),position = position_dodge(width = 0.9), width=0.2)+
  # ggplot(aes(ext, value, color=learner))+
  # geom_boxplot()+
  facet_wrap(~roi, scales="free")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Mean parameter estimate for voxel")
```

*Could that larger variability of betas for non-learners suggest that they are better suited for relating these values to other values? Because they have lower kurtosis/are more evently spread out?*

## Kurtosis

A way to capture between-subjects variability of different types of measures. Kurtosis captures the likelihood (not in the quantitative sense) of an outlier in a distribution. The more evenly spread out a distribution the lower the kurtosis.

Is the kurtosis of parameter estimates for non-learners from ROIs lower than learners? Yes.
**Note that although the difference in distributions between the two learner groups is very stark the estimation of these neural parameter estimates was independent of this post-hoc behavioral delineation of the sample.**
**Can we say that in some sense what the learners' brains are doing are more similar to each other compared to non-learners' brains that are all over the place?**

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  ungroup() %>%
  group_by(roi, vox_num, learner, regressor) %>%
  summarise(kurt = kurtosi(value)) %>%
  ungroup() %>%
  group_by(roi, learner, regressor)%>%
  summarise(mean_kur = mean(kurt),
            sem_kur = sem(kurt)) %>%
  ggplot(aes(roi, mean_kur, col=learner))+
  geom_point(position = position_dodge(width = .9))+
  geom_errorbar(aes(ymin = mean_kur-sem_kur, ymax = mean_kur+sem_kur), position = position_dodge(width = .9), width=.2)+
  facet_wrap(~regressor)+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Mean kurtosis of parameter estimates")
```

Is the kurtosis of parameter estimates for non-learners from ROIs more similar to those of the behavioral RL parameter estimates? (i.e. more suitable for individual difference analyses)

What counts as ab individiaul difference 'measure' in this ROI data
Average betas per ROI?
...?

To get sense of what this should work like do it for the behavioral parameters first:
```{r}
best_sub_pars %>%
  select(sub_id, contains("xopt"), model, x_axis,age_group, learner) %>%
  filter(model %in% exp_exp_models) %>%
  gather(key, value, -sub_id, -model, -x_axis, -age_group, -learner) %>%
  group_by(model, key, learner) %>%
  drop_na() %>%
  summarise(kurt = kurtosi(value)) %>%
  arrange(kurt) %>%
  select(kurt, key, learner, everything()) %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner")) %>%
  ungroup() %>%
  group_by(key, learner) %>%
  summarise(mean_kur = mean(kurt),
            sem_kur = sem(kurt)) %>%
  ggplot(aes(key, mean_kur, col=learner))+
  geom_point(position = position_dodge(width = .9))+
  geom_errorbar(aes(ymin = mean_kur-sem_kur, ymax = mean_kur+sem_kur), position = position_dodge(width = .9), width=.2)+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Mean kurtosis of parameter estimates")
```

Ok so ROI measures start with average ROI activity (which I think is what is used mostly)
In general start with getting the 1 per subject measure and then get its kurtosis

these have more variability than the behavioral measures? It's not that their outliers can be further apart from their means that I care for. It's how likely it is that there is an outlier vs how evenly spread out the distribution is.

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, regressor, roi) %>%
  summarise(m_val = mean(value)) %>%
  ungroup() %>%
  group_by(regressor, roi) %>%
  summarise(kurt = kurtosi(m_val)) %>%
  arrange(kurt)
```

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, regressor, roi) %>%
  summarise(m_val = mean(value)) %>%
  arrange(m_val) %>%
  filter(roi == "l_vstr" & regressor == "hpe") %>%
  left_join(best_sub_pars %>%
              select(sub_id, contains("xopt"), model, x_axis,age_group, learner) %>%
              filter(model %in% exp_exp_models[1]) %>%
              gather(key, value, -sub_id, -model, -x_axis, -age_group, -learner) %>%
              group_by(model, key) %>%
              drop_na() %>%
              filter(key == "xopt_beta") %>%
              rename(sub_num = sub_id)%>%
              ungroup()%>%
              select(sub_num, value), by="sub_num") %>%
  ggplot(aes(value, m_val))+
  geom_point()
```

Then check "peak values" for each ROI

```{r}

```


## Brain-behavior correlation

Brain - parameter correlations? (Remember the question of interest is: what is the best/a good marker of self-regulation)

```{r}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
```

Is average PE related activity in any of the ROIs associated with the any model parameters?

This does 696 comparisons 
9 rois


```{r}
roi_par_cors = all_roi_pe_zvals %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner"),
         roi = factor(roi, levels = c("l_vstr", "r_vstr", "vmpfc", "l_ains", "r_ains", "pcc", "acc", "pre_sma", "r_dstr"))) %>%
  group_by(sub_num, learner, roi, regressor) %>%
  summarise(max_zval = mean(value),
            min_zval = min(value)) %>%
  # mutate(max_zval = ifelse(abs(max_zval)>abs(min_zval), max_zval, min_zval)) %>%
  select(-min_zval)%>%
  ungroup() %>%
  rename(sub_id = sub_num) %>% 
  left_join(best_sub_pars %>%
              filter(model %in% exp_exp_models)%>%
              select(sub_id, model, contains("xopt")) %>%
              select_if(~sum(!is.na(.)) > 0), by="sub_id") %>%
  gather(par, estimate, -sub_id, -learner, -roi, -regressor, -max_zval, -model) %>%
  group_by(model, roi, par, regressor) %>%
  drop_na()%>%
  do(tidy(lm(scale(max_zval) ~ scale(estimate)*learner, .))) 
```

```{r}
roi_par_cors %>% 
  filter(term != "(Intercept)") %>%
  mutate(adj_p.value = p.adjust(p.value)) %>%
  filter(adj_p.value<0.05)
```

```{r}
col_col_df = 
  roi_par_cors %>% 
  filter(term != "(Intercept)") %>%
  mutate(adj_p.value = p.adjust(p.value)) %>%
  filter(adj_p.value<0.05) %>%
  mutate(col_col = paste(roi, par, regressor, model, sep="-")) %>%
  ungroup()%>%
  select(col_col, roi, par, regressor, model) %>%
  distinct()
table(col_col_df$roi)
table(col_col_df$par)
table(col_col_df$regressor)
table(col_col_df$model)

#focusing on
col_col_df = col_col_df %>%
  filter(roi == "pcc") %>%
  filter(regressor == "hpe")
```

If I take the peak per ROI per subject:

```{r warning=FALSE, message=FALSE}
all_roi_pe_zvals %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner"),
         roi = factor(roi, levels = c("l_vstr", "r_vstr", "vmpfc", "l_ains", "r_ains", "pcc", "acc", "pre_sma", "r_dstr"))) %>%
  group_by(sub_num, learner, roi, regressor) %>%
  summarise(max_zval = max(value),
            min_zval = min(value)) %>%
  mutate(max_zval = ifelse(abs(max_zval)>abs(min_zval), max_zval, min_zval)) %>%
  select(-min_zval)%>%
  ungroup() %>%
  rename(sub_id = sub_num) %>% 
  left_join(best_sub_pars %>%
              filter(model %in% exp_exp_models)%>%
              select(sub_id, model, contains("xopt")) %>%
              select_if(~sum(!is.na(.)) > 0), by="sub_id") %>%
  gather(par, estimate, -sub_id, -learner, -roi, -regressor, -max_zval, -model) %>%
  mutate(col_col = paste(roi, par, regressor, model, sep="-")) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(model, x_axis), by="model")%>%
  filter(col_col %in% col_col_df$col_col) %>%
  filter(roi == "l_vstr") %>%
  ggplot(aes(estimate, max_zval, col=learner))+
  geom_point() +
  facet_grid(par~x_axis)+
  theme(panel.grid = element_blank())+
  xlab("Behavior")+
  ylab("Brain")+
  xlim(c(0,1))

#ggsave("Brain-behavior_ROIs.jpeg", device = "jpeg", path = fig_path, width = 5, height = 4, units = "in", dpi = 450)  
```

If I take the mean

```{r warning=FALSE, message=FALSE}
all_roi_pe_zvals %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner"),
         roi = factor(roi, levels = c("l_vstr", "r_vstr", "vmpfc", "l_ains", "r_ains", "pcc", "acc", "pre_sma", "r_dstr"))) %>%
  group_by(sub_num, learner, roi, regressor) %>%
  summarise(max_zval = mean(value),
            min_zval = min(value)) %>%
  #mutate(max_zval = ifelse(abs(max_zval)>abs(min_zval), max_zval, min_zval)) %>%
  select(-min_zval)%>%
  ungroup() %>%
  rename(sub_id = sub_num) %>% 
  left_join(best_sub_pars %>%
              filter(model %in% exp_exp_models)%>%
              select(sub_id, model, contains("xopt")) %>%
              select_if(~sum(!is.na(.)) > 0), by="sub_id") %>%
  gather(par, estimate, -sub_id, -learner, -roi, -regressor, -max_zval, -model) %>%
  mutate(col_col = paste(roi, par, regressor, model, sep="-")) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              select(model, x_axis), by="model")%>%
  filter(col_col %in% col_col_df$col_col) %>%
  # filter(roi == "pcc" & regressor == "hpe") %>%
  ggplot(aes(estimate, max_zval, col=learner))+
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", se=FALSE)+
  facet_grid(x_axis~par, scales="free")+
  theme(panel.grid = element_blank())+
  xlab("Behavior")+
  ylab("Brain")
```

# Functional connectivity

Data from different ROIs can be analyzed with respect to each other instead of in isolation as well.  

In this case, for example, we examined whether the time series in an apriori l_vstr seed was similar to any other voxels time series.  
**(SHOULD THIS BE ONE TIME STEP AHEAD?)**

Prior literature has examples suggesting that such connectivity patterns might differ depending on performance (van den Bos, et al.).  

If so, we could expect differentces between the learner and non-learner groups.  

We do not find any voxels where the time series correlates significantly/above 0 on average for all subjects *This doesn't sound right when looking at the boxplots of correlations. Except for pre-sma the average correlations between an ROI and l-vstr seed are always >0*
**(CORRELATION BETWEEN TIME SERIES; WHAT WOULD BE THEIR T/Z/P-VALUES? You know the number of time points and can calculate t values based on that)**

This is the same for both learner groups as well. **(DON'T THINK I CHECKED FOR THIS STATISTICALLY. EXTRACT AVERAGE CORRELATION VALUES FOR LEARNERS AND NON-LEARNERS FROM WHOLE BRAIN AND COMPARE TO EACH OTHER?)**


```{r}
input_path = "~/Dropbox/PoldrackLab/DevStudy_Analyses/input/"
all_cors = read.csv(paste0(input_path, 'func_cor/all_l_vstr_cors.csv'))
all_cors
```

```{r}
all_cors %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  # group_by(learner, roi) %>%
  group_by(sub_num,learner, roi) %>%
  summarise(mean_cor = mean(cor),
            sem_cor = sem(cor)) %>%
  ggplot(aes(roi, mean_cor, fill=learner))+
  geom_boxplot()+
  # geom_bar(stat="identity", position = position_dodge(width = 0.9))+
  # geom_errorbar(aes(ymin= mean_cor-sem_cor, ymax=mean_cor+sem_cor), position = position_dodge(width = 0.9), width=0.25)+
  theme(legend.title = element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Mean Correlation with L-vStr seed")

ggsave("FC_lvstr_mean.jpeg", device = "jpeg", path = fig_path, width = 5, height = 4, units = "in", dpi = 450)    
```

None of the interactions is significant.

```{r}
tmp = all_cors %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  group_by(sub_num, learner, roi) %>%
  summarise(mean_cor = mean(cor))

summary(lm(mean_cor ~ roi*learner, tmp))
```

Is the change in connectivity different between the learner groups?

```{r}
all_cors %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  group_by(sub_num,learner, roi, run_num) %>%
  summarise(mean_cor = mean(cor)) %>%
  spread(run_num, mean_cor) %>%
  mutate(ch_1_to_6 = `1`-`6`) %>%
  ggplot(aes(roi, ch_1_to_6, fill=learner))+
  geom_boxplot()+
  theme(legend.title = element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Change in Mean Correlation \nwith L-vStr seed")
```

Connectivity data seems to vary more between participants. Is it suitable to be related to behavioral parameter estimates?

```{r}
fc_par_cors = all_cors %>%
  group_by(sub_num, roi)%>%
  summarise(mean_cor= mean(cor)) %>%
  rename(sub_id = sub_num) %>% 
  left_join(best_sub_pars %>%
              filter(model %in% exp_exp_models)%>%
              select(sub_id, learner, model, contains("xopt")) %>%
              select_if(~sum(!is.na(.)) > 0), by="sub_id") %>%
  filter(xopt_beta<3) %>%
  gather(key, value, -roi, -sub_id, -learner, -model, -mean_cor)%>%
  group_by(model, roi, key) %>%
  drop_na()%>%
  do(tidy(lm(mean_cor ~ value*learner, data = .))) %>%
  filter(term != "(Intercept)") %>%
  filter(p.value<0.05) 

fc_par_cors

```

```{r warning=FALSE, message=FALSE}
tmp = all_cors %>%
  group_by(sub_num, roi)%>%
  summarise(mean_cor= mean(cor)) %>%
  rename(sub_id = sub_num)%>% 
  left_join(best_sub_pars %>%
              filter(model %in% names(table(fc_par_cors$model))) %>%
              select(sub_id, learner, contains("xopt"), model), by="sub_id") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  filter(xopt_beta <3) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              filter(model %in% exp_exp_models) %>%
              select(model, x_axis), by = "model") %>%
  gather(key, value, -sub_id, -roi, -mean_cor, -learner,-model, -x_axis)

tmp$col_cor = NA

#this works but takes a while
# for(i in 1:nrow(tmp)){
#   a = with(tmp[i,], paste0(roi,model,key))
#   for(j in 1:nrow(fc_par_cors)){
#     b = with(fc_par_cors[j,], paste0(roi,model,key))
#     if(a == b){
#       tmp$col_cor[i] = fc_par_cors$term[j]
#     }
#   }
# }
```  

```{r}
tmp %>%
  filter(roi %in% c("l_ains", "pcc", "r_ains", "r_vstr", "vmpfc")) %>%
  filter(key == "xopt_alpha_neg")%>%
  filter(model %in% c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_","Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_") == FALSE) %>%
  ggplot(aes(value, mean_cor))+
  geom_point(aes(col=learner), alpha=0.5)+
  geom_smooth(method="lm", aes(col=learner, linetype=col_cor), se=FALSE)+
  facet_grid(roi~x_axis, labeller = label_wrap_gen(10))+
  theme(panel.grid = element_blank(),
        legend.title = element_blank())+
  ylab("Mean FC with l-Vstr seed")+
  xlab("\u03b1_loss")
```

The story that the brain behavior correlation tells you is about different cognitive processes than those that relate to behavior.

```{r}
tmp %>%
  filter(roi %in% c("l_ains", "pcc", "r_ains", "r_vstr", "vmpfc")) %>%
  filter(key %in%  c("xopt_alpha_neg","xopt_alpha_pos","xopt_beta", "xopt_exp"))%>%
  filter(model %in% c("Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")) %>%
  ggplot(aes(value, mean_cor))+
  geom_point(aes(col=learner), alpha=0.5)+
  geom_smooth(method="lm", aes(col=learner, linetype=col_cor), se=FALSE)+
  facet_grid(roi~key, labeller = label_wrap_gen(10))+
  theme(panel.grid = element_blank(),
        legend.title = element_blank())+
  ylab("Mean FC with l-Vstr seed")
```