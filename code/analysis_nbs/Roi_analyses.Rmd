---
title: "ROI analyses"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/helper_functions/ggplot_colors.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')

source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
exp_exp_models = c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_",
                   "Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_",
                   "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_", 
                   "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos-lossave_Fix_",
                   "Fit_alpha_neg-alpha_pos-beta-exp-lossave_Fix_", 
                   "Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")

library(lme4)
library(broom)
library(psych)
learner_info = read.csv("~/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_3/learner_info.csv")
learner_info = learner_info %>%
  select(-non_learner) %>%
  rename(sub_id = Sub_id) %>%
  mutate(sub_id = as.numeric(as.character(gsub("sub-", "", sub_id))))

```

# ROI data

## Group differences in PE-related activity

- The only difference we found between the two learner groups in response to task-related regressors was for value sensitivity. 

- Schoenberg et al. (2007) groups their participants similarly based on task performance. They find an uncorrected significant difference between the two learner groups in the caudate/ dorsal striatum which survives small volume correction in an anatomically defined ROI. They interpret this as higher PE related activity for learners compared to non-learners ('engagement of RL signals'). Further, they regress parameter estimates from the peak voxel onto a behavioral measure (*individual difference analysis*).    

- We did not find any difference that survived whole-brain correction between learners and non-learners in response to RPE related activity. Relaxing the threshold to examine RL-related activity more closely for the two learner groups we find:  
  - For LPE we find higher activity for learners in the ACC/dMPFC.    
  - For HPE we find higher activity for non-learners in bilateral caudate and PCC.  

- These are not easily compatible with Schoenberg's findings (in fact might even suggest the opposite since we find larger PE related striatal activity for non-learners compared to learners) but warrant a closer look at these ROIs. We compared PE related activity in 9 apriori ROIs from an earlier meta-analysis. We have two PE related regressors for high and low variance stimuli.  

These are the parameter estimates of PE regressors extracted from level 1's (i.e. each run of each subject) from voxels in 9 apriori ROIs defined in Bartra et al.

```{r}
all_roi_pe_betas = read.csv('~/Dropbox/PoldrackLab/DevStudy_Analyses/input/rois/all_roi_pe_betas.csv')
all_roi_pe_betas = all_roi_pe_betas %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner")) %>%
  filter(abs(value)<10)
all_roi_pe_betas
```

How should this data be analyzed to answer "Does activitiy in ROI X differ between learners and non-learners"?  

Thinking through a single ROI: In the l-vstr ROI there are 81 voxels. One can look at:   
1. parameter estimate with highest absolute value --> might inflate an outlier
2. mean of parameter estimates from all voxels in ROI --> might wash out effect
**3. parameter estimate in each voxel separately and then checking whether any effect is consistent across an ROI. (i.e. compare the distribution of 36 values to 38 values 81 times)**

### HPE

Is there a difference between the distributions of HPE parameter estimates of learner for any of the ROIs? Not when looking at adjusted p's. As expected from the GLMs (model 3). So relaxing it to look at non-adjusted p's

```{r}
rois = unique(all_roi_pe_betas$roi)
out = data.frame()
for(i in 1:length(rois)){
  
  cur_roi = rois[i]
  
  cur_out = all_roi_pe_betas %>% 
    filter(regressor == "hpe" & roi == cur_roi) %>%
    select(-regressor, -roi, -value_type, -X) %>%
    group_by(sub_num, run_num) %>%
    mutate(vox_num = 1:n()) %>%
    group_by(vox_num) %>%
    do(tidy(lm(value ~ run_num*learner,.))) %>%
    filter(term=="learnerNon-learner") %>%
    select(-estimate,-std.error)%>%
    arrange(p.value) %>%
    ungroup()%>%
    mutate(adj_p_value = p.adjust(p.value),
           roi = cur_roi) %>%
    filter(p.value<0.05)
  
  if(nrow(cur_out)>0){
    print(paste0("Detected voxels in ",as.character(cur_roi), " that have different means between the learner groups"))
    out = rbind(out, cur_out)
  }
}
```

```{r}
out = out %>%
  mutate(ext = paste(roi, vox_num, sep = "_"))
table(out$roi)
out
```

Looking at means of parameter estimates per voxel for ease of vieweing (otherwise they are more confusing than helpful due to the range of values; plus this is what the regressions are comparing)

Visually comparing the two groups it looks like the larger range of the parameters estimates for non-learners is more noticable than the mean difference between the groups. 

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  mutate(ext = paste(roi, vox_num, sep = "_")) %>%
  filter(ext %in% out$ext) %>%
  ungroup()%>%
  group_by(ext, learner) %>%
  summarise(mean_val = mean(value),
            sem_val = sem(value),
            roi= unique(roi)) %>%
  ggplot(aes(ext, mean_val, color=learner))+
  geom_point(position = position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val),position = position_dodge(width = 0.9), width=0.2)+
  facet_wrap(~roi, scales="free")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Mean HPE parameter estimate for voxel")
```

### LPE

```{r}
rois = unique(all_roi_pe_betas$roi)
out = data.frame()
for(i in 1:length(rois)){
  
  cur_roi = rois[i]
  
  cur_out = all_roi_pe_betas %>% 
    filter(regressor == "lpe" & roi == cur_roi) %>%
    select(-regressor, -roi, -value_type, -X) %>%
    group_by(sub_num, run_num) %>%
    mutate(vox_num = 1:n()) %>%
    group_by(vox_num) %>%
    do(tidy(lm(value ~ run_num*learner,.))) %>%
    filter(term=="learnerNon-learner") %>%
    select(-estimate,-std.error)%>%
    arrange(p.value) %>%
    ungroup()%>%
    mutate(adj_p_value = p.adjust(p.value),
           roi = cur_roi) %>%
    # filter(adj_p_value<0.05)
    filter(p.value<0.05)
  
  if(nrow(cur_out)>0){
    print(paste0("Detected voxels in ",as.character(cur_roi), " that have different means between the learner groups"))
    out = rbind(out, cur_out)
  }
}
```

```{r}
out = out %>%
  mutate(ext = paste(roi, vox_num, sep = "_"))
table(out$roi)
out
```

Again comparing the two groups larger range of the parameters estimates for non-learners is more noticable than the mean difference between the groups. 

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  mutate(ext = paste(roi, vox_num, sep = "_")) %>%
  filter(ext %in% out$ext) %>%
  ungroup()%>%
  group_by(ext, learner) %>%
  summarise(mean_val = mean(value),
            sem_val = sem(value),
            roi= unique(roi)) %>%
  ggplot(aes(ext, mean_val, color=learner))+
  geom_point(position = position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val),position = position_dodge(width = 0.9), width=0.2)+
  facet_wrap(~roi, scales="free")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Mean LPE parameter estimate for voxel")
```

Looking closer at the parameter estimates in each voxel of all ROIs did not reveal any region where there was a clear majority of voxels that showed a difference between the learning groups. It did reveal a difference in the between subject variability of the parameter estimates for each learner group.

*Could that larger variability of betas for non-learners suggest that they are better suited for relating these values to other values? Because they have lower kurtosis/are more evently spread out?*

## Kurtosis

The kurtosis of a DV distribution describes the between-subjects variability and is comparable across different types of measures. It captures how unlikely an outlier would be in a distribution. The more evenly spread out a distribution the lower the kurtosis. Two distributions cannot relate to each other strongly if they do not have comparable spreads.  

First we check whether this is true for all voxels within each ROI. [Yes]  

Then we compare the different summary statistics one can extract from ROIs in their between subject variability. [mostly the same conclusion regardless of what is extracted from the ROI]  

Finally we compare the kurtosis of brain measures to behavioral RL parameter estimates to determine whether an individual difference analysis relating the two data types is sensible. [No]  

### Brain 

Is the kurtosis of *raw* parameter estimates for non-learners from ROIs lower than learners? Yes. The spread of the distributions can be made more similar between the learning groups by (log-) transforming (not more similar between brain and behavior measures).  

Two things to note from this graph:  

**Note that although the difference in distributions between the two learner groups is very stark the estimation of these neural parameter estimates was independent of this post-hoc behavioral delineation of the sample.**  

**Can we say that in some sense what the learners' brains are doing are more similar to each other compared to non-learners' brains that are all over the place?**  

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  ungroup() %>%
  group_by(roi, vox_num, learner, regressor) %>%
  summarise(kurt = kurtosi(value)) %>%
  ungroup() %>%
  group_by(roi, learner, regressor)%>%
  summarise(mean_kur = mean(kurt),
            sem_kur = sem(kurt)) %>%
  ggplot(aes(roi, mean_kur, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  geom_errorbar(aes(ymin = mean_kur-sem_kur, ymax = mean_kur+sem_kur), position = position_dodge(width = .9), width=.2)+
  facet_wrap(~regressor)+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Mean kurtosis of parameter estimates \n across voxel of ROI")
```

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, regressor, roi, learner) %>%
  summarise(m_val = mean(value)) %>%
  ungroup() %>%
  group_by(regressor, roi, learner) %>%
  summarise(kurt = kurtosi(m_val)) %>%
  ggplot(aes(roi, kurt, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  facet_grid(.~regressor)+
  theme(panel.grid=element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Kurtosis of mean parameter \nestimates across voxels of ROI")
```

Then check "peak values" for each ROI

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, regressor, roi, learner) %>%
  summarise(m_val = max(abs(value))) %>%
  ungroup() %>%
  group_by(regressor, roi, learner) %>%
  summarise(kurt = kurtosi(m_val)) %>%
  ggplot(aes(roi, kurt, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  facet_grid(.~regressor)+
  theme(panel.grid=element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Kurtosis of peak parameter \nestimates across voxels of ROI")
```

### Behavior

Is the kurtosis of parameter estimates for non-learners from ROIs more similar to those of the behavioral RL parameter estimates? (i.e. more suitable for individual difference analyses)

The distributions of behavioral parameter estimates do not show a similar divergence between the learner groups. They also have much lower kurtosis overall. **Does this make a brain-behavior correlation/individual difference analysis attempt useless?**

```{r}
best_sub_pars %>%
  select(sub_id, contains("xopt"), model, x_axis, age_group, learner) %>%
  filter(model %in% exp_exp_models) %>%
  gather(key, value, -sub_id, -model, -x_axis, -age_group, -learner) %>%
  group_by(x_axis, key, learner) %>%
  drop_na() %>%
  summarise(kurt = kurtosi(value)) %>%
  ungroup()%>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner"),
         key = gsub("xopt_", "", key),
         key = ifelse(key == "alpha", "\u03b1", ifelse(key=="alpha_pos", "\u03b1_pos", ifelse(key=="alpha_neg", "\u03b1_neg", ifelse(key=="beta", "\u03b2", ifelse(key == "exp", "\u03b3", ifelse(key == "exp_neg", "\u03b3_neg", ifelse(key == "exp_pos", "\u03b3_pos", ifelse(key == "lossave", "\u03bb", NA))))))))) %>%
  ggplot(aes(key, kurt, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  facet_wrap(~x_axis, scales="free", labeller = label_wrap_gen(10))+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Kurtosis of RL parameter estimates")
```

# Functional connectivity

Data from different ROIs can be analyzed with respect to each other instead of in isolation as well.  

In this case, for example, we examined whether the time series in an apriori l_vstr seed was similar to any other voxels' time series in 8 other subjective value network ROI spheres.  

Prior literature has examples suggesting that connectivity patterns might differ depending on age.
van den Bos, et al. (2012) extract the time series of striatal seed regions that correlate with prediction errors.
They normalize these time series and multiply is with binary negative and positive prediction error regressors to create two interaction (PPI) regressors
Level 1 images per subject for pos>neg feedback was entered into a level 2 with an age regressor
They found that correlation between mpfc and vstr was higher for pos feedback compared to negative feedback and that this correlation increased with age
Finally they also correlated functional connectivity strength with RL parameters and found a negative relationship between learning rate for negative feedback

If so, we could expect differentces between the learner and non-learner groups.  

We do not find any voxels where the time series correlates significantly/above 0 on average for all subjects *This doesn't sound right when looking at the boxplots of correlations. Except for pre-sma the average correlations between an ROI and l-vstr seed are always >0*
**(CORRELATION BETWEEN TIME SERIES; WHAT WOULD BE THEIR T/Z/P-VALUES? You know the number of time points and can calculate t values based on that)**

This is the same for both learner groups as well. **(DON'T THINK I CHECKED FOR THIS STATISTICALLY. EXTRACT AVERAGE CORRELATION VALUES FOR LEARNERS AND NON-LEARNERS FROM WHOLE BRAIN AND COMPARE TO EACH OTHER?)**

### Seed2vox whole task

### Seed2seed whole task

### Seed2vox or seed? for M1 vs others


This is the correlation of the TS for the lvstr seed with each voxel in each ROI for each run for the whole task (accounting for confounds)
```{r}
input_path = "~/Dropbox/PoldrackLab/DevStudy_Analyses/input/"
all_cors = read.csv(paste0(input_path, 'func_cor/all_l_vstr_cors.csv'))
all_cors
```

```{r}
all_cors %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  # group_by(learner, roi) %>%
  group_by(sub_num,learner, roi) %>%
  summarise(mean_cor = mean(cor),
            sem_cor = sem(cor)) %>%
  ggplot(aes(roi, mean_cor, fill=learner))+
  geom_boxplot()+
  # geom_bar(stat="identity", position = position_dodge(width = 0.9))+
  # geom_errorbar(aes(ymin= mean_cor-sem_cor, ymax=mean_cor+sem_cor), position = position_dodge(width = 0.9), width=0.25)+
  theme(legend.title = element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Mean Correlation with L-vStr seed")

ggsave("FC_lvstr_mean.jpeg", device = "jpeg", path = fig_path, width = 5, height = 4, units = "in", dpi = 450)    
```

None of the interactions is significant.

```{r}
tmp = all_cors %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  group_by(sub_num, learner, roi) %>%
  summarise(mean_cor = mean(cor))

summary(lm(mean_cor ~ roi*learner, tmp))
```

Connectivity data seems to vary more between participants. Is it suitable to be related to behavioral parameter estimates?

```{r}
fc_par_cors = all_cors %>%
  group_by(sub_num, roi)%>%
  summarise(mean_cor= mean(cor)) %>%
  rename(sub_id = sub_num) %>% 
  left_join(best_sub_pars %>%
              filter(model %in% exp_exp_models)%>%
              select(sub_id, learner, model, contains("xopt")) %>%
              select_if(~sum(!is.na(.)) > 0), by="sub_id") %>%
  filter(xopt_beta<3) %>%
  gather(key, value, -roi, -sub_id, -learner, -model, -mean_cor)%>%
  group_by(model, roi, key) %>%
  drop_na()%>%
  do(tidy(lm(mean_cor ~ value*learner, data = .))) %>%
  filter(term != "(Intercept)") %>%
  filter(p.value<0.05) 

fc_par_cors

```

```{r warning=FALSE, message=FALSE}
tmp = all_cors %>%
  group_by(sub_num, roi)%>%
  summarise(mean_cor= mean(cor)) %>%
  rename(sub_id = sub_num)%>% 
  left_join(best_sub_pars %>%
              filter(model %in% names(table(fc_par_cors$model))) %>%
              select(sub_id, learner, contains("xopt"), model), by="sub_id") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  filter(xopt_beta <3) %>%
  left_join(num_pars_df %>%
              mutate(model = gsub("LearningParams_", "", model)) %>%
              filter(model %in% exp_exp_models) %>%
              select(model, x_axis), by = "model") %>%
  gather(key, value, -sub_id, -roi, -mean_cor, -learner,-model, -x_axis)

tmp$col_cor = NA

#this works but takes a while
# for(i in 1:nrow(tmp)){
#   a = with(tmp[i,], paste0(roi,model,key))
#   for(j in 1:nrow(fc_par_cors)){
#     b = with(fc_par_cors[j,], paste0(roi,model,key))
#     if(a == b){
#       tmp$col_cor[i] = fc_par_cors$term[j]
#     }
#   }
# }
```  

```{r}
tmp %>%
  filter(roi %in% c("l_ains", "pcc", "r_ains", "r_vstr", "vmpfc")) %>%
  filter(key == "xopt_alpha_neg")%>%
  filter(model %in% c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_","Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_") == FALSE) %>%
  ggplot(aes(value, mean_cor))+
  geom_point(aes(col=learner), alpha=0.5)+
  geom_smooth(method="lm", aes(col=learner, linetype=col_cor), se=FALSE)+
  facet_grid(roi~x_axis, labeller = label_wrap_gen(10))+
  theme(panel.grid = element_blank(),
        legend.title = element_blank())+
  ylab("Mean FC with l-Vstr seed")+
  xlab("\u03b1_loss")
```

The story that the brain behavior correlation tells you is about different cognitive processes than those that relate to behavior.

```{r}
tmp %>%
  filter(roi %in% c("l_ains", "pcc", "r_ains", "r_vstr", "vmpfc")) %>%
  filter(key %in%  c("xopt_alpha_neg","xopt_alpha_pos","xopt_beta", "xopt_exp"))%>%
  filter(model %in% c("Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")) %>%
  ggplot(aes(value, mean_cor))+
  geom_point(aes(col=learner), alpha=0.5)+
  geom_smooth(method="lm", aes(col=learner, linetype=col_cor), se=FALSE)+
  facet_grid(roi~key, labeller = label_wrap_gen(10))+
  theme(panel.grid = element_blank(),
        legend.title = element_blank())+
  ylab("Mean FC with l-Vstr seed")
```