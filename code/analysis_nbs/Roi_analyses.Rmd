---
title: "ROI analyses"
output:
github_document:
toc: yes
toc_float: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/output/figures/'

from_gh=FALSE
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/helper_functions/ggplot_colors.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/transform_remove_skew.R')

source('/Users/zeynepenkavi/Dropbox/PoldrackLab/DevStudy_Analyses/code/workspace_scripts/rl_fits_data.R')
exp_exp_models = c("Fit_alpha-beta-exp_neg-exp_pos-lossave_Fix_",
                   "Fit_alpha-beta-exp_neg-exp_pos_Fix_lossave_",
                   "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos_Fix_lossave_", 
                   "Fit_alpha_neg-alpha_pos-beta-exp_neg-exp_pos-lossave_Fix_",
                   "Fit_alpha_neg-alpha_pos-beta-exp-lossave_Fix_", 
                   "Fit_alpha_neg-alpha_pos-beta-exp_Fix_lossave_")

library(lme4)
library(broom)
library(psych)
learner_info = read.csv("~/Dropbox/PoldrackLab/DevStudy_ServerScripts/nistats/level_3/learner_info.csv")
learner_info = learner_info %>%
  select(-non_learner) %>%
  rename(sub_id = Sub_id) %>%
  mutate(sub_id = as.numeric(as.character(gsub("sub-", "", sub_id))))

```

# ROI data

## Group differences in PE-related activity

- The only difference we found between the two learner groups in response to task-related regressors was for value sensitivity. 

- Schoenberg et al. (2007) groups their participants similarly based on task performance. They find an uncorrected significant difference between the two learner groups in the caudate/ dorsal striatum which survives small volume correction in an anatomically defined ROI. They interpret this as higher PE related activity for learners compared to non-learners ('engagement of RL signals'). Further, they regress parameter estimates from the peak voxel onto a behavioral RL parameter estimate. Thus they conduct an *individual difference analysis* relating neural and behavioral findings.    

- We did not find any difference that survived whole-brain correction between learners and non-learners in response to RPE related activity. Relaxing the threshold to examine RL-related activity more closely for the two learner groups we find:  
  - For LPE we find higher activity for learners in the ACC/dMPFC.    
  - For HPE we find higher activity for non-learners in bilateral caudate and PCC.  

- These are not easily compatible with Schoenberg's findings (in fact might even suggest the opposite since we find larger PE related striatal activity for non-learners compared to learners) but warrant a closer look at these ROIs. We compared PE related activity in 9 apriori ROIs from an earlier meta-analysis. We have two PE related regressors for high and low variance stimuli.  

These are the parameter estimates of PE regressors extracted from level 1's (i.e. each run of each subject) from voxels in 9 apriori ROIs defined in Bartra et al.

```{r}
all_roi_pe_betas = read.csv('~/Dropbox/PoldrackLab/DevStudy_Analyses/input/rois/all_roi_pe_betas.csv')
all_roi_pe_betas = all_roi_pe_betas %>%
  left_join(learner_info %>% rename(sub_num=sub_id), by="sub_num") %>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner")) %>%
  filter(abs(value)<10)
all_roi_pe_betas
```

How should this data be analyzed to answer "Does activitiy in ROI X differ between learners and non-learners"?  

Thinking through a single ROI: In the l-vstr ROI there are 81 voxels. One can look at:   
1. parameter estimate with highest absolute value --> might inflate an outlier
2. mean of parameter estimates from all voxels in ROI --> might wash out effect
**3. parameter estimate in each voxel separately and then checking whether any effect is consistent across an ROI. (i.e. compare the distribution of 36 values to 38 values 81 times)**

### HPE

Is there a difference between the distributions of HPE parameter estimates of learner for any of the ROIs? Not when looking at adjusted p's. As expected from the GLMs (model 3). So relaxing it to look at non-adjusted p's

```{r}
rois = unique(all_roi_pe_betas$roi)
out = data.frame()
for(i in 1:length(rois)){
  
  cur_roi = rois[i]
  
  cur_out = all_roi_pe_betas %>% 
    filter(regressor == "hpe" & roi == cur_roi) %>%
    select(-regressor, -roi, -value_type, -X) %>%
    group_by(sub_num, run_num) %>%
    mutate(vox_num = 1:n()) %>%
    group_by(vox_num) %>%
    do(tidy(lm(value ~ run_num*learner,.))) %>%
    filter(term=="learnerNon-learner") %>%
    select(-estimate,-std.error)%>%
    arrange(p.value) %>%
    ungroup()%>%
    mutate(adj_p_value = p.adjust(p.value),
           roi = cur_roi) %>%
    filter(p.value<0.05)
  
  if(nrow(cur_out)>0){
    print(paste0("Detected voxels in ",as.character(cur_roi), " that have different means between the learner groups"))
    out = rbind(out, cur_out)
  }
}
```

```{r}
out = out %>%
  mutate(ext = paste(roi, vox_num, sep = "_"))
table(out$roi)
out
```

Looking at means of parameter estimates per voxel for ease of vieweing (otherwise they are more confusing than helpful due to the range of values; plus this is what the regressions are comparing)

Visually comparing the two groups it looks like the larger range of the parameters estimates for non-learners is more noticable than the mean difference between the groups. 

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  mutate(ext = paste(roi, vox_num, sep = "_")) %>%
  filter(ext %in% out$ext) %>%
  ungroup()%>%
  group_by(ext, learner) %>%
  summarise(mean_val = mean(value),
            sem_val = sem(value),
            roi= unique(roi)) %>%
  ggplot(aes(ext, mean_val, color=learner))+
  geom_point(position = position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val),position = position_dodge(width = 0.9), width=0.2)+
  facet_wrap(~roi, scales="free")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Mean HPE parameter estimate for voxel")

ggsave("HPE_roi.jpeg", device = "jpeg", path = fig_path, width = 6, height = 4, units = "in", dpi = 450)
```

### LPE

```{r}
rois = unique(all_roi_pe_betas$roi)
out = data.frame()
for(i in 1:length(rois)){
  
  cur_roi = rois[i]
  
  cur_out = all_roi_pe_betas %>% 
    filter(regressor == "lpe" & roi == cur_roi) %>%
    select(-regressor, -roi, -value_type, -X) %>%
    group_by(sub_num, run_num) %>%
    mutate(vox_num = 1:n()) %>%
    group_by(vox_num) %>%
    do(tidy(lm(value ~ run_num*learner,.))) %>%
    filter(term=="learnerNon-learner") %>%
    select(-estimate,-std.error)%>%
    arrange(p.value) %>%
    ungroup()%>%
    mutate(adj_p_value = p.adjust(p.value),
           roi = cur_roi) %>%
    # filter(adj_p_value<0.05)
    filter(p.value<0.05)
  
  if(nrow(cur_out)>0){
    print(paste0("Detected voxels in ",as.character(cur_roi), " that have different means between the learner groups"))
    out = rbind(out, cur_out)
  }
}
```

```{r}
out = out %>%
  mutate(ext = paste(roi, vox_num, sep = "_"))
table(out$roi)
out
```

Again comparing the two groups larger range of the parameters estimates for non-learners is more noticable than the mean difference between the groups. 

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor) %>%
  mutate(vox_num = 1:n()) %>%
  mutate(ext = paste(roi, vox_num, sep = "_")) %>%
  filter(ext %in% out$ext) %>%
  ungroup()%>%
  group_by(ext, learner) %>%
  summarise(mean_val = mean(value),
            sem_val = sem(value),
            roi= unique(roi)) %>%
  ggplot(aes(ext, mean_val, color=learner))+
  geom_point(position = position_dodge(width = 0.9))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val),position = position_dodge(width = 0.9), width=0.2)+
  facet_wrap(~roi, scales="free")+
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Mean LPE parameter estimate for voxel")

ggsave("LPE_roi.jpeg", device = "jpeg", path = fig_path, width = 6, height = 4, units = "in", dpi = 450)    
```

Looking closer at the parameter estimates in each voxel of all ROIs did not reveal any region where there was a clear majority of voxels that showed a difference between the learning groups. It did reveal a difference in the between subject variability of the parameter estimates for each learner group.

*Could that larger variability of betas for non-learners suggest that they are better suited for relating these values to other values? Because they have lower kurtosis/are more evenly spread out?*

## Kurtosis

The kurtosis of a DV distribution describes the between-subjects variability and is comparable across different types of measures. It captures how unlikely an outlier would be in a distribution. The more evenly spread out a distribution the lower the kurtosis. Two distributions cannot relate to each other strongly if they do not have comparable spreads.  

First we check whether this is true for all voxels within each ROI. [Yes]  

Then we compare the different summary statistics one can extract from ROIs in their between subject variability. [mostly the same conclusion regardless of what is extracted from the ROI]  

Finally we compare the kurtosis of brain measures to behavioral RL parameter estimates to determine whether an individual difference analysis relating the two data types is sensible. [No]  

### Brain 

Is the kurtosis of *raw* parameter estimates for non-learners from ROIs lower than learners? Yes. The spread of the distributions can be made more similar between the learning groups by (log-) transforming (not more similar between brain and behavior measures).  

Two things to note from this graph:  

**Note that although the difference in distributions between the two learner groups is very stark the estimation of these neural parameter estimates was independent of this post-hoc behavioral delineation of the sample.**  

**Can we say that in some sense what the learners' brains are doing are more similar to each other compared to non-learners' brains that are all over the place?**  

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, run_num, roi, regressor, learner) %>%
  mutate(vox_num = 1:n()) %>%
  summarise(mean_val=mean(value)) %>%
  ungroup() %>%
  group_by(roi, learner, regressor)%>%
  summarise(sem_val = sem(mean_val),
            mean_val = mean(mean_val)) %>%
  ggplot(aes(roi, mean_val, col=learner))+
  geom_point(stat="identity",position = position_dodge(width = .4))+
  geom_errorbar(aes(ymin = mean_val-sem_val, ymax = mean_val+sem_val), position = position_dodge(width = .4), width=.2)+
  facet_wrap(~regressor)+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Parameter estimates \n across voxel of ROI")

ggsave("ROI_variance.jpeg", device = "jpeg", path = fig_path, width = 9, height = 3, units = "in", dpi = 450)    
```

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, regressor, roi, learner) %>%
  summarise(m_val = mean(value)) %>%
  ungroup() %>%
  group_by(regressor, roi, learner) %>%
  summarise(kurt = kurtosi(m_val)) %>%
  ggplot(aes(roi, kurt, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  facet_grid(.~regressor)+
  theme(panel.grid=element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Kurtosis of mean parameter \nestimates across voxels of ROI")
```

Then check "peak values" for each ROI

```{r}
all_roi_pe_betas %>%
  group_by(sub_num, regressor, roi, learner) %>%
  summarise(m_val = max(abs(value))) %>%
  ungroup() %>%
  group_by(regressor, roi, learner) %>%
  summarise(kurt = kurtosi(m_val)) %>%
  ggplot(aes(roi, kurt, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  facet_grid(.~regressor)+
  theme(panel.grid=element_blank(), 
        legend.title = element_blank())+
  xlab("")+
  ylab("Kurtosis of peak parameter \nestimates across voxels of ROI")
```

### Behavior

Is the kurtosis of parameter estimates for non-learners from ROIs more similar to those of the behavioral RL parameter estimates? (i.e. more suitable for individual difference analyses)

The distributions of behavioral parameter estimates do not show a similar divergence between the learner groups. They also have much lower kurtosis overall. **Does this make a brain-behavior correlation/individual difference analysis attempt useless?**

```{r}
best_sub_pars %>%
  select(sub_id, contains("xopt"), model, x_axis, age_group, learner) %>%
  filter(model %in% exp_exp_models) %>%
  gather(key, value, -sub_id, -model, -x_axis, -age_group, -learner) %>%
  group_by(x_axis, key, learner) %>%
  drop_na() %>%
  summarise(kurt = kurtosi(value)) %>%
  ungroup()%>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner"),
         key = gsub("xopt_", "", key),
         key = ifelse(key == "alpha", "\u03b1", ifelse(key=="alpha_pos", "\u03b1_pos", ifelse(key=="alpha_neg", "\u03b1_neg", ifelse(key=="beta", "\u03b2", ifelse(key == "exp", "\u03b3", ifelse(key == "exp_neg", "\u03b3_neg", ifelse(key == "exp_pos", "\u03b3_pos", ifelse(key == "lossave", "\u03bb", NA))))))))) %>%
  ggplot(aes(key, kurt, fill=learner))+
  geom_bar(stat="identity",position = position_dodge(width = .9))+
  facet_wrap(~x_axis, scales="free", labeller = label_wrap_gen(20))+
  theme(legend.title=element_blank(),
        panel.grid = element_blank())+
  xlab("")+
  ylab("Kurtosis of RL parameter estimates")

ggsave("RL_kurtosis.jpeg", device = "jpeg", path = fig_path, width = 8, height = 6, units = "in", dpi = 450)    
```

# Functional connectivity

Data from different ROIs can be analyzed with respect to each other instead of in isolation as well.  

Prior literature has examples suggesting that connectivity patterns might differ depending on age. For example, van den Bos, et al. (2012) extract the time series of striatal seed regions that correlate with prediction errors. They normalize these time series and multiply with binary negative and positive prediction error regressors to create two interaction (PPI) regressors. Level 1 images per subject for pos>neg feedback was entered into a level 2 with an age regressor. They found that correlation between mPFC and vStr was higher for positive feedback compared to negative feedback and that this correlation increased with age. Connecting their neural results to their behavioral results they also correlated functional connectivity strength with RL parameters and found a negative relationship between learning rate for negative feedback.

Inspired by these results we checked for differentces between the learner and non-learner groups in connectivity patterns as well. We did this in three ways. 

Where we found differences between the learner groups we followed up with analyses connecting the neural results with behavioral findings.

### PPI

First we conducted a PPI analysis following the example in the literature. We checked whether the association between a l_vstr seed and any other voxel changed depending on the machine. We created four interaction regressors for each machine. At group level we compared these interactions to baseline, across age groups and between the learner groups. We did not find any difference in connectivity between the l_vstr seed and other voxels depending on the condition or any differences across age or learner groups in connectivity.

Details of the PPI results can be found [here](https://github.com/zenkavi/DevStudy_fmri-nbs/blob/master/notebooks/connectivity_sandbox.ipynb).

### Seed2seed

```{r}
all_seed2seed = read.csv(paste0(input_path, "/func_con/seed2seed/all_seed2seed_cors.csv"))
all_seed2seed
```

```{r}
seed2seed_tests = all_seed2seed %>%
  left_join(learner_info %>% rename(subnum=sub_id), by="subnum") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner")) %>%
  mutate(seeds = paste(seed1, seed2, sep="-")) %>%
  group_by(seeds) %>%
  do(tidy(lm(cor_val ~ runnum*learner, data = .)))

seed2seed_tests

seed2seed_runef = seed2seed_tests %>%
  filter(term == "runnum")
seed2seed_runef$adj_p = p.adjust(seed2seed_runef$p.value)

seed2seed_learnef = seed2seed_tests %>%
  filter(term == "learnerNon-learner")
seed2seed_learnef$adj_p = p.adjust(seed2seed_learnef$p.value)

seed2seed_runlearnint = seed2seed_tests %>%
  filter(term == "runnum:learnerNon-learner") %>%
  mutate(adj_p = p.adjust(p.value))
seed2seed_runlearnint$adj_p = p.adjust(seed2seed_runlearnint$p.value)
```

```{r}
seed2seed_runef %>%
  filter(p.value<0.05)
```

```{r}
seed2seed_learnef %>%
  filter(p.value<0.05)
```

```{r}
seed2seed_runlearnint %>%
  filter(p.value<0.05)
```

```{r}
all_seed2seed %>%
  left_join(learner_info %>% rename(subnum=sub_id), by="subnum") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner"),
         seeds = paste(seed1, seed2, sep="-"),
         runnum= factor(runnum)) %>%
  filter(seeds %in% c("l_ains-pre_sma", "vmpfc-acc")) %>%
  # ggplot(aes(as.numeric(runnum), cor_val, col=learner))+
  # geom_smooth(method="lm", alpha=.25)+
  group_by(seeds, runnum, learner) %>%
  summarise(mean_cor_val = mean(cor_val),
            sem_cor_val = sem(cor_val)) %>%
  ggplot(aes(runnum, mean_cor_val, col=learner))+
  geom_point(position = position_dodge(width=.5))+
  geom_errorbar(aes(ymin = mean_cor_val-sem_cor_val, ymax=mean_cor_val+sem_cor_val), position=position_dodge(width=.5), width=0.2)+
  facet_wrap(~seeds)+
  xlab("Run")+
  ylab("Mean Partial Correlation")+
  theme(legend.title = element_blank(), 
        panel.grid=element_blank())
  # scale_x_continuous(breaks = c(1:6))

ggsave("PartialCorrs.jpeg", device = "jpeg", path = fig_path, width = 5, height = 3, units = "in", dpi = 450)   
```

#### Marker?

So is this a "marker"? Does it correlate with behavior? (In some sense it already does since the learner groups are defined by behavior)
Does it correlate with any behavioral RL parameters?

First: are the distributions comparable? Yes!

```{r}
all_seed2seed %>%
  left_join(learner_info %>% rename(subnum=sub_id), by="subnum") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner"),
         seeds = paste(seed1, seed2, sep="-"),
         runnum= factor(runnum)) %>%
  filter(seeds %in% c("l_vstr-pcc", "vmpfc-acc")) %>%
  group_by(learner, seeds, runnum) %>%
  summarise(kurt = kurtosi(cor_val)) %>%
  arrange(kurt)
```

```{r}
best_sub_pars %>%
  select(sub_id, contains("xopt"), model, x_axis, age_group, learner) %>%
  filter(model %in% exp_exp_models) %>%
  gather(key, value, -sub_id, -model, -x_axis, -age_group, -learner) %>%
  group_by(x_axis, key, learner) %>%
  drop_na() %>%
  summarise(kurt = kurtosi(value)) %>%
  ungroup()%>%
  mutate(learner=ifelse(learner == 1, "Learner", "Non-learner"),
         key = gsub("xopt_", "", key),
         key = ifelse(key == "alpha", "\u03b1", ifelse(key=="alpha_pos", "\u03b1_pos", ifelse(key=="alpha_neg", "\u03b1_neg", ifelse(key=="beta", "\u03b2", ifelse(key == "exp", "\u03b3", ifelse(key == "exp_neg", "\u03b3_neg", ifelse(key == "exp_pos", "\u03b3_pos", ifelse(key == "lossave", "\u03bb", NA))))))))) %>%
  arrange(kurt)
```

What relationship is same across models for both learner groups:

std of slopes for both learner groups for the brain-behavior relationship across models for each run

```{r}
all_seed2seed %>%
  select(-X)%>%
  left_join(learner_info %>% rename(subnum=sub_id), by="subnum") %>%
  mutate(learner = ifelse(learner == 1, "Learner", "Non-learner"),
         seeds = paste(seed1, seed2, sep="-"),
         runnum= factor(runnum)) %>%
  filter(seeds %in% c("l_vstr-pcc", "vmpfc-acc")) %>%
  left_join(best_sub_pars %>%
  select(sub_id, contains("xopt"), model, x_axis) %>%
  filter(model %in% exp_exp_models) %>%
  rename(subnum = sub_id), by = "subnum") %>%
  gather(key, value, -seed1, -seed2, -cor_val, -subnum, -runnum, -learner, -seeds, -model, -x_axis)%>%
  group_by(seeds, runnum, key, x_axis) %>%
  drop_na()%>%
  do(tidy(lm(cor_val~value*learner, data=.))) %>%
  select(-std.error, -statistic, -p.value) %>%
  filter(term != "(Intercept)") %>%
  spread(term, estimate) %>%
  mutate(non_learner_slope = value+`learnerNon-learner`+`value:learnerNon-learner`) %>%
  rename(learner_slope=value) %>%
  select(-`learnerNon-learner`,-`value:learnerNon-learner`) %>%
  gather(slope_type, value, -seeds, -runnum, -key, -x_axis) %>%
  group_by(runnum, seeds, key, slope_type) %>%
  summarise(mean_slope = mean(value),
            sem_slope = sem(value)) %>%
  ungroup()%>%
  mutate(key = gsub("xopt_", "", key))%>%
  mutate(key = ifelse(key == "alpha", "\u03b1", ifelse(key=="alpha_pos", "\u03b1_pos", ifelse(key=="alpha_neg", "\u03b1_neg", ifelse(key=="beta", "\u03b2", ifelse(key == "exp", "\u03b3", ifelse(key == "exp_neg", "\u03b3_neg", ifelse(key == "exp_pos", "\u03b3_pos", ifelse(key == "lossave", "\u03bb", NA))))))))) %>%
  ggplot(aes(runnum, mean_slope, col = slope_type))+
  geom_errorbar(aes(ymin = mean_slope-sem_slope, ymax = mean_slope+sem_slope), position=position_dodge(width = .9), width=.2)+
  geom_hline(aes(yintercept=0),linetype="dashed")+
  facet_grid(key~seeds, scale="free")+
  theme(legend.title = element_blank(), 
        panel.grid=element_blank())+
  xlab("Run")+
  ylab("Mean brain-behavior correlation")

ggsave("tmp.jpeg", device = "jpeg", path = fig_path, width = 10, height = 8, units = "in", dpi = 450)
```

