---
title: "Developmental changes in sensitivity to high variance feedback relates to differences in risky decision making"
output: 
html_document:
toc: true
toc_depts: 2
---

```{r setup_env, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(gridExtra)
library(GGally)
library(lme4)
sem<-function(x)sd(x, na.rm=T)/sqrt(length(x[!is.na(x)]))
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values=cbbPalette) + scale_color_manual(values=cbbPalette)+theme(legend.position="bottom")
```

This notebook contains exploratory analyses of behavioral data collected to investigate the relationship between risk taking behavior and probabilistic learning. 

The sample consists of three age groups: kids, teens and adults and we hypothesize that sensitivity to learn from high variance feedback improves with age and this is related to better risky decisions.  

Subjects completed a probabilistic learning task in the scanner, a realistic risky decision making task (BART) outside the scanner and numerous questionnaires. The focus of this notebook is on the first two tasks.  

The plan of analysis is to establish that adults are more sensitive to high variance feedback in the probabilistic learning task and relate this (modeled) sensitivity to behavior in BART.  

```{r echo=FALSE}
##################
#Machine game data
##################
file_list <- list.files("/Users/zeynepenkavi/Downloads/machine_game")

for (file in file_list){
  
  tmp <- read.csv(paste0("/Users/zeynepenkavi/Downloads/machine_game/",file))
  tmp$Sub_id <- as.numeric(gsub("[^0-9]", "", file))
  tmp$Trial_number <- 1:nrow(tmp)
  
  if('X' %in% names(tmp)){
    tmp <- tmp[,-which(names(tmp) == "X")]
  }
  
  if (file == file_list[1]){
    machine_game_data = tmp
  }
  else{
    machine_game_data = rbind(machine_game_data, tmp)
  }
  rm(tmp)
}

rm(file, file_list)

#Remove subjects with incomplete data
incomplete_subs <- as.numeric(names(which(table(machine_game_data$Sub_id)!=180)))

machine_game_data_clean <- machine_game_data[machine_game_data$Sub_id %in% incomplete_subs == F,]

rm(incomplete_subs)

#Add cols for machine properties
assign.machine.info <- function(data){
  data$facet_labels <- with(data, ifelse(Trial_type == 1, "+5,-495", ifelse(Trial_type == 2, "-5,+495", ifelse(Trial_type == 3, "-10,+100", ifelse(Trial_type == 4, "+10,-100", NA)))))
  
  data$gain_mag <- with(data, ifelse(Trial_type == 1, "5", ifelse(Trial_type == 2, "495", ifelse(Trial_type == 3, "100", ifelse(Trial_type == 4, "10", NA)))))
  
  data$loss_mag <- with(data, ifelse(Trial_type == 1, "495", ifelse(Trial_type == 2, "5", ifelse(Trial_type == 3, "10", ifelse(Trial_type == 4, "100", NA)))))
  
  data$gain_freq <- with(data, ifelse(Trial_type == 1, "90", ifelse(Trial_type == 2, "10", ifelse(Trial_type == 3, "50", ifelse(Trial_type == 4, "50", NA)))))
  
  data$loss_freq <-  with(data, ifelse(Trial_type == 1, "10", ifelse(Trial_type == 2, "90", ifelse(Trial_type == 3, "50", ifelse(Trial_type == 4, "50", NA)))))
  
  data$magnitude <- with(data, ifelse(Trial_type == 1, "large", ifelse(Trial_type == 2, "large", ifelse(Trial_type == 3, "small", ifelse(Trial_type == 4, "small", NA)))))
  
  data$variance <- with(data, ifelse(Trial_type == 1, "high", ifelse(Trial_type == 2, "high", ifelse(Trial_type == 3, "low", ifelse(Trial_type == 4, "low", NA)))))
  
  return(data)
}

machine_game_data_clean <- assign.machine.info(machine_game_data_clean)


##################
#Demographics data
##################
demog_data <- read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/Sarah_Developmental study/Final Redcap Data/DevelopmentalStudy_DATA_2015-03-25_1258.csv')

# head(names(demog_data), 20)
# str(demog_data$subj_id)
# str(demog_data$calc_age)

# Add age data to machine_game_data_clean
assign.age.info <- function(data){
  
  data$age_group <- with(data, ifelse(Sub_id<200000, "kid", ifelse(Sub_id>200000 & Sub_id<300000, "teen", "adult")))
  
  data %>%
    group_by(Sub_id) %>%
    left_join(demog_data[,c('subj_id', 'calc_age')], by = c("Sub_id" = "subj_id")) %>%
    mutate(age_group=factor(age_group, levels=c('kid', 'teen', 'adult')))
}

machine_game_data_clean <- assign.age.info(machine_game_data_clean)

# Check if there are any problems with the merged ages
# summary(machine_game_data_clean$calc_age)
# which(is.na(machine_game_data_clean$calc_age))

machine_game_data_clean = machine_game_data_clean  %>%
  mutate(correct1_incorrect0 = ifelse(facet_labels %in% c('-10,+100', '-5,+495') & Response ==1,1,ifelse(facet_labels %in% c('+10,-100', '+5,-495') & Response ==2,1,0)),
         Response = factor(Response, levels = c(0,1,2) ,labels=c('time-out', 'play', 'pass')))

##########
#Bart data
##########
file_list <- list.files("/Users/zeynepenkavi/Downloads/bart_tsv", pattern = '*.tsv')

for (file in file_list){
  
  tmp <- read.csv(paste0("/Users/zeynepenkavi/Downloads/bart_tsv/",file), sep = "\t")
  tmp$Sub_id <- as.numeric(strsplit(file, "_")[[1]][1])
  
  if (file == file_list[1]){
    bart_data = tmp
  }
  else{
    bart_data = rbind(bart_data, tmp)
  }
  rm(tmp)
}

rm(file, file_list)
```

# Sample info

First let's get a sense of the sample. Here is how many subjects we have who have complete datasets for the probabilistic learning task and their age break downs.

```{r sample_info, warning=FALSE}
machine_game_data_clean %>% 
  group_by(age_group) %>%
  summarise(min_age = min(calc_age),
            mean_age = mean(calc_age),
            sd_age = sd(calc_age),
            max_age = max(calc_age),
            n = n()/180)
```

# Performance in probabilistic learning task

We first examine the behavior in the probabilistic learning task.   

In this task subjects are presented with a fractal in each trial. The fractals represent different machines. Subjects choose to play or pass in each trial. Each machine yields a probabilistic reward. There are four machines in total. Two with positive and two with negative expected value. One of each of these machines has a low variance reward schedule while the other has a high variance reward schedule. More clearly:  
- One machine gives \$5 90% of the time and -\$495 %10 of the time  
- One machine gives -\$5 90% of the time and \$495 %10 of the time  
- One machine gives \$10 50% of the time and -$100 %50 of the time  
- One machine gives -\$10 50% of the time and $100 %50 of the time  

## Points earned

Performance in this task can be assessed by looking at the total number of points subjects make at the end of task. The following graph shows that adults collect more points in this task compared to kids.

```{r}
machine_game_data_clean %>%
  group_by(Sub_id, facet_labels) %>%
  summarise(total_points = sum(Points_earned)) %>%
  do(assign.age.info(.)) %>%
  group_by(age_group) %>%
  summarise(mean_points = mean(total_points),
            sem_points = sem(total_points)) %>%
  ggplot(aes(age_group, mean_points))+
  geom_bar(stat='identity', position = position_dodge((0.9)))+
  geom_errorbar(aes(ymin=mean_points-sem_points, ymax=mean_points+sem_points), position = position_dodge(0.9), width=0.25)+
  theme_bw()+
  xlab('Machine')+
  ylab('Mean points')+
  labs(fill='Age group')

```

This difference is statistically significant: adults earn more points compared to the kids (t = 2.771).

```{r}
tmp = machine_game_data_clean %>%
  group_by(Sub_id) %>%
  summarise(total_points = sum(Points_earned)) %>%
  do(assign.age.info(.))

summary(lm(total_points~age_group, data=tmp))
```
```{r echo=FALSE}
rm(tmp)
```

Since we are interested in the age differences between sensitivity to different feedback schedules, **we should show that this difference in performance exists especially for the high variance feedback condition(s)**. Here is the plot of performance (total points earned) broken down by conditions.

```{r}
machine_game_data_clean %>%
  group_by(Sub_id, facet_labels) %>%
  summarise(total_points = sum(Points_earned)) %>%
  do(assign.age.info(.)) %>%
  group_by(age_group, facet_labels) %>%
  summarise(mean_points = mean(total_points),
            sem_points = sem(total_points)) %>%
  ggplot(aes(facet_labels, mean_points, fill=age_group))+
  geom_bar(stat='identity', position = position_dodge((0.9)))+
  geom_errorbar(aes(ymin=mean_points-sem_points, ymax=mean_points+sem_points), position = position_dodge(0.9), width=0.25)+
  theme_bw()+
  xlab('Machine')+
  ylab('Mean points')+
  labs(fill='Age group')

```

Running separate models for positive and negative EV machines for ease of interpretation.

```{r}
tmp <- machine_game_data_clean %>%
  group_by(Sub_id, facet_labels) %>%
  summarise(total_points = sum(Points_earned)) %>%
  do(assign.age.info(.))
```

In the positive EV machines there is only a main effect of variance. All groups perform better is low variance condition. There are no age differences.

```{r}
summary(lm(total_points ~ age_group*facet_labels, data = tmp %>% filter(facet_labels %in% c("-10,+100", "-5,+495"))))
```

In the negative EV machines there is again the effect of variance: Everyone losses fewer points in the low variance condition. There is also a main effect for adults: Adults perform better than kids for both negative EV machines.

```{r}
summary(lm(total_points ~ age_group*facet_labels, data = tmp %>% filter(facet_labels %in% c("+10,-100", "+5,-495"))))
```

```{r echo=FALSE}
rm(tmp)
```

So the age diffence in performance is driven by difference in performance in negative EV machines. The question is what difference in behavior in these conditions is leading to this difference in performance?  To anticipate possible cognitive processes that will be parameterized in RL models differences can lie in: how quickly the groups learn the probabilities, how much weight they put on the outcomes and/or how much like an optimal agent they behave.

## Proportion of playing

The first thing we can look at is how often do subjects play versus pass. It's hard to see any age differences when we just look at frequency of overall playing as below.   

```{r}
machine_game_data_clean %>%
  group_by(Sub_id, Response) %>%
  tally %>%
  group_by(Sub_id) %>%
  mutate(pct=(100*n)/sum(n)) %>%
  do(assign.age.info(.)) %>%
  group_by(age_group, Response) %>%
  dplyr::summarise(mean_pct = mean(pct),
            sem_pct = sem(pct)) %>%
  ggplot(aes(Response, mean_pct, fill = age_group))+
  geom_bar(stat='identity', position = position_dodge(0.9))+
  geom_errorbar(aes(ymin = mean_pct - sem_pct, ymax = mean_pct + sem_pct), position = position_dodge(width = 0.9), width=0.25)+
  theme_bw()+
  ylab('Percentage of trials')+
  labs(fill = 'Age group')
```

It is also not immediately apparent how to translate this to better performance/learning in this task but one way to think about it: If people learned perfectly they should play half of the time (always for the positive expected value trial and never for the negative expected value trials). The fact that all play proportions are above 50% suggests that nobody learns perfectly and that adults might be closest to it. But this is very crude and a better way to look at it would be to see   
1. how this depends on the different machines and   
2. how it changes throughout the task.

To get a better sense of overall behavior in different contingency states we break this proportion of playing down by machines.

Now we can see age differences in playing frequency in different conditions, particularly in the negative expected value machines (bottom row).

```{r warning=FALSE, message=FALSE}
machine_game_data_clean %>%
  group_by(Sub_id, facet_labels, Response) %>%
  tally %>%
  group_by(Sub_id, facet_labels) %>%
  mutate(pct=(100*n)/sum(n)) %>%
  do(assign.age.info(.)) %>%
  group_by(age_group, facet_labels, Response) %>%
  dplyr::summarise(mean_pct = mean(pct),
            sem_pct = sem(pct)) %>%
  ggplot(aes(Response, mean_pct, fill = age_group))+
  geom_bar(stat='identity', position = position_dodge(0.9))+
  geom_errorbar(aes(ymin = mean_pct - sem_pct, ymax = mean_pct + sem_pct), position = position_dodge(width = 0.9), width=0.25)+
  theme_bw()+
  ylab('Percentage of trials')+
  facet_wrap(~facet_labels)+
  labs(fill = 'Age group')
```

The differences in points earned map directly on to proportion of choosing to play each machine:  

- Adults (baseline in the regression below) play most for the low variance positive expected value condition (top left) then significantly less for the high variance positive expected value condition (top right), even less for high variance negative expected value condition (bottom right), and least for the low variance negative expected value condition (bottom left). **This pattern reflects a sensitivity to low variance (better learning for low variance conditions comparing the equivalent expected values).**
- Teens do not differ significantly from adults in any condition.
- Kids do not differ from adults in playing proportion in the positive expected value conditions (top row) but they do choose to play the machine for both of the negative expected value conditions (bottom row)

```{r}
tmp <- machine_game_data_clean %>%
  group_by(Sub_id, facet_labels, Response) %>%
  tally %>%
  group_by(Sub_id, facet_labels) %>%
  mutate(pct_play=(100*n)/sum(n)) %>%
  filter(Response == 'play') %>%
  do(assign.age.info(.))

summary(lmer(pct_play ~ age_group*facet_labels + (1|Sub_id), data = tmp))
```
```{r echo=FALSE}
rm(tmp)
```

This is not surprising given what the number of points earned already showed. But now that we are looking at a behavioral measure instead of an outcome measure we might be able to quantify constructs of interest like sensitivity to variance or sensitivity to the expected values of the machines.  

Initially I tried defining these in terms of probability of playing a machine but as I worked through this I have changed my mind about the dependent variable these changes should be defined by (or rather the coding of the dependent variable). I kept confusing myself about when playing a machine was 'good' or suggested learning. So I recoded the choices to be `correct` when a subject chooses to play a positive expected value machine and pass a negative expected value machine and `incorrect` when the reverse is true. If a subject is learning they should be learning to play the positive expected machines and to pass the others.

## Learning 

Recoding the behavior in this way gave a clearer picture of the age difference *in learning of optimal behavior* between the conditions. Specifically we can now look at how the probability of a correct choice changes for each age group in each condition across trials.

```{r warning=FALSE, message=FALSE}
machine_game_data_clean %>%
  ggplot(aes(scale(Trial_number), correct1_incorrect0))+
  geom_line(aes(group = Sub_id, col= factor(age_group, levels=c('kid', 'teen', 'adult'))),stat='smooth', method = 'glm', method.args = list(family = "binomial"), se = FALSE, alpha=0.2)+
  geom_line(aes(col= factor(age_group, levels=c('kid', 'teen', 'adult'))),stat='smooth', method = 'glm', method.args = list(family = "binomial"), se = FALSE, alpha=1, size=2)+
  facet_wrap(~facet_labels)+
  theme_bw()+
  xlab("Relative trial number")+
  scale_y_continuous(breaks=c(0,1))+
  labs(col="Age group")+
  ylab('Optimal choice')+
  theme(legend.position = "bottom")
```
Adults are more likely to make correct decisions in low var positive EV machine.

```{r}
summary(glmer(correct1_incorrect0 ~ age_group*scale(Trial_number)+(1|Sub_id), data = machine_game_data_clean %>% filter(facet_labels %in% c('-10,+100')), family=binomial))
```

The probability of making a correct response for the high var positive EV machine doesn't change for adults or kids but increases for teens across trials.

```{r}
summary(glmer(correct1_incorrect0 ~ age_group*scale(Trial_number)+(1|Sub_id), data = machine_game_data_clean %>% filter(facet_labels %in% c('-5,+495')), family=binomial))
```

All groups show improvement across trials for the low var negative EV machine but adults learn faster than kids and teens.

```{r}
summary(glmer(correct1_incorrect0 ~ age_group*scale(Trial_number)+(1|Sub_id), data = machine_game_data_clean %>% filter(facet_labels %in% c('+10,-100')), family=binomial))
```

Kids don't show learning across trials for the high var negative EV machine but adults and teens do. 

```{r}
summary(glmer(correct1_incorrect0 ~ age_group*scale(Trial_number)+(1|Sub_id), data = machine_game_data_clean%>% filter(facet_labels %in% c('+5,-495')), family=binomial))
```

I tried to capture these effects in 'individual difference' variables by running the logistic regression separately for each subject in each condition. This wouldn't capture anything different than the above analyses but I wanted to see if there were any subject-specific indices that could be correlated with other measues. I looked at three parameters:

- The intercept: whether they are more or less likely to choose the optimal action having seen half of the trials (p>0.5 if intercept>0 (i.e. log(0.5/0.5)))
- The slope: which direction and how fast the sigmoid moves in (for learning this must be positive and the larger it is the better the learning)  
- The learning index: where in the task (i.e. scaled trial number) they are at 50% for each machine (switch point - I came up with this to capture change in both parameters. I'm not sure if it makes sense.) The smaller the better (the sooner they learn the better choice).  

Because each model is run only on 45 trials the fits aren't great and the parameter distributions have large variances.

```{r warning=FALSE, message = FALSE}
get_learning_coef <- function(data){
  model = glm(correct1_incorrect0 ~ scale(Trial_number), family = binomial(link=logit), data = data)
  b0 = coef(model)[1]
  b1 = coef(model)[2]
  learnIndex = -b0/b1                   
  return(data.frame(b0, b1, learnIndex))
}


tmp = machine_game_data_clean %>%
  group_by(Sub_id, facet_labels) %>%
  do(get_learning_coef(.)) %>%
  do(assign.age.info(.)) 
```

(Error bars not shown because they are very large due to bad fits)
As expected the difference between kids and adults in slopes for the high variance negative EV machine is visible here too.

```{r}
tmp %>%
  ungroup()%>%
  select(facet_labels, age_group, b0, b1, learnIndex) %>%
  gather(key, value, -facet_labels, -age_group) %>%
  group_by(age_group, facet_labels, key) %>%
  summarise(mv = median(value),
            sv = sem(value)) %>%
  ggplot(aes(facet_labels, mv, fill=age_group))+
  geom_bar(stat="identity", position = position_dodge())+
  # geom_errorbar(aes(ymin = mv-sv, ymax = mv+sv), position = position_dodge(width = 0.9), width=0)+
  facet_wrap(~key, scale="free")+
  theme(legend.position = "bottom",
        legend.title = element_blank())+
  xlab("")+
  ylab("Median value")
```

But it's not a good idea to look for group differences in these parameters as they are highly variable due to bad fits from few trials.

## Does this correlate with BART?

Quick look at how this relates to BART data:

```{r}
adjusted.pumps <- function(subject_data){
  subject_data_adjusted = subject_data[subject_data$exploded == 0,]
  subject_pumps <- subject_data_adjusted %>% 
    group_by(trial.num) %>%
    summarise(total_pumps = sum(finished))
  out <- data.frame(mean_adjusted_pumps = mean(subject_pumps$total_pumps))
  return(out)
}
```

Increase in number of pumps with age

```{r warning=FALSE, message=FALSE}
bart_data %>%
  group_by(Sub_id) %>%
  do(adjusted.pumps(.)) %>%
  do(assign.age.info(.)) %>%
  ggplot(aes(x=calc_age, y = mean_adjusted_pumps))+
  geom_point()+
  theme_bw()+
  geom_smooth(method = "lm") +
  xlab("Age")+
  ylab("Risk taking (adjusted pumps)")
```

There aren't any meaningful correlations between slopes and mean adjusted pumps. *BUT* neither of these seem like good individual difference measures.

```{r warning=FALSE, message=FALSE}
tmp = bart_data %>%
  group_by(Sub_id) %>%
  do(adjusted.pumps(.)) %>%
  do(assign.age.info(.)) %>%
  select(Sub_id, mean_adjusted_pumps)

machine_game_data_clean %>%
  group_by(Sub_id, facet_labels) %>%
  do(get_learning_coef(.)) %>%
  do(assign.age.info(.)) %>%
  left_join(tmp, by = 'Sub_id') %>%
  group_by(facet_labels, age_group) %>%
  summarise(cor = cor.test(b1, mean_adjusted_pumps)$estimate,
            p_value = cor.test(b1, mean_adjusted_pumps)$p.value) %>%
  arrange(cor)
```
```{r echo=FALSE}
rm(tmp)
```

## Sensitivity to variance vs. expected value

Does it makes sense to look at these separately?  

Since the machines differ in the variance of the outcomes and expected values it might seem sensible to look at which of these attributes has a larger effect on performance.  

It's tempting to tease apart the relative importance of these attributes for the high variance negative EV machine where we observe the performance difference between age groups.  

**BUT** these attributes are correlated. So we can't look at their effects separately in the same model.   

```{r}
#Function to calculate observed variance and observed expected value based on outcomes in trials that the subject has played.
get_obs_var_ev <- function(data){
  
  new_data = data
  new_data$obs_var <- NA
  new_data$obs_ev <-  NA
  
  for(i in 1:nrow(new_data)){
    if(i == 1){
      obs = 0
      obs_ev = 0
      obs_var = 0
    }
    else{
      #get all the trials until the current trial
      obs = new_data[1:i,]
      #filter only played trials; their belief should not be updated based on the trials they haven't played
      obs = obs %>% filter(Response == "play") %>% ungroup() %>% select(Points_earned)
      obs_var = var(obs)
      obs_probs =  as.numeric(prop.table(table(obs)))
      obs_rewards = as.numeric(names(prop.table(table(obs))))
      obs_ev = sum(obs_probs*obs_rewards)
    }
    new_data$obs_var[i] = obs_var
    new_data$obs_ev[i] = obs_ev
  }
  new_data$obs_var = ifelse(is.na(new_data$obs_var), 0, new_data$obs_var)
  return(new_data)
}
```

```{r}
tmp = machine_game_data_clean %>%
  group_by(Sub_id, facet_labels) %>%
  do(get_obs_var_ev(.))
```

```{r}
tmp %>%
  ggplot(aes(obs_var, obs_ev))+
  geom_point()+
  facet_wrap(~facet_labels, scales="free")+
  xlab("Observed variance")+
  ylab("Observed EV")
```

What we are interested in is the effect of beliefs about the machines on behavior. These beliefs can be summarized quantitatively in an 'expected value.'  

The cognitive processes that can differ with respect to this expected value can be how quickly it approaches the true expected value of a machine (the rate at which one incorporates each new data point to existing beliefs) and how truthfully the expected values are evaluated (is the utility of the expected value the same as its value).   

These two processes can be captured as the learning rate and the exponent on the prediction error in an RL model.

Plotting the effect of EV on choice to confirm that it makes sense and captures the behavioral effect:  
The higher the EV of a machine the more likely it is to be played. This is the correct action for the positive EV machines but incorrect action for the negative EV machines. The behavioral effect in the high var negative EV machine is captured again with the diverging lines for age groups at low EVs.

```{r warning=FALSE, message=FALSE}
tmp %>%
  ggplot(aes(obs_ev, correct1_incorrect0))+
  geom_line(aes(group = Sub_id, col= age_group),stat='smooth', method = 'glm', method.args = list(family = "binomial"), se = FALSE, alpha=0.2)+
  geom_line(aes(col= age_group),stat='smooth', method = 'glm', method.args = list(family = "binomial"), se = FALSE, alpha=1, size=2)+
  facet_wrap(~facet_labels, scales='free')+
  xlab("EV of played trials")+
  scale_y_continuous(breaks=c(0,1))+
  labs(col="Age group")+
  ylab('Correct')+
  theme(legend.position = "bottom",
        legend.title = element_blank())
```
